{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c56f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5py 안 될 때\n",
    "#!brew reinstall hdf5\n",
    "#!export CPATH=\"/opt/homebrew/include/\"\n",
    "#!export HDF5_DIR=/opt/homebrew/\n",
    "#!python3 -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35449e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd drive/MyDrive/cuisine-prediction/Hanseul/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29816293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from sklearn.metrics import f1_score, top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb55c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '../'\n",
    "path_container = './Container/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "788f8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_container + 'id_cuisine_dict.pickle', 'rb') as f:\n",
    "    id_cuisine_dict = pickle.load(f)\n",
    "with open(path_container + 'cuisine_id_dict.pickle', 'rb') as f:\n",
    "    cuisine_id_dict = pickle.load(f)\n",
    "with open(path_container + 'id_ingredient_dict.pickle', 'rb') as f:\n",
    "    id_ingredient_dict = pickle.load(f)\n",
    "with open(path_container + 'ingredient_id_dict.pickle', 'rb') as f:\n",
    "    ingredient_id_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bfefc",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c92050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, data_dir, test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.test = test\n",
    "        self.classify, self.complete = False, False\n",
    "        with h5py.File(data_dir, 'r') as data_file:\n",
    "            self.bin_data = data_file['bin_data'][:]  # Size (num_recipes=23547, num_ingredients=6714)\n",
    "            if 'label_class' in data_file.keys():\n",
    "                self.classify = True\n",
    "                self.label_class = data_file['label_class'][:]  \n",
    "            if 'label_compl' in data_file.keys():\n",
    "                self.complete = True\n",
    "                self.label_compl = data_file['label_compl'][:]\n",
    "        \n",
    "        self.padding_idx = self.bin_data.shape[1]  # == num_ingredient == 6714\n",
    "        self.max_num_ingredients_per_recipe = self.bin_data.sum(1).max()  # valid & test의 경우 65\n",
    "        \n",
    "        # (59나 65로) 고정된 길이의 row vector에 해당 recipe의 indices 넣고 나머지는 padding index로 채워넣기\n",
    "        # self.int_data: Size (num_recipes=23547, self.max_num_ingredients_per_recipe=59 or 65)\n",
    "        self.int_data = np.full((len(self.bin_data), self.max_num_ingredients_per_recipe), self.padding_idx) \n",
    "        for i, bin_recipe in enumerate(self.bin_data):\n",
    "            recipe = np.arange(self.padding_idx)[bin_recipe==1]\n",
    "            self.int_data[i][:len(recipe)] = recipe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.bin_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bin_data = self.bin_data[idx]\n",
    "        int_data = self.int_data[idx]\n",
    "        if self.test:\n",
    "            return bin_data, int_data\n",
    "        \n",
    "        if self.classify:\n",
    "            label_class = self.label_class[idx]\n",
    "            if not self.complete:\n",
    "                return bin_data, int_data, label_class\n",
    "        if self.complete:\n",
    "            label_compl = self.label_compl[idx]\n",
    "            if not self.classify:\n",
    "                return bin_data, int_data, label_compl\n",
    "            else:\n",
    "                return bin_data, int_data, label_class, label_compl\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b56130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = ['train_class', 'train_compl', 'valid_class', 'valid_compl', 'test_class', 'test_compl']\n",
    "\n",
    "recipe_datasets = {x: RecipeDataset(os.path.join(path_container, x), test='test' in x) for x in dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "562aabe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "[564, 1263, 4074, 4203, 4901, 5277, 5360, 6232, 7835, 10585, 10777, 12476, 13301, 13989, 15951, 17374, 18153, 19039, 20469]\n"
     ]
    }
   ],
   "source": [
    "count_single_ingredient_recipe = 0\n",
    "list_single_ingredient_recipe = []\n",
    "for i in range(len(recipe_datasets['train_class'])):\n",
    "    _bd, _,_ = recipe_datasets['train_class'][i]\n",
    "    if _bd.sum()<2:\n",
    "        count_single_ingredient_recipe += 1\n",
    "        list_single_ingredient_recipe.append(i)\n",
    "print(count_single_ingredient_recipe)\n",
    "print(list_single_ingredient_recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295438fa",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9be1d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building blocks of Set Transformers ##\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False, dropout=0):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.p = dropout\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Sequential(\n",
    "            nn.Linear(dim_V, dim_V),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_V, dim_V))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, Q, K, mask=None):\n",
    "        Q = self.fc_q(Q)  # (batch, q_len, d_hid == dim_V)\n",
    "        K, V = self.fc_k(K), self.fc_v(K) # (batch, k_len or v_len, d_hid)\n",
    "        \n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)  # (batch * num_heads, q_len, d_hid // num_heads)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)  # (batch * num_heads, c_len, d_hid // num_heads)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)  # (batch * num_heads, v_len, d_hid // num_heads)\n",
    "        \n",
    "        energy = Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V)  # (batch * num_heads, q_len, k_len)\n",
    "        if mask is not None:  # mask: (batch, 1, k_len)\n",
    "            energy.masked_fill_(mask.repeat(self.num_heads, 1, 1), float('-inf'))\n",
    "        A = torch.softmax(energy, 2)  # (batch * num_heads, q_len, k_len)\n",
    "        \n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)  # (batch, q_len, d_hid)\n",
    "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
    "        _O = self.fc_o(O)\n",
    "        if self.p > 0:\n",
    "            _O = self.dropout(_O)\n",
    "        O = O + _O \n",
    "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False, dropout=0.2):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(X, X, mask=mask)\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False, dropout=0.2):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X, mask=mask)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False, dropout=0.2):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln, dropout=dropout)\n",
    "        \n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c6ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(x):\n",
    "        \"\"\" Convert int_data into bin_data, if needed. \"\"\"\n",
    "        if type(x) is not torch.Tensor:\n",
    "            x = torch.LongTensor(x)\n",
    "        if x.dim() > 2:\n",
    "            x = x.squeeze()\n",
    "            if x.dim() > 2:\n",
    "                return False\n",
    "        elif x.dim() < 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        return F.one_hot(x).sum(1)[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e95d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    (BatchNorm - LeakyReLU - Linear) * 2.\n",
    "    Apply skip connection only when dim_input == dim_output.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output, norm='bn', dropout=0.2):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.use_skip_conn = (dim_input == dim_output)\n",
    "        if norm == 'bn':\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        elif norm == 'ln':\n",
    "            norm_layer = nn.LayerNorm\n",
    "        ff = []\n",
    "        if norm in ['bn', 'ln']:\n",
    "            ff.append(norm_layer(dim_input))\n",
    "        ff.extend([nn.LeakyReLU(), nn.Linear(dim_input, dim_hidden)])\n",
    "        if norm in ['bn', 'ln']:\n",
    "            ff.append(norm_layer(dim_hidden))\n",
    "        ff.extend([nn.LeakyReLU(), nn.Linear(dim_hidden, dim_output)])\n",
    "        if dropout > 0:\n",
    "            ff.append(nn.Dropout(dropout))\n",
    "        self.ff = nn.Sequential(*ff)\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.use_skip_conn:\n",
    "            return self.ff(x) + x\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb7c0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" Create Feature Vector of Given Recipe. \"\"\"\n",
    "    def __init__(self, dim_embedding=256,\n",
    "                 num_items=6714, \n",
    "                 num_inds=32,      # For ISAB\n",
    "                 dim_hidden=128, \n",
    "                 num_heads=4, \n",
    "                 num_enc_layers=4,\n",
    "                 ln=True,          # LayerNorm option\n",
    "                 dropout=0.2,       # Dropout option\n",
    "                 encoder_mode = 'set_transformer',\n",
    "                 enc_pool_mode = 'set_transformer',\n",
    "                ):\n",
    "        super(Encoder, self).__init__()\n",
    "        assert num_enc_layers % 2 == 0\n",
    "        self.encoder_mode, self.enc_pool_mode = encoder_mode, enc_pool_mode\n",
    "        self.padding_idx = num_items\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_items+1, embedding_dim=dim_embedding, padding_idx=-1)\n",
    "        if encoder_mode == 'deep_sets':\n",
    "            self.encoder = nn.ModuleList(\n",
    "                [ResBlock(dim_embedding, dim_hidden, dim_hidden, norm='ln', dropout=dropout)] +\n",
    "                [ResBlock(dim_hidden, dim_hidden, dim_hidden, norm='ln', dropout=dropout) for _ in range(num_enc_layers-1)])\n",
    "        elif encoder_mode == 'set_transformer':\n",
    "            self.encoder = nn.ModuleList(\n",
    "                [ISAB(dim_embedding, dim_hidden, num_heads, num_inds, ln=ln, dropout=dropout)] +\n",
    "                [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln, dropout=dropout) for _ in range(num_enc_layers-1)])\n",
    "        elif encoder_mode == 'fusion':\n",
    "            self.encoder = nn.ModuleList(\n",
    "                [ResBlock(dim_embedding, dim_hidden, dim_hidden, norm='ln', dropout=dropout)] +\n",
    "                [ResBlock(dim_hidden, dim_hidden, dim_hidden, norm='ln', dropout=dropout) for _ in range(num_enc_layers//2-1)] +\n",
    "                [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln, dropout=dropout) for _ in range(num_enc_layers//2)])\n",
    "        if enc_pool_mode == 'deep_sets':\n",
    "            def sumpool(x,**kwargs):\n",
    "                return torch.sum(x, 1)\n",
    "            self.pooling = sumpool\n",
    "        elif enc_pool_mode == 'set_transformer':\n",
    "            self.pooling = PMA(dim_hidden, num_heads, 1, ln=ln, dropout=dropout)\n",
    "        self.out = self.mask = None\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # x(=recipes): (batch, max_num_ingredient=65) : int_data.\n",
    "        self.out = self.embedding(x)  # (batch, max_num_ingredient=65, dim_embedding=256)\n",
    "        # cf. embedding.weight: (num_items+1=6715, dim_embedding=256)\n",
    "        for module in self.encoder:\n",
    "            self.out = module(self.out, mask=mask) # (batch, max_num_ingredient=65, dim_hidden=128) : permutation-equivariant.\n",
    "        return self.pooling(self.out, mask=mask) # (batch, 1, dim_hidden=128) : permutation-invariant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cefad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dim_hidden=128, dim_output=20, dropout=0.2, num_dec_layers=4):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.ModuleList(\n",
    "                [ResBlock(dim_hidden, dim_hidden, dim_hidden, norm='bn', dropout=dropout) for _ in range(num_dec_layers-1)]\n",
    "                +[ResBlock(dim_hidden, dim_hidden, dim_output, norm='bn', dropout=dropout)])\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # x: (batch, dim_hidden)\n",
    "        assert x.ndim == 2\n",
    "        self.out = x\n",
    "        for module in self.classifier:\n",
    "            self.out = module(self.out)\n",
    "        return self.out  # (batch, dim_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69d66c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Completer(nn.Module):\n",
    "    def __init__(self, dim_embedding=256,\n",
    "                 num_items=6714, \n",
    "                 #num_inds=32,      # For ISAB\n",
    "                 dim_hidden=128, \n",
    "                 num_heads=4, \n",
    "                 num_dec_layers=4,\n",
    "                 ln=True,          # LayerNorm option\n",
    "                 dropout=0.2,      # Dropout option\n",
    "                 mode = 'simple',\n",
    "                ):\n",
    "        super(Completer, self).__init__()\n",
    "\n",
    "        assert mode in ['simple','concat','concat_attention','attention']\n",
    "        \n",
    "        self.num_items = num_items\n",
    "        self.mode = mode\n",
    "        # feedforward layer to process recipe representation\n",
    "        self.ff = nn.Sequential(\n",
    "                ResBlock(dim_hidden, dim_hidden, dim_hidden, norm='bn', dropout=dropout),\n",
    "                ResBlock(dim_hidden, dim_hidden, dim_hidden, norm='bn', dropout=dropout))\n",
    "        # 'simple': no need of embedding weight\n",
    "        if mode == 'simple': \n",
    "            self.decoder = nn.ModuleList(\n",
    "                [ResBlock(dim_hidden, dim_hidden, dim_hidden, norm='bn', dropout=dropout) for _ in range(num_dec_layers-3)]\n",
    "                +[ResBlock(dim_hidden, dim_hidden, num_items, norm='bn', dropout=dropout)])\n",
    "        # NCF style completer\n",
    "        elif mode == 'concat':\n",
    "            self.emb_encoder = nn.Sequential(\n",
    "                ResBlock(dim_embedding, dim_hidden, dim_hidden, norm='ln', dropout=dropout),\n",
    "                ResBlock(dim_hidden, dim_hidden, dim_hidden, norm='ln', dropout=dropout))\n",
    "            # decoding feedforward layer to deal with a concatenated feature (dim=2*dim_hidden)\n",
    "            self.decoder = nn.ModuleList(\n",
    "                [ResBlock(2*dim_hidden, dim_hidden, dim_hidden//2, norm='ln', dropout=dropout)]\n",
    "                +[ResBlock(dim_hidden//2, dim_hidden//2, dim_hidden//2, norm='ln', dropout=dropout) for _ in range(num_dec_layers-4)]\n",
    "                +[ResBlock(dim_hidden//2, dim_hidden//2, 1, norm='ln', dropout=dropout)])\n",
    "        # completer based on concat + attention\n",
    "        elif mode == 'concat_attention':\n",
    "            self.emb_encoder = nn.Sequential(\n",
    "                ResBlock(dim_embedding, dim_hidden, dim_hidden, norm='ln', dropout=dropout),\n",
    "                ResBlock(dim_hidden, dim_hidden, dim_hidden, norm='ln', dropout=dropout))\n",
    "            self.new_set_encoder = nn.ModuleList(\n",
    "                [SAB(dim_hidden, dim_hidden, num_heads, ln=ln, dropout=dropout) for _ in range(num_dec_layers//2-1)])\n",
    "            self.decoder= nn.ModuleList(\n",
    "                [ResBlock(dim_hidden, dim_hidden, dim_hidden, norm='ln', dropout=dropout) for _ in range(num_dec_layers-num_dec_layers//2-2)]\n",
    "                +[ResBlock(dim_hidden, dim_hidden, 1, norm='ln', dropout=dropout)])\n",
    "        # completer based on attention\n",
    "        elif mode == 'attention':\n",
    "            pass\n",
    "        \n",
    "        self.out = self.emb_feature = None\n",
    "        \n",
    "    def forward(self, x, embedding_weight):\n",
    "        # x: (batch, 1, dim_hidden) / embedding_weight: (num_items, dim_embedding)        \n",
    "        self.out = self.ff(x.squeeze(1))  # (batch, dim_hidden=128)\n",
    "\n",
    "        if self.mode == 'simple':\n",
    "            for module in self.decoder:\n",
    "                self.out = module(self.out)\n",
    "            return self.out # (batch, num_items=6714)\n",
    "        else:\n",
    "            batch_size, num_items = x.size(0), embedding_weight.size(0)\n",
    "            if self.mode == 'concat':\n",
    "                self.emb_feature = self.emb_encoder(embedding_weight)  # (num_items, dim_hidden)\n",
    "                self.out = torch.cat([self.out.unsqueeze(1).expand(-1,num_items,-1),\n",
    "                    self.emb_feature.unsqueeze(0).expand(batch_size,-1,-1)], dim=2)  # (batch, num_items, 2*dim_hidden)\n",
    "                for module in self.decoder:\n",
    "                    self.out = module(self.out)  # (batch, num_items, 1)\n",
    "                return self.out.squeeze(-1)  # (batch, num_items)\n",
    "            elif self.mode == 'concat_attention':\n",
    "                self.emb_feature = self.emb_encoder(embedding_weight)  # (num_items, dim_hidden)\n",
    "                self.out = torch.cat([self.out.view(batch_size,1,1,-1).expand(-1,num_items,-1,-1),\n",
    "                    self.emb_feature.view(1,num_items,1,-1).expand(batch_size,-1,-1,-1)], dim=2).view(batch_size*num_items,2,-1)  # (batch*num_items, 2, dim_hidden)\n",
    "                for module in self.new_set_encoder:\n",
    "                    self.out = module(self.out)  # (batch*num_items, 2, dim_hidden)\n",
    "                self.out = self.out.sum(1).view(batch_size, num_items, -1)  # (batch, num_items, dim_hidden)\n",
    "                for module in self.decoder:\n",
    "                    self.out = module(self.out)  # (batch, num_items, 1)\n",
    "                return self.out.squeeze(-1)  # (batch, num_items)\n",
    "            elif self.mode == 'attention':\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a0c046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCNet(nn.Module):\n",
    "    def __init__(self, dim_embedding=256,\n",
    "                 dim_output=20,\n",
    "                 num_items=6714, \n",
    "                 num_inds=32, \n",
    "                 dim_hidden=128, \n",
    "                 num_heads=4, \n",
    "                 num_enc_layers=4, \n",
    "                 num_dec_layers=2,\n",
    "                 ln=True,          # LayerNorm option\n",
    "                 dropout=0.5,      # Dropout option\n",
    "                 classify=True,    # completion만 하고 싶으면 False로\n",
    "                 complete=True,    # classification만 하고 싶으면 False로\n",
    "                 freeze_classify=False, # classification만 관련된 parameter freeze\n",
    "                 freeze_complete=False,  # completion만 관련된 parameter freeze\n",
    "                 encoder_mode = 'set_transformer',\n",
    "                 enc_pool_mode = 'set_transformer',\n",
    "                 decoder_mode = 'simple',\n",
    "                 ):\n",
    "        super(CCNet, self).__init__()\n",
    "        self.padding_idx = num_items\n",
    "        self.classify, self.complete = classify, complete\n",
    "\n",
    "        self.encoder = Encoder(dim_embedding=dim_embedding,\n",
    "                               num_items=num_items, \n",
    "                               num_inds=num_inds,\n",
    "                               dim_hidden=dim_hidden, \n",
    "                               num_heads=num_heads, \n",
    "                               num_enc_layers=num_enc_layers,\n",
    "                               ln=ln, dropout=dropout,\n",
    "                               encoder_mode=encoder_mode,\n",
    "                               enc_pool_mode=enc_pool_mode)\n",
    "        if classify:\n",
    "            self.classifier = Classifier(dim_hidden=dim_hidden,\n",
    "                                         dim_output=dim_output,\n",
    "                                         dropout=dropout)\n",
    "            if freeze_classify:\n",
    "                for p in self.classifier.parameters():\n",
    "                    p.requires_grad = False\n",
    "        if complete:\n",
    "            self.completer = Completer(dim_embedding=dim_embedding,\n",
    "                                       num_items=num_items, \n",
    "                                       #num_inds=num_inds,\n",
    "                                       dim_hidden=dim_hidden, \n",
    "                                       num_heads=num_heads, \n",
    "                                       num_dec_layers=num_dec_layers,\n",
    "                                       ln=ln, dropout=dropout,\n",
    "                                       mode = decoder_mode)\n",
    "            if freeze_complete:\n",
    "                for p in self.completer.parameters():\n",
    "                    p.requires_grad = False\n",
    "    \n",
    "    def forward(self, x, bin_x=None): \n",
    "        # x(=recipes): (batch, max_num_ingredients=65) : int_data.\n",
    "        if not (self.classify or self.complete):\n",
    "            return\n",
    "        self.mask = (x == self.padding_idx).unsqueeze(1)  # (batch, 1, max_num_ingredients)\n",
    "        recipe_feature = self.encoder(x, self.mask)  # (batch, 1, dim_hidden)\n",
    "        \n",
    "        logit_classification, logit_completion = None, None\n",
    "\n",
    "        # Classification:\n",
    "        if self.classify:\n",
    "            logit_classification = self.classifier(recipe_feature.squeeze(1))  # (batch, dim_output)\n",
    "            \n",
    "        # Completion:\n",
    "        if self.complete:\n",
    "            embedding_weight = self.encoder.embedding.weight[:-1]  # (num_items=6714, dim_embedding=256)\n",
    "            logit_completion = self.completer(recipe_feature, embedding_weight)  # (batch, num_items)\n",
    "\n",
    "        return logit_classification, logit_completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1997ed3",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "732e5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitSelector(nn.Module):\n",
    "    \"\"\"\n",
    "    For each 3-tuple (output vector, label, data), choose 'important' model outputs.\n",
    "    Specifically, choose a fixed number (e.g. rank = 100) of outputs consist of\n",
    "    1) output for label index(=missing ingredient index),\n",
    "    2) outputs for data indices(=given ingredient indices),  -- maybe unnecessary. can be included or not by 'contain_data' option.\n",
    "    3) and several highest outputs for non-label indices.\n",
    "\n",
    "    x: (batch, max_ingredient_num = 65 or 59), LongTensor\n",
    "    output: (batch, num_items = 6714), FloatTensor\n",
    "    labels: (batch, ), LongTensor\n",
    "    rank: int\n",
    "    \"\"\"\n",
    "    def __init__(self, rank=100, contain_data=False):\n",
    "        super(LogitSelector, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.contain_data=contain_data\n",
    "\n",
    "    def forward(self, output, labels, x=None):\n",
    "        num_items = output.size(1)\n",
    "        if self.rank > num_items:\n",
    "            raise ValueError\n",
    "        target_indices = output.argsort(1)[:,-self.rank:]  # (batch, rank)\n",
    "        label_where = (target_indices == labels.view(-1,1))  # target_indices의 각 batch마다 이미 label이 있으면 그 위치에 True\n",
    "        no_label = torch.logical_not(label_where).all(dim=1)  # label 없는 batch에 대해 True\n",
    "        yes_label, label_where = label_where.nonzero(as_tuple=True)  # label 있는 batch와 그 때 label의 위치를 long으로\n",
    "        target_indices[no_label,0] = labels[no_label]  # label이 안 보였던 경우 맨 앞에 label 갖다 놓기\n",
    "        if self.contain_data and x is not None:\n",
    "            x_extended = F.pad(x, (1, self.rank-1-x.size(1)), 'constant', num_items)  # (batch, rank)\n",
    "            target_indices[x_extended != num_items] = x_extended[x_extended != num_items]\n",
    "        new_output = torch.gather(output, 1, target_indices) # (batch, rank)\n",
    "        new_labels = torch.zeros_like(labels).long()  # label을 맨 앞에 갖다 놨음\n",
    "        new_labels[yes_label] = label_where  # label이 이미 있었던 batch에 대해서만 수정\n",
    "        return new_output, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "848ee4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassASLoss(nn.Module):\n",
    "    '''\n",
    "    MultiClass ASL(single label) + F1 Loss.\n",
    "    \n",
    "    References:\n",
    "    - ASL paper: https://arxiv.org/abs/2009.14119\n",
    "    - optimized ASL: https://github.com/Alibaba-MIIL/ASL/blob/main/src/loss_functions/losses.py\n",
    "    '''\n",
    "    def __init__(self, gamma_pos=1, gamma_neg=4, eps: float = 0.1, reduction='mean', average='macro'):\n",
    "        super(MultiClassASLoss, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "        self.targets_classes = []\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.reduction = reduction\n",
    "        self.average = average\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        '''\n",
    "        \"input\" dimensions: - (batch_size,number_classes)\n",
    "        \"target\" dimensions: - (batch_size)\n",
    "        '''\n",
    "        log_preds = self.logsoftmax(inputs)\n",
    "        self.targets_classes = torch.zeros_like(inputs).scatter_(1, target.long().unsqueeze(1), 1) # make binary label\n",
    "\n",
    "        targets = self.targets_classes\n",
    "        anti_targets = 1. - targets\n",
    "        xs_pos = torch.exp(log_preds)\n",
    "        xs_neg = 1. - xs_pos\n",
    "        \n",
    "        # TP / FP / FN\n",
    "        tp = (xs_pos * targets).sum(dim=0)\n",
    "        fp = (xs_pos * anti_targets).sum(dim=0)\n",
    "        fn = (xs_neg * targets).sum(dim=0) \n",
    "        \n",
    "        if self.average == 'micro':\n",
    "            tp = tp.sum()\n",
    "            fp = fp.sum()\n",
    "            fn = fn.sum()\n",
    "        \n",
    "        # F1 score\n",
    "        f1 = (tp / (tp + 0.5*(fp + fn) + self.eps)).clamp(min=self.eps, max=1.-self.eps).mean()\n",
    "        \n",
    "        # ASL weights\n",
    "        xs_pos = xs_pos * targets\n",
    "        xs_neg = xs_neg * anti_targets\n",
    "        if self.gamma_pos > 0 or self.gamma_neg > 0:\n",
    "            asymmetric_w = torch.pow(1. - xs_pos - xs_neg,\n",
    "                                    self.gamma_pos * targets + self.gamma_neg * anti_targets)\n",
    "            log_preds = log_preds * asymmetric_w\n",
    "\n",
    "        if self.eps > 0:  # label smoothing\n",
    "            num_classes = inputs.size()[-1]\n",
    "            self.targets_classes = self.targets_classes.mul(1. - self.eps).add(self.eps / num_classes)\n",
    "\n",
    "        # loss calculation\n",
    "        loss = - self.targets_classes.mul(log_preds)\n",
    "\n",
    "        loss = loss.sum(dim=-1)\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss + (1. - f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a76b6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassFocalLoss(nn.Module):\n",
    "    '''\n",
    "    MultiClass F1 Loss + FocalLoss.\n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
    "    - https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\n",
    "    '''\n",
    "    def __init__(self, eps=1e-8, average='macro', reduction='mean', gamma=2):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.average = average\n",
    "        self.reduction = reduction\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # focal loss\n",
    "        loss = F.cross_entropy(pred, target, reduction=self.reduction)\n",
    "        pt = torch.exp(-loss)\n",
    "        if self.gamma>0:\n",
    "            loss = (1-pt)**self.gamma * loss\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        \n",
    "        # f1 loss\n",
    "        target = F.one_hot(target, pred.size(-1)).float()\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        \n",
    "        tp = (target * pred).sum(dim=0).float()\n",
    "        fp = ((1 - target) * pred).sum(dim=0).float()\n",
    "        fn = (target * (1 - pred)).sum(dim=0).float()\n",
    "\n",
    "        if self.average == 'micro':\n",
    "            tp, fp, fn = tp.sum(), fp.sum(), fn.sum()\n",
    "        \n",
    "        f1 = (tp / (tp + 0.5*(fp + fn) + self.eps)).clamp(min=self.eps, max=1-self.eps).mean()\n",
    "\n",
    "        return 1. - f1 + loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bc3dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelASLoss(nn.Module):\n",
    "    '''\n",
    "    MultiLabel ASL Loss + F1 Loss\n",
    "    \n",
    "    References:\n",
    "    - ASL paper: https://arxiv.org/abs/2009.14119\n",
    "    - optimized ASL: https://github.com/Alibaba-MIIL/ASL/blob/main/src/loss_functions/losses.py\n",
    "    '''\n",
    "    def __init__(self, gamma_pos=1, gamma_neg=4, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False, average='macro'):\n",
    "        super(MultiLabelASLoss, self).__init__()\n",
    "        \n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "        self.average = average\n",
    "\n",
    "        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n",
    "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
    "        self.tp = self.fp = self.fn = self.f1 = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "\n",
    "        self.targets = y.float()\n",
    "        self.anti_targets = 1 - y.float()\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        self.xs_pos = torch.sigmoid(x.float())\n",
    "        self.xs_neg = 1.0 - self.xs_pos\n",
    "        \n",
    "        # TP/FP/FN\n",
    "        self.tp = (self.xs_pos * self.targets).sum(dim=0)\n",
    "        self.fp = (self.xs_pos * self.anti_targets).sum(dim=0)\n",
    "        self.fn = (self.xs_neg * self.targets).sum(dim=0)        \n",
    "        \n",
    "        if self.average == 'micro':\n",
    "            self.tp = self.tp.sum()\n",
    "            self.fp = self.fp.sum()\n",
    "            self.fn = self.fn.sum()\n",
    "        \n",
    "        # F1 score\n",
    "        self.f1 = (self.tp / (self.tp + 0.5*(self.fp + self.fn) + self.eps)).clamp(\n",
    "            min=self.eps, max=1-self.eps)\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            self.xs_neg.add_(self.clip)\n",
    "            self.xs_neg.clamp_(max=1.)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
    "        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            self.xs_pos = self.xs_pos * self.targets\n",
    "            self.xs_neg = self.xs_neg * self.anti_targets\n",
    "            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            self.loss *= self.asymmetric_w\n",
    "\n",
    "        return -self.loss.mean(dim=0).sum() + (1.-self.f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2c34830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelBCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiLabel weighted F1_Loss + BCEWithLogitsLosss.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-8, average='macro', reduction='mean', weight=None, gamma=2):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "        self.eps = eps\n",
    "        self.average = average\n",
    "        self.reduction = reduction\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        if average not in ['macro', 'micro']:\n",
    "            raise ValueError('average should be macro or micro.')\n",
    "        \n",
    "    def forward(self, pred, target): # same dimension\n",
    "        bce_loss = self.bce(pred.float(), target.float())\n",
    "        \n",
    "        # f1 loss\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        \n",
    "        tp = (target * pred).sum(dim=0).float()\n",
    "        fp = ((1 - target) * pred).sum(dim=0).float()\n",
    "        fn = (target * (1 - pred)).sum(dim=0).float()\n",
    "\n",
    "        if self.average == 'micro':\n",
    "            tp, fp, fn = tp.sum(), fp.sum(), fn.sum()\n",
    "        \n",
    "        f1 = tp / (tp + 0.5*(fp + fn) + self.eps).clamp(min=self.eps, max=1-self.eps)\n",
    "\n",
    "        return 1 - f1.mean() + bce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd775f9",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "183a9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          dataloaders,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          dataset_sizes,\n",
    "          device='cpu',\n",
    "          num_epochs=20,\n",
    "          #wandb_log=False,\n",
    "          early_stop_patience=None,\n",
    "          classify=True,\n",
    "          complete=True,\n",
    "          random_seed=1):\n",
    "\n",
    "    def _concatenate(running_v, new_v):\n",
    "        if running_v is not None:\n",
    "            return np.concatenate((running_v, new_v.clone().detach().cpu().numpy()), axis=0)\n",
    "        else:\n",
    "            return new_v.clone().detach().cpu().numpy()\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    torch.random.manual_seed(random_seed)\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    # Logit selector\n",
    "    #logit_selector = LogitSelector(rank=100).to(device)\n",
    "    \n",
    "    # BEST MODEL SAVING\n",
    "    best = {'loss': float('inf')}\n",
    "    if classify:\n",
    "        best['F1micro'] = -1.\n",
    "        best['F1macro'] = -1.\n",
    "        best['top5cls'] = -1.\n",
    "    if complete:\n",
    "        best['acc'] = -1.\n",
    "        best['top10cmp'] = -1.\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "\n",
    "    if early_stop_patience is not None:\n",
    "        if not isinstance(early_stop_patience, int):\n",
    "            raise TypeError('early_stop_patience should be an integer.')\n",
    "        patience_cnt = 0\n",
    "    \n",
    "    print('-' * 5 + 'Training the model' + '-' * 5)\n",
    "    for epoch in tqdm(range(1,num_epochs+1)):\n",
    "        print(f'\\nEpoch {epoch}/{num_epochs}')\n",
    "\n",
    "        val_loss = 0. # sum of classification and completion loss\n",
    "\n",
    "        # Each epoch has a training phase and two validation phases\n",
    "        for phase in ['train', 'valid_class', 'valid_compl']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                if not classify and phase == 'valid_class':\n",
    "                    continue\n",
    "                elif not complete and phase == 'valid_compl':\n",
    "                    continue\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss_class = 0.\n",
    "            running_corrects_class = 0.\n",
    "            running_labels_class = None\n",
    "            running_preds_class = None\n",
    "            running_top_k_class = 0.\n",
    "            \n",
    "            running_loss_compl = 0.\n",
    "            running_corrects_compl = 0.\n",
    "            running_top_k_compl = 0.\n",
    "            \n",
    "            dataset_name = phase\n",
    "            if phase == 'train':\n",
    "                dataset_name = 'train_class' if classify and not complete else 'train_compl'\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for idx, loaded_data in enumerate(dataloaders[dataset_name]):\n",
    "                if phase == 'train':\n",
    "                    if complete:\n",
    "                        bin_inputs, int_inputs, label_class, label_compl = loaded_data\n",
    "                    else:\n",
    "                        bin_inputs, int_inputs, label_class = loaded_data\n",
    "                elif phase == 'valid_class':\n",
    "                        bin_inputs, int_inputs, label_class = loaded_data\n",
    "                elif phase == 'valid_compl':\n",
    "                        bin_inputs, int_inputs, label_compl = loaded_data\n",
    "                \n",
    "                batch_size, num_items = bin_inputs.size()\n",
    "                if classify and phase in ['train', 'valid_class']:\n",
    "                    labels_class = label_class.to(device)\n",
    "                if complete and phase in ['train', 'valid_compl']:\n",
    "                    labels_compl = label_compl.to(device)\n",
    "                bin_inputs = bin_inputs.to(device)\n",
    "                int_inputs = int_inputs.to(device)\n",
    "                    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs_class, outputs_compl = model(int_inputs, bin_x=bin_inputs)\n",
    "                    new_outputs_compl, new_labels_compl = None, None\n",
    "                    if classify and phase in ['train', 'valid_class']:\n",
    "                        _, preds_class = torch.max(outputs_class, 1)\n",
    "                    if complete and phase in ['train', 'valid_compl']:\n",
    "                        _, preds_compl = torch.max(outputs_compl, 1)\n",
    "                        #new_outputs_compl, new_labels_compl = logit_selector(outputs_compl, labels_compl)\n",
    "                        \n",
    "                    \"\"\"if idx == 0:\n",
    "                        if classify and phase in ['train', 'valid_class']:\n",
    "                            print('labels_classification', labels_class.cpu().numpy())\n",
    "                            print('preds_classification', preds_class.cpu().numpy())\n",
    "                        if complete and phase in ['train', 'valid_compl']:\n",
    "                            if new_labels_compl is not None:\n",
    "                                print('new label', new_labels_compl.cpu().numpy())\n",
    "                            print('labels_completion', labels_compl.cpu().numpy())\n",
    "                            print('preds_completion', preds_compl.cpu().numpy())\"\"\"\n",
    "                    \n",
    "                    if classify and phase in ['train', 'valid_class']:\n",
    "                        loss_class = criterion(outputs_class, labels_class.long())\n",
    "                    if complete and phase in ['train', 'valid_compl']:\n",
    "                        if new_outputs_compl is None:\n",
    "                            loss_compl = criterion(outputs_compl, labels_compl.long())\n",
    "                        else:\n",
    "                            loss_compl = criterion(new_outputs_compl, new_labels_compl)\n",
    "\n",
    "                    if classify and complete and phase == 'train':\n",
    "                        loss = loss_class + loss_compl\n",
    "                    elif classify and phase in ['train', 'valid_class']:\n",
    "                        loss = loss_class\n",
    "                    elif complete and phase in ['train', 'valid_compl']:\n",
    "                        loss = loss_compl\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1) # gradient clipping\n",
    "                        optimizer.step()\n",
    "\n",
    "                if idx % 100 == 0 and phase == 'train':\n",
    "                    log_str = f'    {phase} {idx * 100 // len(dataloaders[dataset_name]):3d}% of an epoch | '\n",
    "                    if classify and phase in ['train', 'valid_class']:\n",
    "                        log_str += f'Loss(classif.): {loss_class.item():.4f} | '\n",
    "                    if complete and phase in ['train', 'valid_compl']:\n",
    "                        log_str += f'Loss(complet.): {loss_compl.item():.4f} | '\n",
    "                    print(log_str)\n",
    "\n",
    "                # statistics\n",
    "                if classify and phase in ['train', 'valid_class']: # for F1 score\n",
    "                    running_loss_class += loss_class.item() * batch_size\n",
    "                    running_labels_class = _concatenate(running_labels_class, labels_class)\n",
    "                    running_preds_class = _concatenate(running_preds_class, preds_class)\n",
    "                    running_top_k_class += top_k_accuracy_score(labels_class.cpu().numpy(), outputs_class.detach().cpu().numpy(), k=5, labels=np.arange(outputs_class.size(1)), normalize=False)\n",
    "                if complete and phase in ['train', 'valid_compl']: # for accuracy\n",
    "                    running_loss_compl += loss_compl.item() * batch_size\n",
    "                    running_corrects_compl += torch.sum(preds_compl == labels_compl)\n",
    "                    running_top_k_compl += top_k_accuracy_score(labels_compl.cpu().numpy(), outputs_compl.detach().cpu().numpy(), k=10, labels=np.arange(outputs_compl.size(1)), normalize=False)\n",
    "\n",
    "            epoch_loss = 0.\n",
    "            log_str = f'{phase.upper()} | '\n",
    "            if classify and phase in ['train', 'valid_class']:\n",
    "                epoch_loss_class = running_loss_class / dataset_sizes[dataset_name]\n",
    "                epoch_loss += epoch_loss_class\n",
    "                running_labels_class = torch.from_numpy(running_labels_class)\n",
    "                running_preds_class = torch.from_numpy(running_preds_class)\n",
    "                epoch_macro_f1 = f1_score(running_labels_class, running_preds_class, average='macro')  # classification: f1 scores.\n",
    "                epoch_micro_f1 = f1_score(running_labels_class, running_preds_class, average='micro')  # micro f1 score == accuracy, for single-label classification.\n",
    "                epoch_top_k_class = running_top_k_class / dataset_sizes[dataset_name]\n",
    "                log_str += f'Loss(classif.): {epoch_loss_class:.3f} Macro-F1: {epoch_macro_f1:.3f} Micro-F1: {epoch_micro_f1:.3f} Top-5 Acc: {epoch_top_k_class:.3f} | '\n",
    "            if complete and phase in ['train', 'valid_compl']:\n",
    "                epoch_loss_compl = running_loss_compl / dataset_sizes[dataset_name]\n",
    "                epoch_loss += epoch_loss_compl\n",
    "                epoch_acc_compl = running_corrects_compl / dataset_sizes[dataset_name]  # completion task: accuracy.\n",
    "                epoch_top_k_compl = running_top_k_compl / dataset_sizes[dataset_name]\n",
    "                log_str += f'Loss(complet.): {epoch_loss_compl:.3f} Acc(complet.): {epoch_acc_compl:.3f} Top-10 Acc: {epoch_top_k_compl:.3f} | '\n",
    "            print(log_str)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = epoch_loss\n",
    "                if classify:\n",
    "                    train_macro_f1 = epoch_macro_f1\n",
    "                    train_micro_f1 = epoch_micro_f1\n",
    "                    train_top_k_class = epoch_top_k_class\n",
    "                if complete:\n",
    "                    train_acc = epoch_acc_compl\n",
    "                    train_top_k_compl = epoch_top_k_compl\n",
    "                # if wandb_log:\n",
    "                #     wandb.watch(model)\n",
    "            elif 'val' in phase:\n",
    "                val_loss += epoch_loss\n",
    "                if classify and phase == 'valid_class':\n",
    "                    val_macro_f1 = epoch_macro_f1\n",
    "                    val_micro_f1 = epoch_micro_f1\n",
    "                    val_top_k_class = epoch_top_k_class\n",
    "                if complete and phase == 'valid_compl':\n",
    "                    val_acc = epoch_acc_compl\n",
    "                    val_top_k_compl = epoch_top_k_compl\n",
    "        \n",
    "        scheduler.step(-val_micro_f1 if classify and not complete else -val_acc)\n",
    "        is_new_best = val_micro_f1 > best['F1micro'] if classify and not complete else val_acc > best['acc']\n",
    "        if is_new_best:\n",
    "            best['bestEpoch'] = int(epoch)\n",
    "            best['loss'] = val_loss\n",
    "            if classify:\n",
    "                best['F1micro'] = val_micro_f1\n",
    "                best['F1macro'] = val_macro_f1\n",
    "                best['top5cls'] = val_top_k_class\n",
    "            if complete:\n",
    "                best['acc'] = val_acc\n",
    "                best['top10cmp'] = val_top_k_compl\n",
    "            best_model_wts = deepcopy(model.state_dict()) # deep copy the model\n",
    "            if early_stop_patience is not None:\n",
    "                patience_cnt = 0\n",
    "        elif early_stop_patience is not None:\n",
    "            patience_cnt += 1\n",
    "\n",
    "        \"\"\"\n",
    "        if wandb_log:\n",
    "            wandb.log({'train_loss': train_loss,\n",
    "                       'val_loss': val_loss,\n",
    "                       'train_macro_f1': train_macro_f1,\n",
    "                       'train_micro_f1': train_micro_f1,\n",
    "                       'val_macro_f1': val_macro_f1,\n",
    "                       'val_micro_f1': val_micro_f1,\n",
    "                       'best_val_loss': best['loss'],\n",
    "                       'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "                                        # scheduler.get_last_lr()[0] for CosineAnnealingWarmRestarts\n",
    "        \"\"\"\n",
    "        if early_stop_patience is not None:\n",
    "            if patience_cnt > early_stop_patience:\n",
    "                print(f'Early stop at epoch {epoch}.')\n",
    "                break\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('==== Best Result ====')\n",
    "    for k in best:\n",
    "        if k == 'bestEpoch':\n",
    "            print(f\"{k}: {int(best[k])}\")\n",
    "        else:\n",
    "            print(f\"{k}: {float(best[k]):.8f}\")\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0c5eb",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "28f82046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'y']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(x=1, y=2):\n",
    "    print(x+y)\n",
    "list({'x':3, 'y':6}.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5c6d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(dim_embedding=256,\n",
    "               dim_hidden=128,\n",
    "               dropout=0.5,\n",
    "               subset_length=None,\n",
    "               encoder_mode='deep_sets',\n",
    "               enc_pool_mode='set_transformer',\n",
    "               decoder_mode='simple',\n",
    "               num_enc_layers=4,\n",
    "               num_dec_layers=4,\n",
    "               batch_size=16,\n",
    "               n_epochs=50,\n",
    "               loss='ASLoss',\n",
    "               opt='AdamW',\n",
    "               lr=1e-3,\n",
    "               step_size=10,  # lr_scheduler\n",
    "               step_factor=0.1, # lr_scheduler\n",
    "               patience=20,   # early stop\n",
    "               seed=0,\n",
    "               classify=True,\n",
    "               complete=True,\n",
    "               freeze_classify=False,\n",
    "               freeze_complete=False,\n",
    "               pretrained_model_path=None\n",
    "               ):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    \n",
    "    train_data_name = 'train_class' if classify and not complete else 'train_compl'\n",
    "    dataset_names = [train_data_name, 'valid_class', 'valid_compl']\n",
    "    subset_indices = {x: [i for i in range(len(recipe_datasets[x]) if subset_length is None else subset_length)\n",
    "                          ] for x in dataset_names}\n",
    "    dataloaders = {x: DataLoader(Subset(recipe_datasets[x], subset_indices[x]),\n",
    "                                 batch_size=batch_size, shuffle=('train' in x)) for x in dataset_names}\n",
    "    dataset_sizes = {x: len(subset_indices[x]) for x in dataset_names}\n",
    "    print(dataset_sizes)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('device: ', device)\n",
    "\n",
    "    # Get a batch of training data\n",
    "    loaded_data = next(iter(dataloaders[train_data_name]))\n",
    "    print('bin_inputs, int_inputs, *labels:', [x.shape for x in loaded_data])\n",
    "\n",
    "    model_ft = CCNet(dim_embedding=dim_embedding, dim_output=20, dim_hidden=dim_hidden,\n",
    "                     num_items=len(loaded_data[0][0]), num_enc_layers=num_enc_layers, num_dec_layers=num_dec_layers,\n",
    "                     ln=True, dropout=dropout, encoder_mode=encoder_mode, enc_pool_mode=enc_pool_mode, decoder_mode=decoder_mode,\n",
    "                     classify=classify, complete=complete, freeze_classify=freeze_classify, freeze_complete=freeze_complete).to(device)\n",
    "    if pretrained_model_path is not None:\n",
    "        pretrained_dict = torch.load(pretrained_model_path)\n",
    "        model_dict = model_ft.state_dict()\n",
    "        \n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict) \n",
    "        # 3. load the new state dict\n",
    "        model_ft.load_state_dict(model_dict)\n",
    "        \n",
    "    # Model Info\n",
    "    #print(model_ft)\n",
    "    total_params = sum(dict((p.data_ptr(), p.numel()) for p in model_ft.parameters() if p.requires_grad).values())\n",
    "    print(\"Total Number of Parameters\", total_params)\n",
    "\n",
    "    # Loss, Optimizer, LR Scheduler\n",
    "    LOSSES = {\n",
    "        'CrossEntropyLoss': nn.CrossEntropyLoss,\n",
    "        'FocalLoss': MultiClassFocalLoss,\n",
    "        'ASLoss': MultiClassASLoss,\n",
    "    }\n",
    "    OPTIMIZERS = {\n",
    "        'SGD': optim.SGD,\n",
    "        'MomentumSGD': optim.SGD,\n",
    "        'NestrovSGD': optim.SGD,\n",
    "        'Adam': optim.Adam,\n",
    "        'AdamW': optim.AdamW,\n",
    "    }\n",
    "    OPTIMIZERS_ARG = {\n",
    "        'SGD': {'lr':lr, 'weight_decay':0.2},\n",
    "        'MomentumSGD': {'lr':lr, 'weight_decay':0.2, 'momentum':0.9},\n",
    "        'NestrovSGD': {'lr':lr, 'weight_decay':0.2, 'momentum':0.9, 'nesterov':True},\n",
    "        'Adam': {'lr':lr, 'weight_decay':0.2},\n",
    "        'AdamW': {'lr':lr, 'weight_decay':0.2},\n",
    "    }\n",
    "    criterion = LOSSES[loss]().to(device)\n",
    "    optimizer = OPTIMIZERS[opt]([p for p in model_ft.parameters() if p.requires_grad == True], **OPTIMIZERS_ARG[opt])\n",
    "    exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=step_factor, patience=step_size, verbose=True)\n",
    "\n",
    "    model_ft, best = train(model_ft, dataloaders, criterion, optimizer, exp_lr_scheduler,\n",
    "                           dataset_sizes, device=device, num_epochs=n_epochs, early_stop_patience=patience,\n",
    "                           classify=classify, complete=complete, random_seed=seed)\n",
    "\n",
    "    fname = ['ckpt', 'CCNet']\n",
    "    if classify:\n",
    "        fname.append('cls')\n",
    "    if complete:\n",
    "        fname.append('cmp')\n",
    "    for k in best:\n",
    "        if k == 'bestEpoch':\n",
    "            fname.append(f'bestEpoch{int(best[k]):2d}')\n",
    "        else:\n",
    "            fname += [f\"{k}{float(best[k]):.4f}\"]\n",
    "    fname += [f'bs{batch_size}',f'lr{lr}', f'seed{seed}',f'nEpochs{n_epochs}',]\n",
    "    fname += ['encoder', encoder_mode, 'encPool', enc_pool_mode]\n",
    "    if complete:\n",
    "        fname += ['decoder', decoder_mode]\n",
    "    fname = '_'.join(fname) + '.pt'\n",
    "    if not os.path.isdir('./weights/'):\n",
    "        os.mkdir('./weights/')\n",
    "    torch.save(model_ft.state_dict(), os.path.join('./weights/', fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e799b4",
   "metadata": {},
   "source": [
    "### Classification Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720708da",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(classify=True, complete=False, batch_size=128, n_epochs=1, lr=1e-3, encoder_mode='deep_sets', enc_pool_mode='deep_sets', num_enc_layers=4, num_dec_layers=4, dim_embedding=256, dim_hidden=256, dropout=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14745a",
   "metadata": {},
   "source": [
    "### Completion Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbfcaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(classify=False, complete=True, batch_size=1024, n_epochs=1, lr=1e-4, encoder_mode='deep_sets', enc_pool_mode='deep_sets', num_enc_layers=4, decoder_mode='simple', num_dec_layers=4, dim_embedding=256, dim_hidden=256, dropout=0.2, subset_length=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0027ddcc",
   "metadata": {},
   "source": [
    "### Classification + Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(classify=True, complete=True, batch_size=1024, n_epochs=100, lr=1e-4, encoder_mode='deep_sets', enc_pool_mode='deep_sets', num_enc_layers=2, decoder_mode='simple', num_dec_layers=2, dim_embedding=256, dim_hidden=256, dropout=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dce140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Full on Python 3.7 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
