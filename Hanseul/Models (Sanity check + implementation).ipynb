{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb27b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# h5py 안 될 때\n",
    "!brew reinstall hdf5\n",
    "!export CPATH=\"/opt/homebrew/include/\"\n",
    "!export HDF5_DIR=/opt/homebrew/\n",
    "!python3 -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ba75710c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e31a79ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_root = '../'\n",
    "path_container = './Container/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bed3b7",
   "metadata": {},
   "source": [
    "### Dataset 가져오기 (1) dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1e2e1250",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'brazilian', 1: 'british', 2: 'cajun_creole', 3: 'chinese', 4: 'filipino', 5: 'french', 6: 'greek', 7: 'indian', 8: 'irish', 9: 'italian', 10: 'jamaican', 11: 'japanese', 12: 'korean', 13: 'mexican', 14: 'moroccan', 15: 'russian', 16: 'southern_us', 17: 'spanish', 18: 'thai', 19: 'vietnamese'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict, 20)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path_container + 'id_cuisine_dict.pickle', 'rb') as f:\n",
    "    id_cuisine_dict = pickle.load(f)\n",
    "\n",
    "print(id_cuisine_dict)\n",
    "type(id_cuisine_dict), len(id_cuisine_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4c047dc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brazilian': 0, 'british': 1, 'cajun_creole': 2, 'chinese': 3, 'filipino': 4, 'french': 5, 'greek': 6, 'indian': 7, 'irish': 8, 'italian': 9, 'jamaican': 10, 'japanese': 11, 'korean': 12, 'mexican': 13, 'moroccan': 14, 'russian': 15, 'southern_us': 16, 'spanish': 17, 'thai': 18, 'vietnamese': 19}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict, 20)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path_container + 'cuisine_id_dict.pickle', 'rb') as f:\n",
    "    cuisine_id_dict = pickle.load(f)\n",
    "\n",
    "print(cuisine_id_dict)\n",
    "type(cuisine_id_dict), len(cuisine_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "da975806",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Franks Hot Sauce\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict, 6714)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path_container + 'id_ingredient_dict.pickle', 'rb') as f:\n",
    "    id_ingredient_dict = pickle.load(f)\n",
    "\n",
    "print(id_ingredient_dict[6694])\n",
    "type(id_ingredient_dict), len(id_ingredient_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e7658b76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "561\n",
      "[698, 1039, 4315]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict, 6694)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path_container + 'ingredient_id_dict.pickle', 'rb') as f:\n",
    "    ingredient_id_dict = pickle.load(f)\n",
    "\n",
    "print(ingredient_id_dict['lemon grass'])\n",
    "print(ingredient_id_dict['lemongrass'])\n",
    "print(ingredient_id_dict['bread'])\n",
    "type(ingredient_id_dict), len(ingredient_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb04258e",
   "metadata": {},
   "source": [
    "### Datastet 가져오기 (2) h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0f315",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "_train = [arr_data_train, bin_data_train, int_labels_train, bin_labels_train]\n",
    "_valid_class = [arr_data_valid_class, bin_data_valid_class, int_labels_valid_class, bin_labels_valid_class]\n",
    "_valid_compl = [arr_data_valid_compl, bin_data_valid_compl, int_labels_valid_compl, bin_labels_valid_compl]\n",
    "_test_class = [arr_data_test_class, bin_data_test_class, None, None]\n",
    "_test_compl = [arr_data_test_compl, bin_data_test_compl, None, None]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "80f9a9ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23547, 65)\n",
      "(23547, 6714)\n",
      "(23547,)\n",
      "(23547, 20)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_container + 'train', 'r') as f:\n",
    "    arr_data_train = f['arr_data'][:]\n",
    "    bin_data_train = f['bin_data'][:]\n",
    "    int_labels_train = f['int_labels'][:]\n",
    "    bin_labels_train = f['bin_labels'][:]\n",
    "\n",
    "print(arr_data_train.shape)\n",
    "print(bin_data_train.shape)\n",
    "print(int_labels_train.shape)\n",
    "print(bin_labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "76265323",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 65)\n",
      "(7848, 6714)\n",
      "(7848,)\n",
      "(7848, 20)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_container + 'valid_class', 'r') as f:\n",
    "    arr_data_valid_class = f['arr_data'][:]\n",
    "    bin_data_valid_class = f['bin_data'][:]\n",
    "    int_labels_valid_class = f['int_labels'][:]\n",
    "    bin_labels_valid_class = f['bin_labels'][:]\n",
    "\n",
    "print(arr_data_valid_class.shape)\n",
    "print(bin_data_valid_class.shape)\n",
    "print(int_labels_valid_class.shape)\n",
    "print(bin_labels_valid_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ec2a1500",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 65)\n",
      "(7848, 6714)\n",
      "(7848,)\n",
      "(7848, 6714)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_container + 'valid_compl', 'r') as f:\n",
    "    arr_data_valid_compl = f['arr_data'][:]\n",
    "    bin_data_valid_compl = f['bin_data'][:]\n",
    "    int_labels_valid_compl = f['int_labels'][:]\n",
    "    bin_labels_valid_compl = f['bin_labels'][:]\n",
    "\n",
    "print(arr_data_valid_compl.shape)\n",
    "print(bin_data_valid_compl.shape)\n",
    "print(int_labels_valid_compl.shape)\n",
    "print(bin_labels_valid_compl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2af74861",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3924, 65)\n",
      "(3924, 6714)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_container + 'test_class', 'r') as f:\n",
    "    arr_data_test_class = f['arr_data'][:]\n",
    "    bin_data_test_class = f['bin_data'][:]\n",
    "\n",
    "print(arr_data_test_class.shape)\n",
    "print(bin_data_test_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4f48f265",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3924, 65)\n",
      "(3924, 6714)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_container + 'test_compl', 'r') as f:\n",
    "    arr_data_test_compl = f['arr_data'][:]\n",
    "    bin_data_test_compl = f['bin_data'][:]\n",
    "\n",
    "print(arr_data_test_compl.shape)\n",
    "print(bin_data_test_compl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0af4949c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -1로 padding한 것을 6714로 갈아치우기 - for Embedding layer\n",
    "pad_idx = 6714\n",
    "\n",
    "arr_data_train[arr_data_train == -1] = pad_idx\n",
    "arr_data_valid_class[arr_data_valid_class == -1] = pad_idx\n",
    "arr_data_valid_compl[arr_data_valid_compl == -1] = pad_idx\n",
    "arr_data_test_class[arr_data_test_class == -1] = pad_idx\n",
    "arr_data_test_compl[arr_data_test_compl == -1] = pad_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc43311",
   "metadata": {},
   "source": [
    "### Ingredient feature vectors (nn.Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "250e6c67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2813, 3146, 3229, 3885, 4379, 4390, 5250, 5456, 6187, 6714, 6714,\n",
       "       6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "       6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "       6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "       6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "       6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "45b005ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.5085,  1.3205, -0.5292,  ..., -1.7537, -0.2567, -1.2333],\n",
      "        [ 0.4654,  0.8227, -1.6501,  ..., -1.0737,  0.7688, -0.6282],\n",
      "        [ 0.5623, -0.7719,  0.6491,  ...,  1.9821,  0.3590, -1.4031],\n",
      "        ...,\n",
      "        [-0.8299,  0.3412, -0.0203,  ..., -0.2970,  0.5006, -0.1194],\n",
      "        [-0.8445, -0.4461, -0.4429,  ..., -0.7884,  0.5765, -1.4849],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       requires_grad=True)\n",
      "torch.Size([6715, 256])\n"
     ]
    }
   ],
   "source": [
    "num_ingredients = pad_idx +1  # = len(id_ingredient_dict) + 1  # +1 for padding (last)\n",
    "max_length = 65  # maximum num of ingredients per recipe\n",
    "embedding_dim = 256\n",
    "\n",
    "\n",
    "Embed = nn.Embedding(num_embeddings=num_ingredients, embedding_dim=embedding_dim, padding_idx=-1)\n",
    "x = torch.LongTensor(arr_data_train[:2])\n",
    "\n",
    "print(Embed.weight)\n",
    "print(Embed.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "0c15ee47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5085,  1.3205, -0.5292, -0.8147, -0.2471,  1.6666, -1.7047,  1.5168,\n",
      "        -0.9718, -2.2025,  0.7877,  1.6499,  0.1197,  1.1889,  1.6560,  2.8253,\n",
      "         1.6521,  0.9913, -0.4929, -1.4329, -1.5740,  1.7747, -0.6462,  1.5872,\n",
      "        -0.0715, -0.9060, -0.5537,  2.2482,  0.4627,  1.0123, -0.9640,  0.4060,\n",
      "         1.2748, -2.3461,  0.1684,  0.0468, -0.7596,  0.7578,  0.0778, -0.4288,\n",
      "         0.9581, -0.9092,  0.0042,  0.5394, -0.5707, -0.7965, -0.5663,  0.1300,\n",
      "        -0.7374,  0.0830,  0.8134, -1.0099, -0.5603, -1.0637, -1.3713,  0.8295,\n",
      "         1.0266,  1.3354,  0.3258, -0.0153,  1.2029,  0.0743,  1.3934, -1.1238,\n",
      "         1.3378,  0.9338, -0.2577,  0.1934, -0.7783,  0.4122, -0.7936, -1.0152,\n",
      "         0.3274, -1.5270,  1.9012, -0.0071,  1.3207, -1.5196, -1.0794,  0.3727,\n",
      "         1.9110,  0.3411,  0.9885, -0.0055,  1.1217,  0.0272,  0.6174, -0.5513,\n",
      "        -0.1308,  1.1469,  0.5315,  0.0492, -1.4430,  0.3096,  0.1211,  1.5994,\n",
      "         2.0192, -0.9977,  0.9147,  0.0915,  0.0400,  0.7268, -0.5319, -1.3661,\n",
      "         0.6064, -0.9641,  0.9737,  0.6473, -0.3613,  0.5048,  0.6635,  1.1642,\n",
      "         0.2651, -0.2478, -0.6960, -0.5226, -0.0905,  0.9965, -0.4892,  1.7284,\n",
      "        -0.4490,  1.0644,  1.1483, -0.0312,  0.6227,  1.8247,  0.8160, -1.6930,\n",
      "        -1.2225,  0.5263,  0.4803, -1.2768,  0.9978,  1.1216,  1.5030,  0.8110,\n",
      "         1.1797,  0.2285,  0.4106,  1.8668, -0.7385,  0.1695,  0.0479,  0.5647,\n",
      "         0.8973, -0.1239,  1.6482,  0.8189,  0.5373, -0.8673, -0.6300,  0.4666,\n",
      "        -1.9742,  0.7643,  0.9698, -0.7051,  0.5834,  0.4148, -0.6764, -0.4813,\n",
      "         0.4539, -2.3518,  0.9027, -0.0433,  1.2515, -1.4709,  0.7362,  1.1198,\n",
      "         0.7014, -1.0773, -0.2614,  1.5502,  2.2245,  1.6304, -0.4747,  0.7565,\n",
      "        -1.3871,  0.3743, -1.4587,  0.3046, -0.5693,  3.1432,  0.2107,  0.4664,\n",
      "        -1.2840, -0.2108,  0.8242,  1.1462, -1.9413, -0.2080, -0.0625,  1.4205,\n",
      "        -1.2753, -0.6955,  1.3244, -0.0403, -0.1931,  0.0765,  1.1321,  0.5624,\n",
      "         0.8751, -0.3542, -0.8758, -1.2227,  0.6849,  0.2680,  0.5957,  0.2474,\n",
      "        -1.5402,  0.6161, -1.6173,  0.0745,  1.3992,  0.3554,  0.3120, -0.9771,\n",
      "         2.3367,  1.4935, -0.7900, -2.7622, -1.7051,  0.1687,  0.9963, -0.5398,\n",
      "         0.1895,  1.3910, -1.1481, -1.0700,  0.1223, -0.7065, -0.9108,  0.9382,\n",
      "         0.4802, -1.1634,  0.3129, -0.9811, -0.6318,  1.3223, -0.6718, -0.9872,\n",
      "         1.1629,  0.4426,  1.6913,  0.1678, -0.4565, -0.3326,  1.1855, -0.0214,\n",
      "         0.7770, -0.6984, -0.0086, -1.2992, -1.9634, -1.7537, -0.2567, -1.2333],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(Embed.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "8fd45a6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2813, 3146, 3229, 3885, 4379, 4390, 5250, 5456, 6187, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714],\n",
      "        [ 392,  937, 1476, 2172, 2351, 2813, 3350, 3554, 3857, 3978, 5249, 5648,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714]])\n",
      "torch.Size([2, 65, 256])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "with torch.no_grad():\n",
    "    print(Embed(x).size())\n",
    "    print(Embed(x)[0][11]) # zero padded feature matrix! (0번째 recipe는 9개라 10~65번째 feature vector는 zeros.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d82e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fe16b36",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "Set transformer: ( https://github.com/juho-lee/set_transformer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "fe4f440c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Building blocks of Set Transformers ##\n",
    "# added masks.\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Sequential(\n",
    "            nn.Linear(dim_V, 2*dim_V),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*dim_V, dim_V))\n",
    "\n",
    "    def forward(self, Q, K, mask=None):\n",
    "        # Q (batch, q_len, d_hid)\n",
    "        # K (batch, k_len, d_hid)\n",
    "        # V (batch, v_len, d_hid == dim_V)\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "        \n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        \n",
    "        # Q_ (batch * num_heads, q_len, d_hid // num_heads)\n",
    "        # K_ (batch * num_heads, k_len, d_hid // num_heads)\n",
    "        # V_ (batch * num_heads, v_len, d_hid // num_heads)\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "        \n",
    "        # energy (batch * num_heads, q_len, k_len)\n",
    "        energy = Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V)\n",
    "        if mask is not None:\n",
    "            energy.masked_fill_(mask, float('-inf'))\n",
    "        A = torch.softmax(energy, 2)\n",
    "        \n",
    "        # O (batch, q_len, d_hid)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
    "        O = O + self.fc_o(O)\n",
    "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(X, X, mask=mask)\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X, mask=mask)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
    "        \n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "c70a1665",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_input,\n",
    "                 num_inds=32, dim_hidden=128, num_heads=4, num_layers=2, ln=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc = nn.ModuleList(\n",
    "            [ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln)] +\n",
    "            [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln) for _ in range(num_layers-1)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, X, mask=None):\n",
    "        out = X.detach()\n",
    "        for module in self.enc:\n",
    "            out = module(out, mask=mask)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_outputs,\n",
    "                 num_inds=32, dim_hidden=128, num_heads=4, ln=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec = nn.Sequential(\n",
    "                PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "                SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "                SAB(dim_hidden, dim_hidden, num_heads, ln=ln))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.dec(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb796399",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "660267cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size torch.Size([2, 65])\n",
      "input tensor([[2813, 3146, 3229, 3885, 4379, 4390, 5250, 5456, 6187, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714],\n",
      "        [ 392,  937, 1476, 2172, 2351, 2813, 3350, 3554, 3857, 3978, 5249, 5648,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714]])\n",
      "feature size torch.Size([2, 65, 256])\n",
      "feature tensor([[[-0.6592,  0.5575,  0.4495,  ..., -0.3449,  0.2993,  1.7724],\n",
      "         [ 2.2880,  0.3431,  1.1873,  ...,  0.9160, -0.3171, -1.3607],\n",
      "         [-2.1151, -1.4350,  0.3835,  ..., -0.3619, -0.4009,  2.0794],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.1562,  0.1832,  1.7256,  ...,  0.5019, -0.3480,  0.1813],\n",
      "         [ 1.6066, -0.6828,  0.7747,  ...,  0.3529, -0.1757, -1.8978],\n",
      "         [ 0.7475,  0.8273, -1.4289,  ..., -1.1378,  1.5095, -0.9886],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.LongTensor(arr_data_train[:2])\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('input size', x.size())\n",
    "    print('input', x)\n",
    "    feature = Embed(x)  # Size 2, 65, 256\n",
    "    print('feature size', feature.size())\n",
    "    print('feature', feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "f0d5b797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 65]),\n",
       " tensor([[[False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]]]))"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_mask = (x == pad_idx).unsqueeze(1).repeat(4,1,1)\n",
    "enc_mask.size(), enc_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "37eca200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 size torch.Size([2, 65])\n",
      "x1 tensor([[3146, 2813, 3229, 3885, 4379, 4390, 5250, 5456, 6187, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714],\n",
      "        [ 392,  937, 1476, 2172, 2351, 2813, 3350, 3554, 3857, 3978, 5249, 5648,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714]])\n",
      "feature1 tensor([[[ 2.2880,  0.3431,  1.1873,  ...,  0.9160, -0.3171, -1.3607],\n",
      "         [-0.6592,  0.5575,  0.4495,  ..., -0.3449,  0.2993,  1.7724],\n",
      "         [-2.1151, -1.4350,  0.3835,  ..., -0.3619, -0.4009,  2.0794],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.1562,  0.1832,  1.7256,  ...,  0.5019, -0.3480,  0.1813],\n",
      "         [ 1.6066, -0.6828,  0.7747,  ...,  0.3529, -0.1757, -1.8978],\n",
      "         [ 0.7475,  0.8273, -1.4289,  ..., -1.1378,  1.5095, -0.9886],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "x1 = deepcopy(x)\n",
    "x1[0][:2] = x[0][[1,0]]\n",
    "print('x1 size', x1.size())\n",
    "print('x1', x1)\n",
    "with torch.no_grad():\n",
    "    feature1 = Embed(x1)\n",
    "    print('feature1', feature1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "4c52c160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_sab = SAB(256, 128, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "5e915b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sab(feature) tensor([[[ 0.5165,  0.3423, -0.7918,  ...,  0.3747, -0.4377, -0.1863],\n",
      "         [ 0.1475, -1.1637, -0.1631,  ..., -0.8286, -0.3840, -0.5151],\n",
      "         [ 0.6952, -0.4090,  0.6754,  ..., -0.3958, -0.4861,  0.0334],\n",
      "         ...,\n",
      "         [-0.0704, -0.2253, -0.2671,  ..., -0.0589, -0.0949, -0.1920],\n",
      "         [-0.0704, -0.2253, -0.2671,  ..., -0.0589, -0.0949, -0.1920],\n",
      "         [-0.0704, -0.2253, -0.2671,  ..., -0.0589, -0.0949, -0.1920]],\n",
      "\n",
      "        [[-0.0725,  0.1918,  0.5322,  ..., -0.2370, -1.3910,  0.1049],\n",
      "         [-0.4460, -0.8585,  0.2142,  ..., -0.0111, -1.0845,  0.3749],\n",
      "         [-1.1908, -0.5597,  1.1593,  ...,  0.9088,  0.3672, -0.1515],\n",
      "         ...,\n",
      "         [ 0.0911, -0.1334,  0.0170,  ..., -0.0028, -0.1610,  0.1357],\n",
      "         [ 0.0911, -0.1334,  0.0170,  ..., -0.0028, -0.1610,  0.1357],\n",
      "         [ 0.0911, -0.1334,  0.0170,  ..., -0.0028, -0.1610,  0.1357]]])\n",
      "torch.Size([2, 65, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z = _sab(feature, mask=enc_mask)\n",
    "    print('sab(feature)',z)\n",
    "    print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "23faf472",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_enc = Encoder(256, dim_hidden=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "9a835b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 65, 256])\n",
      "tensor([[ 0.0304, -0.0662, -0.0070,  ..., -0.0596, -0.2768,  0.2174],\n",
      "        [ 0.0304, -0.0662, -0.0070,  ..., -0.0596, -0.2768,  0.2174],\n",
      "        [ 0.0304, -0.0662, -0.0070,  ..., -0.0596, -0.2768,  0.2174],\n",
      "        ...,\n",
      "        [ 0.0304, -0.0662, -0.0070,  ..., -0.0596, -0.2768,  0.2174],\n",
      "        [ 0.0304, -0.0662, -0.0070,  ..., -0.0596, -0.2768,  0.2174],\n",
      "        [ 0.0304, -0.0662, -0.0070,  ..., -0.0596, -0.2768,  0.2174]])\n",
      "torch.Size([2, 65, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    code = _enc(feature,mask=enc_mask)\n",
    "    print(feature.size())\n",
    "    print(code[0][15:])\n",
    "    print(code.size())  # Size 2, 65, dim_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "68c57696",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3653, -0.2419,  0.2929,  ...,  0.2913, -0.6550,  0.6975],\n",
      "         [-0.2268, -0.2857, -0.2599,  ...,  0.4555,  0.0017,  0.2129],\n",
      "         [ 0.1202, -0.3273,  0.1191,  ..., -0.6078, -0.1978,  0.2386],\n",
      "         ...,\n",
      "         [ 0.0304, -0.0662, -0.0070,  ..., -0.0596, -0.2768,  0.2174],\n",
      "         [ 0.0304, -0.0662, -0.0070,  ..., -0.0596, -0.2768,  0.2174],\n",
      "         [ 0.0304, -0.0662, -0.0070,  ..., -0.0596, -0.2768,  0.2174]],\n",
      "\n",
      "        [[-0.2721, -0.4374,  0.3580,  ..., -0.3973, -0.2365, -0.1932],\n",
      "         [-0.3750, -0.1942,  0.2701,  ..., -0.1424,  0.4709,  0.1798],\n",
      "         [-0.1730, -0.0925, -0.1718,  ..., -0.2075,  1.1529,  0.8532],\n",
      "         ...,\n",
      "         [-0.2592, -0.1330, -0.0783,  ..., -0.1548,  0.0371,  0.1877],\n",
      "         [-0.2592, -0.1330, -0.0783,  ..., -0.1548,  0.0371,  0.1877],\n",
      "         [-0.2592, -0.1330, -0.0783,  ..., -0.1548,  0.0371,  0.1877]]])\n",
      "torch.Size([2, 65, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    code1 = _enc(feature1, mask=enc_mask)\n",
    "    print(code1)\n",
    "    print(code1.size())  # Size 2, 65, dim_hidden  # permutation equivariant! (observe first 2 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "d59ebf3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_dec = Decoder(1, dim_hidden=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "7422dc22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1508, -0.2508, -0.1915,  0.3444, -0.2414, -0.0401,  0.1389,\n",
      "           0.0513, -0.1571,  0.2136,  0.0504, -0.0411,  0.0559,  0.0982,\n",
      "           0.1302, -0.0359,  0.1272, -0.2365,  0.3331,  0.1819,  0.2597,\n",
      "          -0.0376,  0.1688,  0.1096, -0.1450,  0.0516, -0.0742,  0.0605,\n",
      "           0.0728, -0.0476,  0.1194, -0.0662, -0.1504,  0.0394,  0.0018,\n",
      "          -0.3478,  0.1906, -0.0150,  0.3143, -0.3142, -0.1716, -0.1427,\n",
      "           0.3266, -0.0731,  0.0844,  0.0579, -0.0226,  0.0569, -0.0264,\n",
      "           0.1366, -0.0414, -0.0115, -0.1819, -0.1998, -0.0806, -0.2261,\n",
      "          -0.1732, -0.1733,  0.1483,  0.0928, -0.0470, -0.2604, -0.0687,\n",
      "          -0.0596, -0.1244, -0.0706,  0.1617,  0.3518, -0.0511,  0.1004,\n",
      "           0.0192,  0.1413, -0.0120,  0.2418, -0.0773,  0.4252,  0.0875,\n",
      "           0.1626,  0.3770, -0.1370,  0.0737, -0.0687,  0.1294,  0.1120,\n",
      "           0.0639,  0.0577,  0.0919, -0.2514, -0.1257, -0.1431, -0.0204,\n",
      "           0.1035, -0.2529,  0.3842,  0.0557, -0.0656,  0.0962,  0.1263,\n",
      "           0.1343, -0.1039, -0.2193,  0.2389, -0.1795,  0.2599, -0.0431,\n",
      "           0.0047, -0.1755,  0.1442, -0.1425, -0.2503,  0.0366,  0.2563,\n",
      "          -0.0061, -0.0337, -0.0585,  0.1604,  0.0175,  0.2520,  0.0886,\n",
      "          -0.0566, -0.0333,  0.1853,  0.2459,  0.0210,  0.0165,  0.0626,\n",
      "           0.0007, -0.1563]],\n",
      "\n",
      "        [[-0.0643, -0.2423, -0.2279,  0.2182, -0.1809, -0.1326,  0.0556,\n",
      "          -0.0259, -0.2193,  0.2313,  0.0568,  0.0020,  0.1100,  0.1223,\n",
      "           0.0060, -0.0943,  0.1082, -0.3120,  0.1595,  0.2424,  0.2365,\n",
      "           0.0144,  0.2065,  0.0375, -0.0394,  0.1271, -0.0535, -0.0595,\n",
      "           0.0558, -0.0646,  0.1788, -0.0175, -0.1514,  0.0681,  0.0228,\n",
      "          -0.2687,  0.1331,  0.0363,  0.2427, -0.2308, -0.0149, -0.1771,\n",
      "           0.2940, -0.1330,  0.1103, -0.0243,  0.0027,  0.0085,  0.0831,\n",
      "           0.0012,  0.0308, -0.0211, -0.0843, -0.2065, -0.1560, -0.1255,\n",
      "          -0.1332, -0.1825,  0.1618,  0.0087, -0.0593, -0.2704, -0.0898,\n",
      "          -0.0315, -0.0514, -0.0982,  0.1029,  0.2793, -0.1240,  0.0377,\n",
      "           0.0722,  0.1058, -0.0138,  0.1359, -0.0741,  0.3345, -0.0547,\n",
      "           0.1968,  0.3643, -0.1075,  0.1222, -0.1444,  0.1580,  0.1508,\n",
      "          -0.0017,  0.0170,  0.2025, -0.2129, -0.1487, -0.1468, -0.0039,\n",
      "           0.0301, -0.1164,  0.3767,  0.1948, -0.1219,  0.1040, -0.0685,\n",
      "           0.0170, -0.1982, -0.1790,  0.1366, -0.1064,  0.3747, -0.0634,\n",
      "          -0.0566, -0.1711,  0.0708, -0.1488, -0.2055,  0.0790,  0.1948,\n",
      "          -0.0310, -0.0355, -0.0474,  0.0991,  0.0008,  0.0981,  0.1675,\n",
      "          -0.0255,  0.0077,  0.1559,  0.2394,  0.0643,  0.0352, -0.0147,\n",
      "          -0.0135, -0.1826]]])\n",
      "torch.Size([2, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = _dec(code)\n",
    "    print(out)\n",
    "    print(out.size())  # Size 2, num_output, dim_hidden. FF 적용 직전 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "08b0aa2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 3146, 6714, 6714, 6714,\n",
      "        6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "        3229, 6714, 4379, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "        6714, 6714, 6714, 6714, 6714, 4390, 6714, 6714, 6714, 3885, 6714, 6714,\n",
      "        6714, 6187, 6714, 5250, 6714, 6714, 6714, 2813, 6714, 6714, 6714, 5456,\n",
      "        6714, 6714, 6714, 6714, 6714])\n",
      "torch.Size([2, 1, 128])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False]])\n",
      "tensor([[ 0.0206, -0.0193,  0.0100, -0.0740,  0.0247, -0.0164, -0.0078, -0.0284,\n",
      "         -0.0451,  0.0208,  0.0107, -0.0306,  0.0013, -0.0027, -0.0348, -0.0138,\n",
      "         -0.0405, -0.0433, -0.0543,  0.0443, -0.0151, -0.0028,  0.0121, -0.0803,\n",
      "          0.0542,  0.0288, -0.0240, -0.0709, -0.0489, -0.0445,  0.0021,  0.0230,\n",
      "          0.0016, -0.0035,  0.0233,  0.0633, -0.0445, -0.0299, -0.0313,  0.0463,\n",
      "          0.0949, -0.0012,  0.0439,  0.0028,  0.0077, -0.0521, -0.0339, -0.0102,\n",
      "          0.0296, -0.0209,  0.0944, -0.0125, -0.0235, -0.0201,  0.0023,  0.0302,\n",
      "          0.0070, -0.0149,  0.0292, -0.0241,  0.0213,  0.0423, -0.0170,  0.0045,\n",
      "         -0.0122,  0.0114, -0.0438,  0.0253,  0.0030,  0.0222, -0.0144,  0.0067,\n",
      "         -0.0026, -0.0182,  0.0055, -0.0282, -0.0664,  0.0045, -0.0414,  0.0233,\n",
      "         -0.0450, -0.0331, -0.0077, -0.0172,  0.0063, -0.0292,  0.0456,  0.0348,\n",
      "          0.0465,  0.0028, -0.0003, -0.0286,  0.0486, -0.0077,  0.0408,  0.0112,\n",
      "         -0.0279, -0.0344, -0.0503,  0.0057,  0.0336, -0.0628,  0.0490,  0.0379,\n",
      "          0.0254, -0.0152, -0.0096, -0.0437, -0.0031,  0.0002,  0.0030, -0.0552,\n",
      "          0.0133,  0.0328,  0.0124, -0.0037,  0.0185, -0.0507,  0.0333,  0.0100,\n",
      "         -0.0016, -0.0131,  0.0170,  0.0167, -0.0337, -0.0573,  0.0160,  0.0088]])\n"
     ]
    }
   ],
   "source": [
    "# permutation invariance\n",
    "\n",
    "with torch.no_grad():\n",
    "    x2 = deepcopy(x)\n",
    "    \"\"\"3146, 2813, 3229, 3885, 4379, 4390, 5250, 5456, 6187\"\"\"\n",
    "    shuf_ind = torch.randperm(x2.size(1))\n",
    "    x2[0] = x2[0][shuf_ind]  # ramdom permuted!\n",
    "    print(x2[0])\n",
    "    feature2 = Embed(x2)\n",
    "    enc_mask2 = deepcopy(enc_mask)\n",
    "    enc_mask2[0] = enc_mask[0][0][shuf_ind]\n",
    "    code2 = _enc(feature2, mask=enc_mask2)\n",
    "    out2 = _dec(code2)\n",
    "    print(out2.size())\n",
    "    print(out2[0].isclose(out[0]))\n",
    "    print(out2[0] - out[0]) # almost the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0a5be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f39da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28555db5",
   "metadata": {},
   "source": [
    "### Unified Model: Classification + Completion (CCNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "b6713e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2813, 3146, 3229, 3885, 4379, 4390, 5250, 5456, 6187, 6714, 6714, 6714,\n",
       "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "         6714, 6714, 6714, 6714, 6714],\n",
       "        [ 392,  937, 1476, 2172, 2351, 2813, 3350, 3554, 3857, 3978, 5249, 5648,\n",
       "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "         6714, 6714, 6714, 6714, 6714]])"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "9981739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(x):\n",
    "    if type(x) is not torch.Tensor:\n",
    "        x = torch.LongTensor(x)\n",
    "    if x.dim() > 2:\n",
    "        x = x.squeeze()\n",
    "        if x.dim() > 2:\n",
    "            return False\n",
    "    elif x.dim() < 2:\n",
    "        x = x.unsqueeze(0)\n",
    "    return F.one_hot(x).sum(1)[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "97da6e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6714])"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_one_hot(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "9dc4a86f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CCNet(nn.Module):\n",
    "    def __init__(self, dim_input=256,\n",
    "                 dim_output=20,\n",
    "                 num_items=6714+1, \n",
    "                 num_inds=32, \n",
    "                 dim_hidden=128, \n",
    "                 num_heads=4, \n",
    "                 num_outputs=1+1,  # class 1 + compl 1\n",
    "                 num_enc_layers=4, \n",
    "                 num_dec_layers=2, \n",
    "                 ln=False, \n",
    "                 classify=True, \n",
    "                 complete=True):\n",
    "        super(CCNet, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.padding_idx = num_items-1\n",
    "        self.classify, self.complete = classify, complete\n",
    "        self.embedding =  nn.Embedding(num_embeddings=num_items, embedding_dim=dim_input, padding_idx=-1)\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln)] +\n",
    "            [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln) for _ in range(num_enc_layers-1)])\n",
    "        self.pooling = PMA(dim_hidden, num_heads, num_outputs, ln=ln)\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            *[SAB(dim_hidden, dim_hidden, num_heads, ln=ln) for _ in range(num_dec_layers)])\n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_output))\n",
    "        self.decoder2 = nn.ModuleList(\n",
    "            [MAB(dim_hidden, dim_input, dim_hidden, num_heads, ln=ln) for _ in range(2)])\n",
    "        self.ff2 = nn.Linear(dim_hidden, num_items-1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x(=recipes): (batch, max_num_ingredient=65)\n",
    "        # bin_x : (batch, num_items-1=6714)\n",
    "        if not (self.classify or self.complete):\n",
    "            return\n",
    "        \n",
    "        x = torch.LongTensor(x)\n",
    "        feature = self.embedding(x)\n",
    "        # feature: (batch, max_num_ingredient=65, dim_input=256)\n",
    "        # cf. embedding.weight: (num_items=6715, dim_input=256)\n",
    "        mask = (x == self.padding_idx).repeat(self.num_heads,1).unsqueeze(1)\n",
    "        # mask: (batch*num_heads, 1, max_num_ingredient=65)\n",
    "        code = feature.clone()\n",
    "        for module in self.encoder:\n",
    "            code = module(code, mask=mask)\n",
    "        # code: (batch, max_num_ingredient=65, dim_hidden=128) : permutation-equivariant.\n",
    "        \n",
    "        pooled = self.pooling(code, mask=mask)\n",
    "        # pooled: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "        \n",
    "        signals = self.decoder1(pooled)\n",
    "        # no mask; signals: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "        \n",
    "        # split two signals: for classification & completion.\n",
    "        signal_classification = signals[:][0]                   # (batch, dim_hidden=128)\n",
    "        signal_completion = signals.clone()[:][1].unsqueeze(1)  # (batch, 1, dim_hidden=128)\n",
    "        \n",
    "        # Classification:\n",
    "        if self.classify:\n",
    "            logit_classification = self.ff1(signal_classification)  # (batch, dim_output)\n",
    "            if not self.complete:\n",
    "                return logit_clasification\n",
    "        \n",
    "        # Completion:\n",
    "        if self.complete:\n",
    "            bool_x = (make_one_hot(x) == True)\n",
    "            used_ingred_mask = bool_x.repeat(self.num_heads,1).unsqueeze(1)\n",
    "            # used_ingred_mask: (batch*num_heads, 1, num_items-1=6714)\n",
    "            \n",
    "            embedding_weight = self.embedding.weight[:-1].unsqueeze(0).repeat(feature.size(0),1,1)\n",
    "            # embedding_weight: (batch, num_items=6715, dim_input=256)\n",
    "            \n",
    "            for module in self.decoder2:\n",
    "                signal_completion = module(signal_completion, embedding_weight, mask=used_ingred_mask)\n",
    "            logit_completion = self.ff2(signal_completion.squeeze()) # (batch, num_items-1=6714)\n",
    "            logit_completion[bool_x] = float('-inf')\n",
    "            if not self.classify:\n",
    "                return logit_completion\n",
    "\n",
    "        return logit_classification, logit_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "ddc5ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = CCNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "855bbc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0206, -0.0218, -0.0604,  0.0219, -0.0341,  0.0685,  0.0029, -0.0262,\n",
      "          0.0789, -0.0135, -0.0330, -0.0573,  0.0831, -0.0547,  0.0514,  0.0190,\n",
      "         -0.0397,  0.0662, -0.0025,  0.0149],\n",
      "        [ 0.0148, -0.0124, -0.0635,  0.0235, -0.0381,  0.0614,  0.0021, -0.0336,\n",
      "          0.0783, -0.0071, -0.0352, -0.0504,  0.0816, -0.0599,  0.0446,  0.0217,\n",
      "         -0.0373,  0.0713, -0.0068,  0.0105]]) torch.Size([2, 20])\n",
      "tensor([[-0.0829,  0.0355,  0.0451,  ...,  0.0636, -0.0870, -0.1389],\n",
      "        [-0.0709,  0.0328,  0.0472,  ...,  0.0696, -0.0889, -0.1402]]) torch.Size([2, 6714])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logit1, logit2 = _model(arr_data_train[:2])\n",
    "    print(logit1, logit1.size())\n",
    "    print(logit2, logit2.size())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "5559599d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf])"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "x = torch.LongTensor(arr_data_train[:2])\n",
    "bin_x = make_one_hot(x)\n",
    "\n",
    "print((F.softmax(logit2, dim=-1) == 0).any())\n",
    "logit2[0][torch.where(bin_x[0]==1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6eb76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
