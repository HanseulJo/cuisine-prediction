{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb27b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# h5py 안 될 때\n",
    "!brew reinstall hdf5\n",
    "!export CPATH=\"/opt/homebrew/include/\"\n",
    "!export HDF5_DIR=/opt/homebrew/\n",
    "!python3 -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ba75710c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e31a79ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_root = '../'\n",
    "path_container = './Container/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bed3b7",
   "metadata": {},
   "source": [
    "### Dataset 가져오기 (1) dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1e2e1250",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'brazilian', 1: 'british', 2: 'cajun_creole', 3: 'chinese', 4: 'filipino', 5: 'french', 6: 'greek', 7: 'indian', 8: 'irish', 9: 'italian', 10: 'jamaican', 11: 'japanese', 12: 'korean', 13: 'mexican', 14: 'moroccan', 15: 'russian', 16: 'southern_us', 17: 'spanish', 18: 'thai', 19: 'vietnamese'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict, 20)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path_container + 'id_cuisine_dict.pickle', 'rb') as f:\n",
    "    id_cuisine_dict = pickle.load(f)\n",
    "\n",
    "print(id_cuisine_dict)\n",
    "type(id_cuisine_dict), len(id_cuisine_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f2e39188",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brazilian': 0, 'british': 1, 'cajun_creole': 2, 'chinese': 3, 'filipino': 4, 'french': 5, 'greek': 6, 'indian': 7, 'irish': 8, 'italian': 9, 'jamaican': 10, 'japanese': 11, 'korean': 12, 'mexican': 13, 'moroccan': 14, 'russian': 15, 'southern_us': 16, 'spanish': 17, 'thai': 18, 'vietnamese': 19}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict, 20)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path_container + 'cuisine_id_dict.pickle', 'rb') as f:\n",
    "    cuisine_id_dict = pickle.load(f)\n",
    "\n",
    "print(cuisine_id_dict)\n",
    "type(cuisine_id_dict), len(cuisine_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f72a5487",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Franks Hot Sauce\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict, 6714)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path_container + 'id_ingredient_dict.pickle', 'rb') as f:\n",
    "    id_ingredient_dict = pickle.load(f)\n",
    "\n",
    "print(id_ingredient_dict[6694])\n",
    "type(id_ingredient_dict), len(id_ingredient_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "696c550a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "561\n",
      "[698, 1039, 4315]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict, 6694)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path_container + 'ingredient_id_dict.pickle', 'rb') as f:\n",
    "    ingredient_id_dict = pickle.load(f)\n",
    "\n",
    "print(ingredient_id_dict['lemon grass'])\n",
    "print(ingredient_id_dict['lemongrass'])\n",
    "print(ingredient_id_dict['bread'])\n",
    "type(ingredient_id_dict), len(ingredient_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2acdf",
   "metadata": {},
   "source": [
    "### Datastet 가져오기 (2) h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b08853",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "_train = [arr_data_train, bin_data_train, int_labels_train, bin_labels_train]\n",
    "_valid_class = [arr_data_valid_class, bin_data_valid_class, int_labels_valid_class, bin_labels_valid_class]\n",
    "_valid_compl = [arr_data_valid_compl, bin_data_valid_compl, int_labels_valid_compl, bin_labels_valid_compl]\n",
    "_test_class = [arr_data_test_class, bin_data_test_class, None, None]\n",
    "_test_compl = [arr_data_test_compl, bin_data_test_compl, None, None]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1a5befc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23547, 65)\n",
      "(23547, 6714)\n",
      "(23547,)\n",
      "(23547, 20)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_container + 'train', 'r') as f:\n",
    "    arr_data_train = f['arr_data'][:]\n",
    "    bin_data_train = f['bin_data'][:]\n",
    "    int_labels_train = f['int_labels'][:]\n",
    "    bin_labels_train = f['bin_labels'][:]\n",
    "\n",
    "print(arr_data_train.shape)\n",
    "print(bin_data_train.shape)\n",
    "print(int_labels_train.shape)\n",
    "print(bin_labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9f94cb80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 65)\n",
      "(7848, 6714)\n",
      "(7848,)\n",
      "(7848, 20)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_container + 'valid_class', 'r') as f:\n",
    "    arr_data_valid_class = f['arr_data'][:]\n",
    "    bin_data_valid_class = f['bin_data'][:]\n",
    "    int_labels_valid_class = f['int_labels'][:]\n",
    "    bin_labels_valid_class = f['bin_labels'][:]\n",
    "\n",
    "print(arr_data_valid_class.shape)\n",
    "print(bin_data_valid_class.shape)\n",
    "print(int_labels_valid_class.shape)\n",
    "print(bin_labels_valid_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "1028eac6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 65)\n",
      "(7848, 6714)\n",
      "(7848,)\n",
      "(7848, 6714)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_container + 'valid_compl', 'r') as f:\n",
    "    arr_data_valid_compl = f['arr_data'][:]\n",
    "    bin_data_valid_compl = f['bin_data'][:]\n",
    "    int_labels_valid_compl = f['int_labels'][:]\n",
    "    bin_labels_valid_compl = f['bin_labels'][:]\n",
    "\n",
    "print(arr_data_valid_compl.shape)\n",
    "print(bin_data_valid_compl.shape)\n",
    "print(int_labels_valid_compl.shape)\n",
    "print(bin_labels_valid_compl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4c6ecee4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3924, 65)\n",
      "(3924, 6714)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_container + 'test_class', 'r') as f:\n",
    "    arr_data_test_class = f['arr_data'][:]\n",
    "    bin_data_test_class = f['bin_data'][:]\n",
    "\n",
    "print(arr_data_test_class.shape)\n",
    "print(bin_data_test_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8b4b701e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3924, 65)\n",
      "(3924, 6714)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_container + 'test_compl', 'r') as f:\n",
    "    arr_data_test_compl = f['arr_data'][:]\n",
    "    bin_data_test_compl = f['bin_data'][:]\n",
    "\n",
    "print(arr_data_test_compl.shape)\n",
    "print(bin_data_test_compl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "89c9330d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -1로 padding한 것을 6714로 갈아치우기 - for Embedding layer\n",
    "pad_idx = 6714\n",
    "\n",
    "arr_data_train[arr_data_train == -1] = pad_idx\n",
    "arr_data_valid_class[arr_data_valid_class == -1] = pad_idx\n",
    "arr_data_valid_compl[arr_data_valid_compl == -1] = pad_idx\n",
    "arr_data_test_class[arr_data_test_class == -1] = pad_idx\n",
    "arr_data_test_compl[arr_data_test_compl == -1] = pad_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b2103",
   "metadata": {},
   "source": [
    "### Ingredient feature vectors (nn.Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "11fda1e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2813, 3146, 3229, 3885, 4379, 4390, 5250, 5456, 6187, 6714, 6714,\n",
       "       6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "       6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "       6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "       6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
       "       6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "10062cc5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.4323,  0.5991,  0.3715,  ..., -0.3158,  1.3486,  0.1057],\n",
      "        [-0.8082, -0.2080,  1.4690,  ...,  0.5922, -1.1721,  1.8911],\n",
      "        [-0.6755, -0.6658, -0.6372,  ...,  0.6404, -0.4855,  1.1124],\n",
      "        ...,\n",
      "        [-0.5711,  2.8740,  0.9948,  ..., -0.1664,  0.7482, -0.9478],\n",
      "        [-0.2148,  1.7516,  0.1853,  ...,  0.2303,  0.9430, -0.1664],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       requires_grad=True)\n",
      "torch.Size([6715, 300])\n"
     ]
    }
   ],
   "source": [
    "num_ingredients = pad_idx +1  # = len(id_ingredient_dict) + 1  # +1 for padding (last)\n",
    "max_length = 65  # maximum num of ingredients per recipe\n",
    "embedding_dim = 300\n",
    "\n",
    "\n",
    "Embed = nn.Embedding(num_embeddings=num_ingredients, embedding_dim=embedding_dim, padding_idx=-1)\n",
    "x = torch.LongTensor(arr_data_train[:2])\n",
    "\n",
    "print(Embed.weight)\n",
    "print(Embed.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8e16a17c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3360,  0.2779, -0.8635, -1.3298,  2.1845, -1.9543,  0.9065,  0.1224,\n",
      "        -1.3027, -0.1567, -0.8056, -0.6019,  1.9613, -0.0181,  1.1118,  0.1249,\n",
      "        -1.6750,  0.5685,  1.8558, -1.0680,  1.8338, -0.8267,  0.1201, -0.4727,\n",
      "        -1.4298,  0.3650,  0.5593,  0.9799,  1.6882,  1.8133,  0.4485,  0.8086,\n",
      "         0.1940, -0.6872,  0.4445, -0.9949,  0.7992, -1.4526,  0.6038, -0.1308,\n",
      "         0.1192,  0.9464, -0.4507,  0.4774, -0.4403, -0.3338,  0.0026, -0.7624,\n",
      "        -0.7807, -0.3809, -2.0264, -0.5952,  0.5740, -1.6634, -1.2131,  0.1480,\n",
      "         1.3084, -1.4630, -0.1164, -0.0936,  0.6297, -0.2444, -0.6659,  1.4703,\n",
      "        -0.0045, -1.1747, -0.7170,  1.6428,  0.0116,  1.1870,  0.7871, -0.0143,\n",
      "        -0.4013, -2.0065,  1.4837,  0.0562,  0.2460,  0.1100,  0.6263,  1.4394,\n",
      "        -0.1018, -0.2406, -1.2441,  1.2254,  0.4320, -0.5434,  0.0320, -0.8656,\n",
      "        -1.0642,  1.8158, -0.1298, -0.7783, -0.9947,  0.0943,  0.5291, -0.3144,\n",
      "         0.3397, -0.8688, -0.3649,  1.4427, -0.9131,  0.5114, -0.0380, -0.1226,\n",
      "         0.8661, -0.2772,  0.0434, -0.7654, -0.4195,  1.6872,  1.0628,  0.8440,\n",
      "        -0.4617, -0.4500,  0.0678, -0.7666,  0.8011,  0.5583, -0.6801,  0.6138,\n",
      "         1.8925,  0.9332,  0.2263,  0.1843,  1.3338,  0.3351,  1.1834,  2.2904,\n",
      "        -1.6211, -0.0731,  0.0668,  1.9663,  0.0390,  0.4764,  0.2611,  0.4720,\n",
      "        -1.1645, -0.5172,  0.3032,  1.2398,  0.4635,  0.5238, -0.4055, -1.3715,\n",
      "        -1.1457,  0.9439, -0.3722, -2.1272,  1.4192, -0.5564, -0.7916,  0.5614,\n",
      "        -0.7377,  0.2481,  1.1199, -0.6685,  1.0799, -1.8194, -0.9307,  0.2216,\n",
      "        -1.6357,  1.7352, -0.8236, -0.2876,  0.7140,  0.7549, -0.5785,  1.3119,\n",
      "        -0.5593, -0.6082, -1.9381,  0.8323, -0.3789, -0.3428, -0.0110,  0.7683,\n",
      "         0.5219, -0.4709, -0.7312,  1.5202, -0.2518,  0.5157,  0.4387,  0.7699,\n",
      "         0.5451,  1.8937, -0.2358, -1.5176,  2.0491, -0.7046,  0.4086,  0.1183,\n",
      "        -0.6584, -0.4245,  0.2752,  0.4187, -0.6933, -0.3273, -0.7193, -0.8061,\n",
      "         0.2936,  0.8753, -0.0231,  1.1012,  1.5257, -0.1426, -0.2443,  1.0583,\n",
      "        -0.5566,  0.7281, -0.2201,  0.9887, -1.0231, -0.0907, -0.2253,  1.5096,\n",
      "        -1.9023, -1.7953,  0.8658,  0.0834,  0.9797, -0.2065, -0.1966, -1.6185,\n",
      "        -0.1782, -1.6374, -1.1510, -0.7879,  1.5531,  0.2105,  0.8564,  0.2982,\n",
      "        -1.1366, -0.0325, -0.5284, -1.7015,  1.1416,  0.7396,  0.1411,  1.4588,\n",
      "        -0.6137, -0.2153,  0.5371, -2.3022,  1.4073,  0.8788,  0.3246,  0.2541,\n",
      "        -1.4163, -1.1840,  1.8372,  0.2645, -0.1754,  1.0795,  0.2460, -0.4914,\n",
      "         0.5150,  0.7646,  0.5817, -1.1545,  0.4530,  0.9890, -0.0838,  0.1746,\n",
      "         0.8825, -0.0326,  0.3080,  0.7734, -0.9886, -0.2863, -1.9319,  1.0541,\n",
      "        -0.4841, -1.3575, -0.0868,  0.6449,  0.2893,  0.1877,  0.0451,  1.5841,\n",
      "         1.4282, -1.3341, -0.4193, -0.3263, -0.8094, -0.2540,  0.0150, -1.0805,\n",
      "         0.9978,  0.4047,  0.3283, -0.1530,  0.0552, -0.2713, -0.4612,  1.3714,\n",
      "         1.0451,  0.0529,  1.1202,  0.7782], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(Embed.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "737a9d3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2813, 3146, 3229, 3885, 4379, 4390, 5250, 5456, 6187, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714],\n",
      "        [ 392,  937, 1476, 2172, 2351, 2813, 3350, 3554, 3857, 3978, 5249, 5648,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714]])\n",
      "torch.Size([2, 65, 300])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "with torch.no_grad():\n",
    "    print(Embed(x).size())\n",
    "    print(Embed(x)[0][11]) # zero padded feature matrix! (0번째 recipe는 9개라 10~65번째 feature vector는 zeros.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "b3c3082f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3]],\n",
       " \n",
       "         [[12, 13, 14, 15]]]),\n",
       " tensor([[[ 4,  5,  6,  7]],\n",
       " \n",
       "         [[16, 17, 18, 19]]]),\n",
       " tensor([[[ 8,  9, 10, 11]],\n",
       " \n",
       "         [[20, 21, 22, 23]]]))"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30d8f158",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "Set transformer: ( https://github.com/juho-lee/set_transformer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "734a2931",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Building blocks of Set Transformers ##\n",
    "# added masks.\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Sequential(\n",
    "            nn.Linear(dim_V, 2*dim_V),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*dim_V, dim_V))\n",
    "\n",
    "    def forward(self, Q, K, mask=None):\n",
    "        # Q (batch, q_len, d_hid)\n",
    "        # K (batch, k_len, d_hid)\n",
    "        # V (batch, v_len, d_hid == dim_V)\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "        \n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        \n",
    "        # Q_ (batch * num_heads, q_len, d_hid // num_heads)\n",
    "        # K_ (batch * num_heads, k_len, d_hid // num_heads)\n",
    "        # V_ (batch * num_heads, v_len, d_hid // num_heads)\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "        \n",
    "        # energy (batch * num_heads, q_len, k_len)\n",
    "        energy = Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V)\n",
    "        if mask is not None:\n",
    "            energy.masked_fill_(mask, float('-inf'))\n",
    "        A = torch.softmax(energy, 2)\n",
    "        \n",
    "        # O (batch, q_len, d_hid)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
    "        O = O + self.fc_o(O)\n",
    "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(X, X, mask=mask)\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X, mask=mask)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
    "        \n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "8c512298",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_input,\n",
    "                 num_inds=32, dim_hidden=128, num_heads=4, num_layers=2, ln=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc = nn.ModuleList(\n",
    "            [ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln)] +\n",
    "            [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln) for _ in range(num_layers-1)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, X, mask=None):\n",
    "        out = X.detach()\n",
    "        for module in self.enc:\n",
    "            out = module(out, mask=mask)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_outputs,\n",
    "                 num_inds=32, dim_hidden=128, num_heads=4, ln=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec = nn.Sequential(\n",
    "                PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "                SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "                SAB(dim_hidden, dim_hidden, num_heads, ln=ln))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.dec(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033bdb8",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "d363f29b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size torch.Size([2, 65])\n",
      "input tensor([[2813, 3146, 3229, 3885, 4379, 4390, 5250, 5456, 6187, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714],\n",
      "        [ 392,  937, 1476, 2172, 2351, 2813, 3350, 3554, 3857, 3978, 5249, 5648,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714]])\n",
      "feature size torch.Size([2, 65, 300])\n",
      "feature tensor([[[-0.3718, -0.7948, -1.0638,  ..., -0.1697,  0.3928, -1.7382],\n",
      "         [-0.2320, -0.6004,  0.7259,  ..., -0.4812, -1.0097,  0.4035],\n",
      "         [ 0.2387, -1.6030,  2.1758,  ...,  1.9942, -0.5199, -1.2055],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.5714,  1.8386, -0.9285,  ...,  0.6606,  0.3063, -0.8093],\n",
      "         [ 1.0274,  1.4440, -1.6936,  ..., -0.0911,  0.6650,  1.0409],\n",
      "         [-0.7797,  3.1893, -1.0458,  ...,  1.0498,  0.1435,  0.2554],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('input size', x.size())\n",
    "    print('input', x)\n",
    "    feature = Embed(x)  # Size 2, 65, 300\n",
    "    print('feature size', feature.size())\n",
    "    print('feature', feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "f6895c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 65]),\n",
       " tensor([[[False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True,  True]]]))"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_mask = (x == pad_idx).unsqueeze(1).repeat(4,1,1)\n",
    "enc_mask.size(), enc_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "cf606f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 size torch.Size([2, 65])\n",
      "x1 tensor([[3146, 2813, 3229, 3885, 4379, 4390, 5250, 5456, 6187, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714],\n",
      "        [ 392,  937, 1476, 2172, 2351, 2813, 3350, 3554, 3857, 3978, 5249, 5648,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "         6714, 6714, 6714, 6714, 6714]])\n",
      "feature1 tensor([[[-0.2320, -0.6004,  0.7259,  ..., -0.4812, -1.0097,  0.4035],\n",
      "         [-0.3718, -0.7948, -1.0638,  ..., -0.1697,  0.3928, -1.7382],\n",
      "         [ 0.2387, -1.6030,  2.1758,  ...,  1.9942, -0.5199, -1.2055],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.5714,  1.8386, -0.9285,  ...,  0.6606,  0.3063, -0.8093],\n",
      "         [ 1.0274,  1.4440, -1.6936,  ..., -0.0911,  0.6650,  1.0409],\n",
      "         [-0.7797,  3.1893, -1.0458,  ...,  1.0498,  0.1435,  0.2554],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "x1 = deepcopy(x)\n",
    "x1[0][:2] = x[0][[1,0]]\n",
    "print('x1 size', x1.size())\n",
    "print('x1', x1)\n",
    "with torch.no_grad():\n",
    "    feature1 = Embed(x1)\n",
    "    print('feature1', feature1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "b76be7e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_sab = SAB(300, 128, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "ac6eb429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sab(feature) tensor([[[-1.0811,  0.8595,  0.2833,  ..., -0.7082, -0.0783, -0.3707],\n",
      "         [-0.2493, -0.4630, -0.0957,  ...,  0.3101, -0.1208,  0.1451],\n",
      "         [ 0.8185,  1.5390, -0.3792,  ...,  1.0239,  0.4700, -0.5495],\n",
      "         ...,\n",
      "         [ 0.1239,  0.0249, -0.2052,  ..., -0.0540,  0.2446, -0.3237],\n",
      "         [ 0.1239,  0.0249, -0.2052,  ..., -0.0540,  0.2446, -0.3237],\n",
      "         [ 0.1239,  0.0249, -0.2052,  ..., -0.0540,  0.2446, -0.3237]],\n",
      "\n",
      "        [[-1.1312, -1.1506, -1.1365,  ..., -0.4485,  0.5151, -1.0360],\n",
      "         [-0.5842,  0.6652, -0.3660,  ..., -0.0806, -0.1796,  0.8712],\n",
      "         [ 0.6200, -0.4383, -0.0136,  ..., -0.1955, -0.3403,  0.6055],\n",
      "         ...,\n",
      "         [-0.0362,  0.1203,  0.2367,  ...,  0.0110, -0.0174,  0.0640],\n",
      "         [-0.0362,  0.1203,  0.2367,  ...,  0.0110, -0.0174,  0.0640],\n",
      "         [-0.0362,  0.1203,  0.2367,  ...,  0.0110, -0.0174,  0.0640]]])\n",
      "torch.Size([2, 65, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z = _sab(feature, mask=enc_mask)\n",
    "    print('sab(feature)',z)\n",
    "    print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "5b1ff076",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_enc = Encoder(300, dim_hidden=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "ef0ec01b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0690,  0.0916,  0.2119,  ..., -0.0187, -0.0996,  0.0199],\n",
      "        [-0.0690,  0.0916,  0.2119,  ..., -0.0187, -0.0996,  0.0199],\n",
      "        [-0.0690,  0.0916,  0.2119,  ..., -0.0187, -0.0996,  0.0199],\n",
      "        ...,\n",
      "        [-0.0690,  0.0916,  0.2119,  ..., -0.0187, -0.0996,  0.0199],\n",
      "        [-0.0690,  0.0916,  0.2119,  ..., -0.0187, -0.0996,  0.0199],\n",
      "        [-0.0690,  0.0916,  0.2119,  ..., -0.0187, -0.0996,  0.0199]])\n",
      "torch.Size([2, 65, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    code = _enc(feature,mask=enc_mask)\n",
    "    print(code[0][15:])\n",
    "    print(code.size())  # Size 2, 65, dim_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "0ae1cf6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1489,  0.3277, -0.1951,  ..., -0.0156, -0.1569,  0.4657],\n",
      "         [-0.5638,  0.8315, -0.1925,  ..., -0.3830,  0.2357, -0.1620],\n",
      "         [-0.3019,  0.5831,  0.8459,  ..., -0.1022, -0.3173, -0.2990],\n",
      "         ...,\n",
      "         [-0.0690,  0.0916,  0.2119,  ..., -0.0187, -0.0996,  0.0199],\n",
      "         [-0.0690,  0.0916,  0.2119,  ..., -0.0187, -0.0996,  0.0199],\n",
      "         [-0.0690,  0.0916,  0.2119,  ..., -0.0187, -0.0996,  0.0199]],\n",
      "\n",
      "        [[-0.2554,  0.3312, -0.0621,  ..., -0.3660,  0.0441, -0.1231],\n",
      "         [-0.4615, -0.3753, -0.6948,  ...,  0.2721,  0.3118,  0.4291],\n",
      "         [-0.2623,  0.6359,  0.4309,  ..., -0.5692, -0.7051,  0.1332],\n",
      "         ...,\n",
      "         [-0.1413,  0.0849, -0.0211,  ..., -0.1963, -0.1153,  0.0526],\n",
      "         [-0.1413,  0.0849, -0.0211,  ..., -0.1963, -0.1153,  0.0526],\n",
      "         [-0.1413,  0.0849, -0.0211,  ..., -0.1963, -0.1153,  0.0526]]])\n",
      "torch.Size([2, 65, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    code1 = _enc(feature1, mask=enc_mask)\n",
    "    print(code1)\n",
    "    print(code1.size())  # Size 2, 65, dim_hidden  # permutation equivariant! (observe first 2 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "757c4fee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_dec = Decoder(1, dim_hidden=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "ef17eee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1978,  0.0129,  0.0625,  0.0120,  0.0346,  0.1257, -0.0215,\n",
      "          -0.1831,  0.0467,  0.0193,  0.0717, -0.1412,  0.0530, -0.0501,\n",
      "          -0.1162,  0.0165,  0.1132,  0.0610, -0.1639,  0.0273,  0.0963,\n",
      "          -0.0051, -0.1811,  0.0597, -0.0864,  0.3383, -0.3138,  0.0682,\n",
      "          -0.0637,  0.0596, -0.0268, -0.1935,  0.5056,  0.3642, -0.0314,\n",
      "           0.2691, -0.1282, -0.3630, -0.0420, -0.0471,  0.1804,  0.1003,\n",
      "          -0.1876, -0.2552, -0.1873,  0.1398,  0.0695, -0.0252, -0.0293,\n",
      "          -0.0218,  0.0517,  0.0418, -0.0733,  0.1663,  0.0441,  0.1436,\n",
      "           0.0106,  0.0543, -0.2302, -0.1391,  0.0185,  0.2868, -0.0059,\n",
      "           0.0476,  0.0071, -0.1018,  0.1085,  0.0848, -0.2863, -0.1488,\n",
      "          -0.0353,  0.0657, -0.1502, -0.0441, -0.0057, -0.0644, -0.0555,\n",
      "          -0.2947,  0.1081, -0.2980, -0.1075,  0.3180,  0.0098,  0.1202,\n",
      "           0.1133, -0.0935,  0.2495,  0.0615,  0.0846, -0.1701, -0.0205,\n",
      "           0.0753, -0.0851, -0.0896,  0.2262, -0.0498,  0.2057, -0.2576,\n",
      "           0.2094,  0.1381, -0.1555, -0.0940,  0.0118, -0.0196, -0.1831,\n",
      "           0.1193,  0.0034,  0.2281,  0.1106, -0.0991, -0.1437, -0.0182,\n",
      "           0.0920,  0.1514, -0.0375,  0.0791, -0.1146,  0.1907,  0.1373,\n",
      "          -0.0626,  0.1586, -0.0743, -0.1140, -0.1943,  0.2408,  0.0022,\n",
      "          -0.0166, -0.1601]],\n",
      "\n",
      "        [[-0.2241, -0.0174,  0.0888, -0.0096,  0.0622,  0.1332,  0.0676,\n",
      "          -0.1416,  0.0047,  0.0358,  0.0697, -0.0963,  0.0898, -0.1053,\n",
      "          -0.0368,  0.1125,  0.0992,  0.0821, -0.2112,  0.1018,  0.0888,\n",
      "          -0.0213, -0.1493,  0.1167,  0.0041,  0.3461, -0.3153,  0.0572,\n",
      "          -0.0989,  0.0590, -0.0509, -0.2139,  0.4181,  0.3376, -0.0264,\n",
      "           0.2663, -0.0870, -0.3093, -0.0071, -0.0300,  0.0900,  0.0201,\n",
      "          -0.1641, -0.2188, -0.1880,  0.0705,  0.0273, -0.0074, -0.0121,\n",
      "           0.0247,  0.0603,  0.1298, -0.0782,  0.1411,  0.0428,  0.1293,\n",
      "          -0.0378,  0.0529, -0.2268, -0.1449,  0.0386,  0.2284,  0.0148,\n",
      "           0.0305,  0.0219, -0.1723,  0.0920,  0.1045, -0.2406, -0.1452,\n",
      "          -0.0815,  0.0662, -0.0792,  0.0507, -0.0458, -0.0125, -0.1126,\n",
      "          -0.2107,  0.0507, -0.2947, -0.0886,  0.3021,  0.0447,  0.0885,\n",
      "           0.0782, -0.2088,  0.2795,  0.0183,  0.0189, -0.1741,  0.0404,\n",
      "           0.0237, -0.1413, -0.0586,  0.2152, -0.0173,  0.1449, -0.2525,\n",
      "           0.2392,  0.0910, -0.2166, -0.0578, -0.0126, -0.0030, -0.1289,\n",
      "           0.1888,  0.0316,  0.1776,  0.1639, -0.1022, -0.1742, -0.0765,\n",
      "           0.1662,  0.1524, -0.0573,  0.0784, -0.1408,  0.1682,  0.1430,\n",
      "          -0.0711,  0.2213, -0.1781, -0.1716, -0.1371,  0.1890,  0.1065,\n",
      "          -0.0612, -0.1103]]])\n",
      "torch.Size([2, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = _dec(code)\n",
    "    print(out)\n",
    "    print(out.size())  # Size 2, num_output, dim_hidden. FF 적용 직전 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "269dc53f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6714, 6714, 3229, 6714, 6714, 6714, 6714, 6714, 6714, 3146, 6714, 2813,\n",
      "        6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "        6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "        6714, 5456, 6714, 6187, 6714, 6714, 6714, 4379, 6714, 6714, 6714, 6714,\n",
      "        6714, 6714, 4390, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714, 6714,\n",
      "        6714, 5250, 6714, 3885, 6714])\n",
      "torch.Size([2, 1, 128])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False]])\n",
      "tensor([[ 9.6001e-03, -4.3056e-02, -2.6206e-03, -3.1018e-02,  2.8785e-02,\n",
      "          9.9849e-03,  2.8926e-03,  6.4940e-03, -4.7544e-02, -8.9468e-03,\n",
      "         -1.7936e-02,  4.3507e-03, -3.1967e-03, -2.6487e-02,  4.6719e-02,\n",
      "          8.1571e-02,  1.4334e-02,  5.6375e-02,  2.0883e-02, -1.8654e-05,\n",
      "          1.8960e-02, -3.4904e-03,  3.0587e-02,  5.2621e-02,  3.6464e-02,\n",
      "         -5.2609e-02,  5.7704e-02, -1.4796e-02, -5.6188e-03, -5.1152e-02,\n",
      "          9.2943e-03, -3.0568e-02, -8.4349e-02, -5.9022e-02,  8.7814e-03,\n",
      "          8.7403e-03,  2.6607e-02,  4.9903e-02,  5.8014e-02,  1.7187e-02,\n",
      "         -6.5681e-02, -8.2290e-02,  9.3349e-03, -3.0583e-03,  5.2922e-02,\n",
      "         -4.9781e-02, -3.5919e-02,  2.8886e-02,  1.0736e-02, -4.0300e-02,\n",
      "         -2.3417e-02,  8.0186e-02, -5.9955e-03, -3.7639e-03, -1.9826e-02,\n",
      "         -1.0620e-02, -4.2026e-02, -2.5191e-04,  4.3296e-02,  2.8613e-02,\n",
      "          4.2983e-02, -4.8039e-02, -3.1140e-02, -4.5740e-02,  3.2507e-02,\n",
      "         -4.7418e-02, -1.1183e-02, -3.5639e-02,  1.0265e-01,  3.0392e-02,\n",
      "         -3.5715e-02,  1.8529e-02,  2.2191e-02,  8.4597e-02, -5.2282e-02,\n",
      "         -1.0779e-02, -4.7478e-02,  1.9775e-02, -2.6558e-02,  3.7745e-02,\n",
      "          7.1702e-03, -5.1453e-02,  3.9119e-02,  2.4023e-02, -2.5576e-02,\n",
      "         -8.0560e-02, -1.5394e-02, -1.9788e-02, -5.4606e-02,  4.1066e-02,\n",
      "          1.1030e-02, -3.7560e-02, -1.0368e-02, -8.1567e-03,  1.0129e-02,\n",
      "         -1.7597e-03, -1.1132e-01,  4.5634e-02, -3.7952e-02, -2.0837e-02,\n",
      "          1.9885e-02,  3.1248e-02,  9.1933e-03,  4.2477e-02,  6.2827e-02,\n",
      "          2.9727e-03,  1.8965e-02, -6.1939e-03, -1.5750e-02,  3.9283e-02,\n",
      "          3.0589e-02, -6.8311e-02, -4.7120e-03, -5.2796e-02, -1.6167e-02,\n",
      "         -1.1771e-02, -7.4340e-03, -7.1902e-03, -3.3625e-02,  4.3543e-02,\n",
      "         -1.2801e-02, -1.3003e-02, -6.3915e-03,  2.2476e-02, -2.1373e-02,\n",
      "          7.4208e-02, -2.0984e-02,  2.5578e-02]])\n"
     ]
    }
   ],
   "source": [
    "# permutation invariance\n",
    "\n",
    "with torch.no_grad():\n",
    "    x2 = deepcopy(x)\n",
    "    \"\"\"3146, 2813, 3229, 3885, 4379, 4390, 5250, 5456, 6187\"\"\"\n",
    "    shuf_ind = torch.randperm(x2.size(1))\n",
    "    x2[0] = x2[0][shuf_ind]  # ramdom permuted!\n",
    "    print(x2[0])\n",
    "    feature2 = Embed(x2)\n",
    "    enc_mask2 = deepcopy(enc_mask)\n",
    "    enc_mask2[0] = enc_mask[0][0][shuf_ind]\n",
    "    code2 = _enc(feature2, mask=enc_mask2)\n",
    "    out2 = _dec(code2)\n",
    "    print(out2.size())\n",
    "    print(out2[0].isclose(out[0]))\n",
    "    print(out2[0] - out[0]) # almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d8796",
   "metadata": {},
   "source": [
    "### Unified Model: Classification + Completion (CCNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "b677b672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_a = torch.LongTensor([1,2,3,1,2,3])\n",
    "torch.Tensor(list(set([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "c1881b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CCNet(nn.Module):\n",
    "    def __init__(self, dim_input=256, dim_output=20, num_items=6714+1, \n",
    "                  num_inds=32, dim_hidden=128, num_heads=4, num_outputs=1+1,  # class 1 + compl 1\n",
    "                  num_enc_layers=4, num_dec_layers=2, ln=False):\n",
    "        super(CCNet, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.padding_idx = num_items-1\n",
    "        self.embedding =  nn.Embedding(num_embeddings=num_items, embedding_dim=dim_input, padding_idx=-1)\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln)] +\n",
    "            [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln) for _ in range(num_enc_layers-1)])\n",
    "        self.pooling = PMA(dim_hidden, num_heads, num_outputs, ln=ln)\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            *[SAB(dim_hidden, dim_hidden, num_heads, ln=ln) for _ in range(num_dec_layers)])\n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_output))\n",
    "        self.decoder2 = nn.ModuleList(\n",
    "            [MAB(dim_hidden, dim_input, dim_hidden, num_heads, ln=ln) for _ in range(2)])\n",
    "        self.ff2 = nn.Linear(dim_hidden, num_items-1)\n",
    "    \n",
    "    def generate_mask(self, x):\n",
    "        return (torch.LongTensor(x) == self.padding_idx).repeat(self.num_heads,1).unsqueeze(1)\n",
    "\n",
    "    \n",
    "    def forward(self, x, bin_x):\n",
    "        # x(=recipes): (batch, max_num_ingredient=65)\n",
    "        # bin_x : (batch, num_items-1=6714)\n",
    "        \n",
    "        feature = self.embedding(torch.LongTensor(x))\n",
    "        # feature: (batch, max_num_ingredient=65, dim_input=256)\n",
    "        # cf. embedding.weight: (num_items=6715, dim_input=256)\n",
    "        \n",
    "        mask = self.generate_mask(x)\n",
    "        # mask: (batch*num_heads, 1, max_num_ingredient=65)\n",
    "        \n",
    "        code = feature.clone()\n",
    "        for module in self.encoder:\n",
    "            code = module(code, mask=mask)\n",
    "        # code: (batch, max_num_ingredient=65, dim_hidden=128) : permutation-equivariant.\n",
    "        \n",
    "        pooled = self.pooling(code, mask=mask)\n",
    "        # pooled: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "        \n",
    "        signals = self.decoder1(pooled)\n",
    "        # no mask; signals: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "        \n",
    "        # split two signals: for classification & completion.\n",
    "        signal_classification = signals[:][0].squeeze()  # (batch, dim_hidden=128)\n",
    "        signal_completion = signals.clone()[:][1]        # (batch, 1, dim_hidden=128)\n",
    "        \n",
    "        # Classification:\n",
    "        logit_classification = self.ff1(signal_classification) # (batch, dim_output)\n",
    "        \n",
    "        # Completion:\n",
    "        used_ingred_mask = self.generate_mask(bin_x)\n",
    "        # used_ingred_mask: (batch*num_heads, 1, num_items-1=6714)\n",
    "        embedding_weight = self.embedding.weight[:-1].unsqueeze(0).repeat(feature.size(0),1,1)\n",
    "        # embedding_weight: (batch, num_items=6715, dim_input=256)\n",
    "        for module in self.decoder2:\n",
    "            signal_completion = module(signal_completion, embedding_weight, mask=used_ingred_mask)\n",
    "        logit_completion = self.ff2(signal_completion.squeeze()) # (batch, num_items-1=6714)\n",
    "        logit_completion[used_ingred_mask.squeeze()] = float('-inf')\n",
    "        \n",
    "        return logit_classification, logit_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "d41e867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = CCNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "6c0b47ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [591]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr_data_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_data_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [589]\u001b[0m, in \u001b[0;36mCCNet.forward\u001b[0;34m(self, x, bin_x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# embedding_weight: (batch, num_items=6715, dim_input=256)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder2:\n\u001b[0;32m---> 63\u001b[0m     signal_completion \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal_completion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mused_ingred_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m logit_completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff2(signal_completion\u001b[38;5;241m.\u001b[39msqueeze()) \u001b[38;5;66;03m# (batch, num_items-1=6714)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m logit_completion[used_ingred_mask\u001b[38;5;241m.\u001b[39msqueeze()] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [546]\u001b[0m, in \u001b[0;36mMAB.forward\u001b[0;34m(self, Q, K, mask)\u001b[0m\n\u001b[1;32m     27\u001b[0m dim_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_V \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Q_ (batch * num_heads, q_len, d_hid // num_heads)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# K_ (batch * num_heads, k_len, d_hid // num_heads)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# V_ (batch * num_heads, v_len, d_hid // num_heads)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m Q_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     33\u001b[0m K_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(K\u001b[38;5;241m.\u001b[39msplit(dim_split, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     34\u001b[0m V_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(V\u001b[38;5;241m.\u001b[39msplit(dim_split, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py:510\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39msplit, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, split_size, dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(split_size, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(split_size, Tensor):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "_model(arr_data_train[:2], bin_data_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080e3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
