{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f208fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5py 안 될 때\n",
    "#!brew reinstall hdf5\n",
    "#!export CPATH=\"/opt/homebrew/include/\"\n",
    "#!export HDF5_DIR=/opt/homebrew/\n",
    "#!python3 -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6909cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb3e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '../'\n",
    "path_container = './Container/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb2659",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db71052",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_container + 'id_cuisine_dict.pickle', 'rb') as f:\n",
    "    id_cuisine_dict = pickle.load(f)\n",
    "with open(path_container + 'cuisine_id_dict.pickle', 'rb') as f:\n",
    "    cuisine_id_dict = pickle.load(f)\n",
    "with open(path_container + 'id_ingredient_dict.pickle', 'rb') as f:\n",
    "    id_ingredient_dict = pickle.load(f)\n",
    "with open(path_container + 'ingredient_id_dict.pickle', 'rb') as f:\n",
    "    ingredient_id_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9249e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, data_dir, test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.test = test\n",
    "        self.data_file = h5py.File(data_dir, 'r')\n",
    "        self.bin_data = self.data_file['bin_data'][:]  # Size (num_recipes=23547, num_ingredients=6714)\n",
    "        if not test:\n",
    "            self.int_labels = self.data_file['int_labels'][:]  # Size (num_recipes=23547,), about cuisines\n",
    "            self.bin_labels = self.data_file['bin_labels'][:]  # Size (num_recipes=23547, 20), about cuisines\n",
    "        \n",
    "        self.padding_idx = self.bin_data.shape[1]  # == num_ingredient == 6714\n",
    "        self.max_num_ingredients_per_recipe = self.bin_data.sum(1).max()  # valid & test의 경우 65\n",
    "        \n",
    "        # (59나 65로) 고정된 길이의 row vector에 해당 recipe의 indices 넣고 나머지는 padding index로 채워넣기\n",
    "        # self.int_data: Size (num_recipes=23547, self.max_num_ingredients_per_recipe=59 or 65)\n",
    "        self.int_data = np.full((len(self.bin_data), self.max_num_ingredients_per_recipe), self.padding_idx) \n",
    "        for i, bin_recipe in enumerate(self.bin_data):\n",
    "            recipe = np.arange(self.padding_idx)[bin_recipe==1]\n",
    "            self.int_data[i][:len(recipe)] = recipe\n",
    "        self.data_file.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.bin_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bin_data = self.bin_data[idx]\n",
    "        int_data = self.int_data[idx]\n",
    "        bin_label = None if self.test else self.bin_labels[idx]\n",
    "        int_label = None if self.test else self.int_labels[idx]\n",
    "        return bin_data, int_data, bin_label, int_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0e17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = ['train', 'valid_class', 'valid_compl', 'test_class', 'test_compl']\n",
    "\n",
    "recipe_datasets = {x: RecipeDataset(os.path.join(path_container, x), test='test' in x) for x in dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1bcc5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0] (6714,)\n",
      "[2813 3146 3229 3885 4379 4390 5250 5456 6187 6714 6714 6714 6714 6714\n",
      " 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714\n",
      " 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714\n",
      " 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714 6714\n",
      " 6714 6714 6714]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "_bd,_id,_bl,_il = recipe_datasets['train'][0]\n",
    "print(_bd, _bd.shape)\n",
    "print(_id)\n",
    "print(_bl)\n",
    "print(_il)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559311cb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd58c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building blocks of Set Transformers ##\n",
    "# added masks.\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False, dropout=0):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.p = dropout\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Sequential(\n",
    "            nn.Linear(dim_V, 2*dim_V),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*dim_V, dim_V))\n",
    "        self.Dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, Q, K, mask=None):\n",
    "        # Q (batch, q_len, d_hid)\n",
    "        # K (batch, k_len, d_hid)\n",
    "        # V (batch, v_len, d_hid == dim_V)\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "        \n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        \n",
    "        # Q_ (batch * num_heads, q_len, d_hid // num_heads)\n",
    "        # K_ (batch * num_heads, k_len, d_hid // num_heads)\n",
    "        # V_ (batch * num_heads, v_len, d_hid // num_heads)\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "        \n",
    "        # energy (batch * num_heads, q_len, k_len)\n",
    "        energy = Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V)\n",
    "        if mask is not None:\n",
    "            energy.masked_fill_(mask, float('-inf'))\n",
    "        A = torch.softmax(energy, 2)\n",
    "        \n",
    "        # O (batch, q_len, d_hid)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
    "        _O = self.fc_o(O)\n",
    "        if self.p > 0:\n",
    "            _O = self.Dropout(_O)\n",
    "        O = O + _O \n",
    "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False, dropout=0.2):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(X, X, mask=mask)\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False, dropout=0.2):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X, mask=mask)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False, dropout=0.2):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln, dropout=dropout)\n",
    "        \n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e90444e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(x):\n",
    "        \"\"\" Convert int_data into bin_data, if needed. \"\"\"\n",
    "        if type(x) is not torch.Tensor:\n",
    "            x = torch.LongTensor(x)\n",
    "        if x.dim() > 2:\n",
    "            x = x.squeeze()\n",
    "            if x.dim() > 2:\n",
    "                return False\n",
    "        elif x.dim() < 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        return F.one_hot(x).sum(1)[:,:-1]\n",
    "\n",
    "class CCNet(nn.Module):\n",
    "    def __init__(self, dim_input=256,\n",
    "                 dim_output=20,\n",
    "                 num_items=6714+1, \n",
    "                 num_inds=32, \n",
    "                 dim_hidden=128, \n",
    "                 num_heads=4, \n",
    "                 num_outputs=1+1,  # classification 1 + completion 1\n",
    "                 num_enc_layers=4, \n",
    "                 num_dec_layers=2,\n",
    "                 ln=True,          # LayerNorm option\n",
    "                 dropout=0.2,      # Dropout option\n",
    "                 classify=True,    # completion만 하고 싶으면 False로\n",
    "                 complete=True):   # classification만 하고 싶으면 False로\n",
    "        super(CCNet, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.padding_idx = num_items-1\n",
    "        self.classify, self.complete = classify, complete\n",
    "        self.embedding =  nn.Embedding(num_embeddings=num_items, embedding_dim=dim_input, padding_idx=-1)\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln)] +\n",
    "            [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln) for _ in range(num_enc_layers-1)])\n",
    "        self.pooling = PMA(dim_hidden, num_heads, num_outputs, ln=ln)\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            *[SAB(dim_hidden, dim_hidden, num_heads, ln=ln, dropout=dropout) for _ in range(num_dec_layers)])\n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_output))\n",
    "        self.decoder2 = nn.ModuleList(\n",
    "            [MAB(dim_hidden, dim_input, dim_hidden, num_heads, ln=ln, dropout=dropout) for _ in range(2)])\n",
    "        self.ff2 = nn.Linear(dim_hidden, num_items-1)\n",
    "    \n",
    "    def forward(self, x, bin_x=None): \n",
    "        # x(=recipes): (batch, max_num_ingredient=65) : int_data.\n",
    "        if not (self.classify or self.complete):\n",
    "            return\n",
    "        \n",
    "        x = torch.LongTensor(x)\n",
    "        feature = self.embedding(x)\n",
    "        # feature: (batch, max_num_ingredient=65, dim_input=256)\n",
    "        # cf. embedding.weight: (num_items=6715, dim_input=256)\n",
    "        mask = (x == self.padding_idx).repeat(self.num_heads,1).unsqueeze(1)\n",
    "        # mask: (batch*num_heads, 1, max_num_ingredient=65)\n",
    "        code = feature.clone()\n",
    "        for module in self.encoder:\n",
    "            code = module(code, mask=mask)\n",
    "        # code: (batch, max_num_ingredient=65, dim_hidden=128) : permutation-equivariant.\n",
    "        \n",
    "        pooled = self.pooling(code, mask=mask)\n",
    "        # pooled: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "        \n",
    "        signals = self.decoder1(pooled)\n",
    "        # no mask; signals: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "        \n",
    "        # split two signals: for classification & completion.\n",
    "        signal_classification = signals[:][0]                   # (batch, dim_hidden=128)\n",
    "        signal_completion = signals.clone()[:][1].unsqueeze(1)  # (batch, 1, dim_hidden=128)\n",
    "        \n",
    "        logit_classification, logit_completion = None, None\n",
    "        \n",
    "        # Classification:\n",
    "        if self.classify:\n",
    "            logit_classification = self.ff1(signal_classification)  # (batch, dim_output)\n",
    "        \n",
    "        # Completion:\n",
    "        if self.complete:\n",
    "            if bin_x is None:\n",
    "                bin_x = make_one_hot(x)\n",
    "            bool_x = (bin_x == True)\n",
    "            used_ingred_mask = bool_x.repeat(self.num_heads,1).unsqueeze(1)\n",
    "            # used_ingred_mask: (batch*num_heads, 1, num_items-1=6714)\n",
    "            \n",
    "            embedding_weight = self.embedding.weight[:-1].unsqueeze(0).repeat(feature.size(0),1,1)\n",
    "            # embedding_weight: (batch, num_items=6715, dim_input=256)\n",
    "            \n",
    "            for module in self.decoder2:\n",
    "                signal_completion = module(signal_completion, embedding_weight, mask=used_ingred_mask)\n",
    "            logit_completion = self.ff2(signal_completion.squeeze()) # (batch, num_items-1=6714)\n",
    "            logit_completion[bool_x] = float('-inf')\n",
    "\n",
    "        return logit_classification, logit_completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2609d3",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6d08699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB, early_stop patience 일단 뺐음\n",
    "\n",
    "def train(model,\n",
    "          dataloaders,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          metrics,\n",
    "          dataset_sizes,\n",
    "          device='cpu',\n",
    "          num_epochs=100,\n",
    "          #wandb_log=False, early_stop_patience=None\n",
    "          classify=True,\n",
    "          complete=True,\n",
    "          random_seed=1):\n",
    "    \n",
    "    assert isinstance(metrics, dict), f\"'metrics' argument should be a dictionary, but {type(metrics)}.\"\n",
    "    \n",
    "    def _concatenate(running_v, new_v):\n",
    "        if running_v is not None:\n",
    "            return np.concatenate((running_v, new_v.clone().detach().cpu().numpy()), axis=0)\n",
    "        else:\n",
    "            return new_v.clone().detach().cpu().numpy()\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    \"\"\"best_model_wts = deepcopy(model.state_dict())\n",
    "    best_loss = 1e4\n",
    "    \n",
    "    if early_stop_patience is not None:\n",
    "        if not isinstance(early_stop_patience, int):\n",
    "            raise TypeError('early_stop_patience should be an integer.')\n",
    "        patience_cnt = 0\"\"\"\n",
    "\n",
    "    print('-' * 5 + 'Training the model' + '-' * 5)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid_class', 'valid_compl']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                if not classify and phase == 'valid_class':\n",
    "                    continue\n",
    "                elif not complete and phase == 'valid_compl':\n",
    "                    continue\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects_compl = 0.0\n",
    "            running_labels_class = None\n",
    "            running_preds_class = None\n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (bin_inputs, int_inputs, bin_labels, int_labels) in enumerate(dataloaders[phase]):\n",
    "                if classify and phase in ['train', 'valid_class']:\n",
    "                    labels_class = int_labels.to(device)\n",
    "                if complete:\n",
    "                    # randomly remove one ingredient for each recipe/batch\n",
    "                    if phase == 'train':\n",
    "                        labels_compl = torch.zeros_like(int_labels)\n",
    "                        for batch in int_labels.size(0):\n",
    "                            ingreds = torch.arange(bin_inputs.size(-1))[bin_inputs[batch]==1]\n",
    "                            mask_ingred_idx = ingreds[np.randint(len(ingreds))]\n",
    "                            bin_inputs[batch][mask_ingred_idx] = 0\n",
    "                            int_inputs[batch][int_inputs == mask_ingred_idx] = int(bin_inputs.size(-1))\n",
    "                            labels_compl[batch] = mask_ingred_idx\n",
    "                    elif phase == 'valid_compl':\n",
    "                        labels_compl = int_labels.to(device)\n",
    "                bin_inputs = bin_inputs.to(device)\n",
    "                int_inputs = int_inputs.to(device)\n",
    "                \n",
    "                    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs_class, outputs_compl = model(int_inputs, bin_x=bin_inputs)  # bin_x 없어도 작동은 가능\n",
    "                    _, preds_class = torch.max(outputs_class, 1)\n",
    "                    _, preds_compl = torch.max(outputs_compl, 1)\n",
    "                    if idx == 0:\n",
    "                        if classify and phase in ['train', 'valid_class']:\n",
    "                            print('labels_classification', labels_class[0])\n",
    "                            print('outputs_classification', outputs_class[0])\n",
    "                            print('preds_classification', preds_class[0])\n",
    "                        if complete and phase in ['train', 'valid_compl']:\n",
    "                            print('labels_completion', labels_compl[0])\n",
    "                            print('outputs_completion', outputs_compl[0])\n",
    "                            print('preds_completion', preds_compl[0])\n",
    "                    \n",
    "                    if classify and complete and phase == 'train':\n",
    "                        loss = criterion(outputs_class, labels_class.long()) \\\n",
    "                                + criterion(outputs_compl, labels_compl.long())\n",
    "                    elif classify and phase in ['train', 'valid_class']:\n",
    "                        loss = criterion(outputs_class, labels_class.long())\n",
    "                    elif complete and phase in ['train', 'valid_compl']:\n",
    "                        loss = criterion(outputs_compl, labels_compl.long())\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * bin_inputs.size(0)\n",
    "                running_corrects_compl += torch.sum(preds_compl == labels_compl.data)  # for accuracy\n",
    "                \n",
    "                if classify and phase in ['train', 'valid_class']: # for F1 score\n",
    "                    running_labels_class = _concatenate(running_labels_class, labels_class)\n",
    "                    running_preds_class = _concatenate(running_preds_class, labels_class)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects_compl.double() / dataset_sizes[phase]  # completion task: accuracy.\n",
    "            running_labels_class = torch.from_numpy(running_labels_class)\n",
    "            running_preds_class = torch.from_numpy(running_preds_class)\n",
    "            epoch_macro_f1 = metrics['macro_f1'](running_labels_class, running_preds_class)  # classification: f1 scores.\n",
    "            epoch_micro_f1 = metrics['micro_f1'](running_labels_class, running_preds_class)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Macro-F1: {:.4f} Micro-F1: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, epoch_macro_f1, epoch_micro_f1))\n",
    "            \n",
    "            \"\"\"\n",
    "            if phase == 'train':\n",
    "                train_loss = epoch_loss\n",
    "                train_macro_f1 = epoch_macro_f1\n",
    "                train_micro_f1 = epoch_micro_f1\n",
    "                if wandb_log:\n",
    "                    wandb.watch(model)\n",
    "            elif phase == 'val':\n",
    "                val_loss = epoch_loss\n",
    "                val_macro_f1 = epoch_macro_f1\n",
    "                val_micro_f1 = epoch_micro_f1\n",
    "\n",
    "            # if phase == 'train':\n",
    "            #     scheduler.step()\n",
    "            if phase == 'val':\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "            if phase == 'val':\n",
    "                # deep copy the model\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    if early_stop_patience is not None:\n",
    "                        patience_cnt = 0\n",
    "                elif early_stop_patience is not None:\n",
    "                    patience_cnt += 1\n",
    "            \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        if wandb_log:\n",
    "            wandb.log({'train_loss': train_loss,\n",
    "                       'val_loss': val_loss,\n",
    "                       'train_macro_f1': train_macro_f1,\n",
    "                       'train_micro_f1': train_micro_f1,\n",
    "                       'val_macro_f1': val_macro_f1,\n",
    "                       'val_micro_f1': val_micro_f1,\n",
    "                       'best_val_loss': best_loss,\n",
    "                       'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "                                        # scheduler.get_last_lr()[0] for CosineAnnealingWarmRestarts\n",
    "\n",
    "        if early_stop_patience is not None:\n",
    "            if patience_cnt > early_stop_patience:\n",
    "                print(f'Early stop at epoch {epoch}.')\n",
    "                break\n",
    "        \"\"\"\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # after last epoch, generate confusion matrix of validation phase\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78a40c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
