{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98f208fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5py 안 될 때\n",
    "#!brew reinstall hdf5\n",
    "#!export CPATH=\"/opt/homebrew/include/\"\n",
    "#!export HDF5_DIR=/opt/homebrew/\n",
    "#!python3 -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0be0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd drive/MyDrive/cuisine-prediction/Hanseul/\n",
    "# !pip3 install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6909cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import F1Score\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdb3e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '../'\n",
    "path_container = './Container/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb2659",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4db71052",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_container + 'id_cuisine_dict.pickle', 'rb') as f:\n",
    "    id_cuisine_dict = pickle.load(f)\n",
    "with open(path_container + 'cuisine_id_dict.pickle', 'rb') as f:\n",
    "    cuisine_id_dict = pickle.load(f)\n",
    "with open(path_container + 'id_ingredient_dict.pickle', 'rb') as f:\n",
    "    id_ingredient_dict = pickle.load(f)\n",
    "with open(path_container + 'ingredient_id_dict.pickle', 'rb') as f:\n",
    "    ingredient_id_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d9249e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, data_dir, test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.test = test\n",
    "        with h5py.File(data_dir, 'r') as data_file:\n",
    "            self.bin_data = data_file['bin_data'][:]  # Size (num_recipes=23547, num_ingredients=6714)\n",
    "            if not test:\n",
    "                self.int_labels = data_file['int_labels'][:]  # Size (num_recipes=23547,), about cuisines\n",
    "                self.bin_labels = data_file['bin_labels'][:]  # Size (num_recipes=23547, 20), about cuisines\n",
    "        \n",
    "        self.padding_idx = self.bin_data.shape[1]  # == num_ingredient == 6714\n",
    "        self.max_num_ingredients_per_recipe = self.bin_data.sum(1).max()  # valid & test의 경우 65\n",
    "        \n",
    "        # (59나 65로) 고정된 길이의 row vector에 해당 recipe의 indices 넣고 나머지는 padding index로 채워넣기\n",
    "        # self.int_data: Size (num_recipes=23547, self.max_num_ingredients_per_recipe=59 or 65)\n",
    "        self.int_data = np.full((len(self.bin_data), self.max_num_ingredients_per_recipe), self.padding_idx) \n",
    "        for i, bin_recipe in enumerate(self.bin_data):\n",
    "            recipe = np.arange(self.padding_idx)[bin_recipe==1]\n",
    "            self.int_data[i][:len(recipe)] = recipe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.bin_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bin_data = self.bin_data[idx]\n",
    "        int_data = self.int_data[idx]\n",
    "        bin_label = None if self.test else self.bin_labels[idx]\n",
    "        int_label = None if self.test else self.int_labels[idx]\n",
    "        \n",
    "        return bin_data, int_data, bin_label, int_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f0e17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = ['train', 'valid_class', 'valid_compl', 'test_class', 'test_compl']\n",
    "\n",
    "recipe_datasets = {x: RecipeDataset(os.path.join(path_container, x), test='test' in x) for x in dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1bcc5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6714,)\n",
      "(59,)\n",
      "(20,)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "_bd,_id,_bl,_il = recipe_datasets['train'][0]\n",
    "print(_bd.shape)\n",
    "print(_id.shape)\n",
    "print(_bl.shape)\n",
    "print(_il.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559311cb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bd58c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building blocks of Set Transformers ##\n",
    "# added masks.\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False, dropout=0):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.p = dropout\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Sequential(\n",
    "            nn.Linear(dim_V, dim_V),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_V, dim_V))\n",
    "        self.Dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, Q, K, mask=None):\n",
    "        # Q (batch, q_len, d_hid)\n",
    "        # K (batch, k_len, d_hid)\n",
    "        # V (batch, v_len, d_hid == dim_V)\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "        \n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        \n",
    "        # Q_ (batch * num_heads, q_len, d_hid // num_heads)\n",
    "        # K_ (batch * num_heads, k_len, d_hid // num_heads)\n",
    "        # V_ (batch * num_heads, v_len, d_hid // num_heads)\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "        \n",
    "        # energy (batch * num_heads, q_len, k_len)\n",
    "        energy = Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V)\n",
    "        if mask is not None:\n",
    "            energy.masked_fill_(mask, float('-inf'))\n",
    "        A = torch.softmax(energy, 2)\n",
    "        \n",
    "        # O (batch, q_len, d_hid)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
    "        _O = self.fc_o(O)\n",
    "        if self.p > 0:\n",
    "            _O = self.Dropout(_O)\n",
    "        O = O + _O \n",
    "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False, dropout=0.2):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(X, X, mask=mask)\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False, dropout=0.2):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X, mask=mask)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False, dropout=0.2):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln, dropout=dropout)\n",
    "        \n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e90444e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(x):\n",
    "        \"\"\" Convert int_data into bin_data, if needed. \"\"\"\n",
    "        if type(x) is not torch.Tensor:\n",
    "            x = torch.LongTensor(x)\n",
    "        if x.dim() > 2:\n",
    "            x = x.squeeze()\n",
    "            if x.dim() > 2:\n",
    "                return False\n",
    "        elif x.dim() < 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        return F.one_hot(x).sum(1)[:,:-1]\n",
    "\n",
    "class CCNet(nn.Module):\n",
    "    def __init__(self, dim_embedding=256, #\n",
    "                 dim_output=20,\n",
    "                 num_items=6714, \n",
    "                 num_inds=32, \n",
    "                 dim_hidden=128, \n",
    "                 num_heads=4, \n",
    "                 num_outputs=1+1,  # classification 1 + completion 1\n",
    "                 num_enc_layers=4, \n",
    "                 num_dec_layers=2,\n",
    "                 ln=True,          # LayerNorm option\n",
    "                 dropout=0.2,      # Dropout option\n",
    "                 classify=True,    # completion만 하고 싶으면 False로\n",
    "                 complete=True):    # classification만 하고 싶으면 False로\n",
    "   \n",
    "        super(CCNet, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.padding_idx = num_items\n",
    "        self.classify, self.complete = classify, complete\n",
    "        self.embedding =  nn.Embedding(num_embeddings=num_items+1, embedding_dim=dim_embedding, padding_idx=-1)\n",
    "        \"\"\"\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [SAB(dim_embedding, dim_hidden, num_heads, ln=ln, dropout=dropout)] +\n",
    "            [SAB(dim_hidden, dim_hidden, num_heads, ln=ln, dropout=dropout) for _ in range(num_enc_layers-1)])\n",
    "        \"\"\"\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [ISAB(dim_embedding, dim_hidden, num_heads, num_inds, ln=ln, dropout=dropout)] +\n",
    "            [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln, dropout=dropout) for _ in range(num_enc_layers-1)])\n",
    "        #for p in self.encoder.parameters():\n",
    "        #    p.requires_grad = False\n",
    "        self.pooling = PMA(dim_hidden, num_heads, num_outputs, ln=ln)\n",
    "        #for p in self.pooling.parameters():\n",
    "        #    p.requires_grad = False\n",
    "        if classify:\n",
    "            self.decoder1 = nn.Sequential(\n",
    "                *[SAB(dim_hidden, dim_hidden, num_heads, ln=ln, dropout=dropout) for _ in range(num_dec_layers)])\n",
    "            self.ff1 = nn.Sequential(\n",
    "                nn.Linear(dim_hidden, dim_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim_hidden, dim_output))\n",
    "        if complete:\n",
    "            self.decoder2 = nn.ModuleList(\n",
    "                [MAB(dim_hidden, dim_embedding, dim_hidden, num_heads, ln=ln, dropout=dropout) for _ in range(num_dec_layers)])\n",
    "            self.ff2 = nn.Linear(dim_hidden, num_items)\n",
    "    \n",
    "    def forward(self, x, bin_x=None): \n",
    "        # x(=recipes): (batch, max_num_ingredient=65) : int_data.\n",
    "        if not (self.classify or self.complete):\n",
    "            return\n",
    "        #print('x',x)\n",
    "        #print(\"===\"*50)\n",
    "        feature = self.embedding(x)\n",
    "        #print('feature',feature)\n",
    "        #print('feature is NaN? :', torch.isnan(feature).any().item())\n",
    "        # feature: (batch, max_num_ingredient=65, dim_embedding=256)\n",
    "        # cf. embedding.weight: (num_items+1=6715, dim_embedding=256)\n",
    "\n",
    "        mask = (x == self.padding_idx).repeat(self.num_heads,1).unsqueeze(1)\n",
    "        #print('mask',mask)\n",
    "        #print('mask is NaN? :', torch.isnan(mask).any().item())\n",
    "        # mask: (batch*num_heads, 1, max_num_ingredient=65)\n",
    "        code = feature.clone()\n",
    "        #cnt = 0\n",
    "        #print(f'code{cnt}',code)\n",
    "        for module in self.encoder:\n",
    "            #cnt += 1\n",
    "            code = module(code, mask=mask)\n",
    "            #print(f'code{cnt}',code)\n",
    "            #print('code is NaN? :', torch.isnan(code).any().item())\n",
    "        # code: (batch, max_num_ingredient=65, dim_hidden=128) : permutation-equivariant.\n",
    "\n",
    "        pooled = self.pooling(code, mask=mask)\n",
    "        #print('pooled',pooled)\n",
    "        #print('pooled is NaN? :', torch.isnan(pooled).any().item())\n",
    "        # pooled: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "\n",
    "        signals = self.decoder1(pooled)\n",
    "        #print('signals', signals)\n",
    "        #print('signals is NaN? :', torch.isnan(signals).any().item())\n",
    "        # no mask; signals: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "\n",
    "        if signals.size(1) == 2 and self.classify and self.complete:\n",
    "            # split two signals: for classification & completion.\n",
    "            signal_classification, signal_completion = signals.chunk(2, dim=1)  # (batch, 1, dim_hidden=128) * 2\n",
    "            #print('signal_classification is NaN? :', torch.isnan(signal_classification).any().item())\n",
    "            #print('signal_completion is NaN? :', torch.isnan(signal_completion).any().item())\n",
    "        elif signals.size(1) == 1:\n",
    "            if self.classify and not self.complete:\n",
    "                signal_classification = signals\n",
    "            elif self.complete and not self.classify:\n",
    "                signal_completion = signals\n",
    "        else:\n",
    "            raise ValueError(f\"num_outputs={signals.size(1)}; but classify={self.classify} and complete={self.complete}\")\n",
    "        #print('signal_classification', signal_classification)\n",
    "\n",
    "        logit_classification, logit_completion = None, None\n",
    "\n",
    "        # Classification:\n",
    "        if self.classify:\n",
    "            logit_classification = self.ff1(signal_classification.squeeze(1))  # (batch, dim_output)\n",
    "            #print('logit_classification', logit_classification)\n",
    "            #print('logit_classification is NaN? :', torch.isnan(logit_classification).any().item())\n",
    "        \n",
    "        # Completion:\n",
    "        if self.complete:\n",
    "            if bin_x is None:\n",
    "                bin_x = make_one_hot(x)\n",
    "            bool_x = (bin_x == True)\n",
    "            #print('bool_x is NaN? :', torch.isnan(bool_x).any().item())\n",
    "\n",
    "            used_ingred_mask = bool_x.repeat(self.num_heads,1).unsqueeze(1)\n",
    "            #print('used_ingred_mask is NaN? :', torch.isnan(used_ingred_mask).any().item())\n",
    "            # used_ingred_mask: (batch*num_heads, 1, num_items=6714)\n",
    "            \n",
    "            embedding_weight = self.embedding.weight[:-1].unsqueeze(0).repeat(feature.size(0),1,1)\n",
    "            #print('embedding_weight is NaN? :', torch.isnan(embedding_weight).any().item())\n",
    "            # embedding_weight: (batch, num_items+1=6715, dim_embedding=256)\n",
    "            \n",
    "            \n",
    "            for module in self.decoder2:\n",
    "                signal_completion = module(signal_completion, embedding_weight, mask=used_ingred_mask)\n",
    "                #print('signal_completion is NaN? :', torch.isnan(signal_completion).any().item())\n",
    "            logit_completion = self.ff2(signal_completion.squeeze()) # (batch, num_items=6714)\n",
    "            #print('logit_completion is NaN? :', torch.isnan(logit_completion).any().item())\n",
    "            logit_completion[bool_x] = float('-inf')\n",
    "\n",
    "        return logit_classification, logit_completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2609d3",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6d08699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB, 일단 뺐음\n",
    "\n",
    "def train(model,\n",
    "          dataloaders,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          metrics,\n",
    "          dataset_sizes,\n",
    "          device='cpu',\n",
    "          num_epochs=20,\n",
    "          #wandb_log=False,\n",
    "          early_stop_patience=None,\n",
    "          classify=True,\n",
    "          complete=True,\n",
    "          random_seed=1):\n",
    "    \n",
    "    assert isinstance(metrics, dict), f\"'metrics' argument should be a dictionary, but {type(metrics)}.\"\n",
    "    \n",
    "    def _concatenate(running_v, new_v):\n",
    "        if running_v is not None:\n",
    "            return np.concatenate((running_v, new_v.clone().detach().cpu().numpy()), axis=0)\n",
    "        else:\n",
    "            return new_v.clone().detach().cpu().numpy()\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    torch.random.manual_seed(random_seed)\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_loss = 1e4\n",
    "    \n",
    "    \n",
    "    if early_stop_patience is not None:\n",
    "        if not isinstance(early_stop_patience, int):\n",
    "            raise TypeError('early_stop_patience should be an integer.')\n",
    "        patience_cnt = 0\n",
    "    \n",
    "\n",
    "    print('-' * 5 + 'Training the model' + '-' * 5)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid_class', 'valid_compl']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                if not classify and phase == 'valid_class':\n",
    "                    continue\n",
    "                elif not complete and phase == 'valid_compl':\n",
    "                    continue\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects_compl = 0.0\n",
    "            running_corrects_class = 0.0\n",
    "            running_labels_class = None\n",
    "            running_preds_class = None\n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (bin_inputs, int_inputs, bin_labels, int_labels) in enumerate(dataloaders[phase]):\n",
    "                #print(\"==\", idx, \"==\")\n",
    "                if classify and phase in ['train', 'valid_class']:\n",
    "                    labels_class = int_labels.to(device)\n",
    "                if complete:\n",
    "                    # randomly remove one ingredient for each recipe/batch\n",
    "                    if phase == 'train':\n",
    "                        labels_compl = torch.zeros_like(int_labels)\n",
    "                        for batch in range(int_labels.size(0)):\n",
    "                            ingreds = torch.arange(bin_inputs.size(-1))[bin_inputs[batch]==1]\n",
    "                            mask_ingred_idx = ingreds[np.random.randint(len(ingreds))]\n",
    "                            bin_inputs[batch][mask_ingred_idx] = 0\n",
    "                            int_inputs[batch][int_inputs[batch] == mask_ingred_idx] = int(bin_inputs.size(-1))\n",
    "                            labels_compl[batch] = mask_ingred_idx\n",
    "                        labels_compl = labels_compl.to(device)\n",
    "                    elif phase == 'valid_compl':\n",
    "                        labels_compl = int_labels.to(device)\n",
    "                bin_inputs = bin_inputs.to(device)\n",
    "                int_inputs = int_inputs.to(device)\n",
    "                \n",
    "                    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs_class, outputs_compl = model(int_inputs, bin_x=bin_inputs)  # bin_x 없어도 작동은 가능\n",
    "                    if classify and phase in ['train', 'valid_class']:\n",
    "                        _, preds_class = torch.max(outputs_class, 1)\n",
    "                    if complete and phase in ['train', 'valid_compl']:\n",
    "                        _, preds_compl = torch.max(outputs_compl, 1)\n",
    "\n",
    "                    if idx == 0 and phase == 'train':  # 원래 idx == 0 \n",
    "                        if classify and phase in ['train', 'valid_class']:\n",
    "                            print('labels_classification', labels_class.cpu().numpy())\n",
    "                            #print('outputs_classification', outputs_class.clone().detach().cpu().numpy())\n",
    "                            print('preds_classification', preds_class.cpu().numpy())\n",
    "                        if complete and phase in ['train', 'valid_compl']:\n",
    "                            print('labels_completion', labels_compl.cpu().numpy())\n",
    "                            #print('outputs_completion', outputs_compl[0])\n",
    "                            print('preds_completion', preds_compl.cpu().numpy())\n",
    "\n",
    "                    if classify and complete and phase == 'train':\n",
    "                        loss = criterion(outputs_class, labels_class.long()) \\\n",
    "                                + criterion(outputs_compl, labels_compl.long())\n",
    "                    elif classify and phase in ['train', 'valid_class']:\n",
    "                        loss = criterion(outputs_class, labels_class.long())\n",
    "                    elif complete and phase in ['train', 'valid_compl']:\n",
    "                        loss = criterion(outputs_compl, labels_compl.long())\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                        optimizer.step()\n",
    "\n",
    "                if idx % 100 == 0:\n",
    "                    print(f'    {phase} {idx * 100 // len(dataloaders[phase]):3d}% of an epoch | Loss: {loss.item()}')\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * bin_inputs.size(0)\n",
    "                if classify and phase in ['train', 'valid_class']: # for F1 score & accuracy\n",
    "                    running_labels_class = _concatenate(running_labels_class, labels_class)\n",
    "                    running_preds_class = _concatenate(running_preds_class, labels_class)\n",
    "                    running_corrects_class += torch.sum(preds_class == labels_class.data)\n",
    "                if complete and phase in ['train', 'valid_compl']: # for accuracy\n",
    "                    running_corrects_compl += torch.sum(preds_compl == labels_compl.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            log_str = f'{phase.upper()} Loss: {epoch_loss:.4f} '\n",
    "            if classify and phase in ['train', 'valid_class']:\n",
    "                running_labels_class = torch.from_numpy(running_labels_class)\n",
    "                running_preds_class = torch.from_numpy(running_preds_class)\n",
    "                epoch_macro_f1 = metrics['macro_f1'](running_labels_class, running_preds_class)  # classification: f1 scores.\n",
    "                epoch_micro_f1 = metrics['micro_f1'](running_labels_class, running_preds_class)\n",
    "                epoch_acc_class = running_corrects_class / dataset_sizes[phase]\n",
    "                log_str += f'Acc(classif.): {epoch_acc_class:.4f} Macro-F1: {epoch_macro_f1:.4f} Micro-F1: {epoch_micro_f1:.4f} '\n",
    "            if complete and phase in ['train', 'valid_compl']:\n",
    "                epoch_acc_compl = running_corrects_compl / dataset_sizes[phase]  # completion task: accuracy.\n",
    "                log_str += f'Acc(compl.): {epoch_acc_compl:.4f} '\n",
    "            print(log_str)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = epoch_loss\n",
    "                train_macro_f1 = epoch_macro_f1\n",
    "                train_micro_f1 = epoch_micro_f1\n",
    "                # if wandb_log:\n",
    "                #     wandb.watch(model)\n",
    "            elif 'val' in phase:\n",
    "                val_loss = epoch_loss\n",
    "                val_macro_f1 = epoch_macro_f1\n",
    "                val_micro_f1 = epoch_micro_f1\n",
    "            # if phase == 'train':\n",
    "            #     scheduler.step()\n",
    "            if 'val' in phase:\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "            if 'val' in phase:\n",
    "                # deep copy the model\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = deepcopy(model.state_dict())\n",
    "                    if early_stop_patience is not None:\n",
    "                        patience_cnt = 0\n",
    "                elif early_stop_patience is not None:\n",
    "                    patience_cnt += 1\n",
    "\n",
    "        \"\"\"\n",
    "        if wandb_log:\n",
    "            wandb.log({'train_loss': train_loss,\n",
    "                       'val_loss': val_loss,\n",
    "                       'train_macro_f1': train_macro_f1,\n",
    "                       'train_micro_f1': train_micro_f1,\n",
    "                       'val_macro_f1': val_macro_f1,\n",
    "                       'val_micro_f1': val_micro_f1,\n",
    "                       'best_val_loss': best_loss,\n",
    "                       'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "                                        # scheduler.get_last_lr()[0] for CosineAnnealingWarmRestarts\n",
    "        \"\"\"\n",
    "        if early_stop_patience is not None:\n",
    "            if patience_cnt > early_stop_patience:\n",
    "                print(f'Early stop at epoch {epoch}.')\n",
    "                break\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # after last epoch, generate confusion matrix of validation phase\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4405ba",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae7f8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(dim_embedding=256,\n",
    "               dropout=0.2,\n",
    "               subset_length=None,\n",
    "               batch_size=16,\n",
    "               n_epochs=50,\n",
    "               lr=1e-3,\n",
    "               step_size=10,  # training scheduler\n",
    "               seed=0,\n",
    "               classify=True,\n",
    "               complete=True):\n",
    "    \n",
    "    dataset_name = ['train', 'valid_class', 'valid_compl']\n",
    "    if subset_length is None:\n",
    "        dataloaders = {x: DataLoader(recipe_datasets[x], batch_size=batch_size,\n",
    "                                    shuffle=True) for x in dataset_name}\n",
    "        dataset_sizes = {x: len(recipe_datasets[x]) for x in dataset_name}\n",
    "    else:\n",
    "        dataloaders = {x: DataLoader(Subset(recipe_datasets[x], range(subset_length)), batch_size=batch_size,\n",
    "                                    shuffle=True) for x in dataset_name}\n",
    "        dataset_sizes = {x: len(Subset(recipe_datasets[x], range(subset_length))) for x in dataset_name}\n",
    "    \n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('device: ', device)\n",
    "\n",
    "    # Get a batch of training data\n",
    "    bin_inputs, int_inputs, bin_labels, int_labels = next(iter(dataloaders['train']))\n",
    "    print('inputs.shape', bin_inputs.shape, int_inputs.shape)\n",
    "    print('labels.shape', bin_labels.shape, int_labels.shape)\n",
    "\n",
    "    model_ft = CCNet(dim_embedding=dim_embedding, dim_output=len(bin_labels[0]),\n",
    "                     num_items=len(bin_inputs[0]), num_outputs=2 if classify and complete else 1,\n",
    "                     num_enc_layers=4, num_dec_layers=2, ln=True, dropout=0.5,\n",
    "                     classify=classify, complete=complete).to(device)\n",
    "    #print(model_ft)\n",
    "    total_params = sum(dict((p.data_ptr(), p.numel()) for p in model_ft.parameters() if p.requires_grad ).values())\n",
    "    print(\"Total Number of Parameters\", total_params)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optimizer = optim.AdamW([p for p in model_ft.parameters() if p.requires_grad == True],\n",
    "                                        lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.2)\n",
    "    exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=step_size,\n",
    "                                                      eps=1e-08, verbose=True)\n",
    "    # (metric) F1Score objects\n",
    "    macro_f1 = F1Score(num_classes=20, average='macro')\n",
    "    micro_f1 = F1Score(num_classes=20, average='micro')\n",
    "    metrics = {'macro_f1': macro_f1, 'micro_f1': micro_f1}\n",
    "\n",
    "    model_ft = train(model_ft, dataloaders, criterion, optimizer, exp_lr_scheduler, metrics,\n",
    "                     dataset_sizes, device=device, num_epochs=n_epochs, early_stop_patience=20,\n",
    "                     classify=classify, complete=complete, random_seed=seed)\n",
    "    \n",
    "    fname = ['ckpt', 'CCNet', 'batch', str(batch_size),\n",
    "             'n_epochs', str(n_epochs), 'lr', str(lr), 'step_size', str(step_size),\n",
    "             'seed', str(seed)]\n",
    "    fname = '_'.join(fname) + '.pt'\n",
    "    if not os.path.isdir('./weights/'):\n",
    "        os.mkdir('./weights/')\n",
    "    torch.save(model_ft.state_dict(), os.path.join('./weights/', fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbbf3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry Run\n",
    "#experiment(n_epochs=1, lr=1e-6, dim_embedding=64, classify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb37603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment\n",
    "experiment(batch_size=16, n_epochs=100, lr=1e-3, dim_embedding=256, complete=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
