{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98f208fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5py 안 될 때\n",
    "#!brew reinstall hdf5\n",
    "#!export CPATH=\"/opt/homebrew/include/\"\n",
    "#!export HDF5_DIR=/opt/homebrew/\n",
    "#!python3 -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0be0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd drive/MyDrive/cuisine-prediction/Hanseul/\n",
    "# !pip3 install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6909cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb3e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '../'\n",
    "path_container = './Container/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb2659",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db71052",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_container + 'id_cuisine_dict.pickle', 'rb') as f:\n",
    "    id_cuisine_dict = pickle.load(f)\n",
    "with open(path_container + 'cuisine_id_dict.pickle', 'rb') as f:\n",
    "    cuisine_id_dict = pickle.load(f)\n",
    "with open(path_container + 'id_ingredient_dict.pickle', 'rb') as f:\n",
    "    id_ingredient_dict = pickle.load(f)\n",
    "with open(path_container + 'ingredient_id_dict.pickle', 'rb') as f:\n",
    "    ingredient_id_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9249e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, data_dir, test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.test = test\n",
    "        with h5py.File(data_dir, 'r') as data_file:\n",
    "            self.bin_data = data_file['bin_data'][:]  # Size (num_recipes=23547, num_ingredients=6714)\n",
    "            if not test:\n",
    "                self.int_labels = data_file['int_labels'][:]  # Size (num_recipes=23547,), about cuisines\n",
    "                self.bin_labels = data_file['bin_labels'][:]  # Size (num_recipes=23547, 20), about cuisines\n",
    "        \n",
    "        self.padding_idx = self.bin_data.shape[1]  # == num_ingredient == 6714\n",
    "        self.max_num_ingredients_per_recipe = self.bin_data.sum(1).max()  # valid & test의 경우 65\n",
    "        \n",
    "        # (59나 65로) 고정된 길이의 row vector에 해당 recipe의 indices 넣고 나머지는 padding index로 채워넣기\n",
    "        # self.int_data: Size (num_recipes=23547, self.max_num_ingredients_per_recipe=59 or 65)\n",
    "        self.int_data = np.full((len(self.bin_data), self.max_num_ingredients_per_recipe), self.padding_idx) \n",
    "        for i, bin_recipe in enumerate(self.bin_data):\n",
    "            recipe = np.arange(self.padding_idx)[bin_recipe==1]\n",
    "            self.int_data[i][:len(recipe)] = recipe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.bin_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bin_data = self.bin_data[idx]\n",
    "        int_data = self.int_data[idx]\n",
    "        bin_label = None if self.test else self.bin_labels[idx]\n",
    "        int_label = None if self.test else self.int_labels[idx]\n",
    "        \n",
    "        return bin_data, int_data, bin_label, int_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0e17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = ['train', 'valid_class', 'valid_compl', 'test_class', 'test_compl']\n",
    "\n",
    "recipe_datasets = {x: RecipeDataset(os.path.join(path_container, x), test='test' in x) for x in dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1bcc5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "[564, 1263, 4074, 4203, 4901, 5277, 5360, 6232, 7835, 10585, 10777, 12476, 13301, 13989, 15951, 17374, 18153, 19039, 20469]\n"
     ]
    }
   ],
   "source": [
    "count_single_ingredient_recipe = 0\n",
    "list_single_ingredient_recipe = []\n",
    "for i in range(len(recipe_datasets['train'])):\n",
    "    _bd, _,_,_ = recipe_datasets['train'][i]\n",
    "    if _bd.sum()<2:\n",
    "        count_single_ingredient_recipe += 1\n",
    "        list_single_ingredient_recipe.append(i)\n",
    "print(count_single_ingredient_recipe)\n",
    "print(list_single_ingredient_recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559311cb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd58c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building blocks of Set Transformers ##\n",
    "# added masks.\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False, dropout=0):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.p = dropout\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Sequential(\n",
    "            nn.Linear(dim_V, dim_V),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_V, dim_V))\n",
    "        self.Dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, Q, K, mask=None):\n",
    "        # Q (batch, q_len, d_hid)\n",
    "        # K (batch, k_len, d_hid)\n",
    "        # V (batch, v_len, d_hid == dim_V)\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "        \n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        \n",
    "        # Q_ (batch * num_heads, q_len, d_hid // num_heads)\n",
    "        # K_ (batch * num_heads, k_len, d_hid // num_heads)\n",
    "        # V_ (batch * num_heads, v_len, d_hid // num_heads)\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "        \n",
    "        # energy (batch * num_heads, q_len, k_len)\n",
    "        energy = Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V)\n",
    "        if mask is not None:\n",
    "            energy.masked_fill_(mask, float('-inf'))\n",
    "        A = torch.softmax(energy, 2)\n",
    "        \n",
    "        # O (batch, q_len, d_hid)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
    "        _O = self.fc_o(O)\n",
    "        if self.p > 0:\n",
    "            _O = self.Dropout(_O)\n",
    "        O = O + _O \n",
    "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False, dropout=0.2):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(X, X, mask=mask)\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False, dropout=0.2):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X, mask=mask)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False, dropout=0.2):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln, dropout=dropout)\n",
    "        \n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e90444e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(x):\n",
    "        \"\"\" Convert int_data into bin_data, if needed. \"\"\"\n",
    "        if type(x) is not torch.Tensor:\n",
    "            x = torch.LongTensor(x)\n",
    "        if x.dim() > 2:\n",
    "            x = x.squeeze()\n",
    "            if x.dim() > 2:\n",
    "                return False\n",
    "        elif x.dim() < 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        return F.one_hot(x).sum(1)[:,:-1]\n",
    "\n",
    "class CCNet(nn.Module):\n",
    "    def __init__(self, dim_embedding=256, #\n",
    "                 dim_output=20,\n",
    "                 num_items=6714, \n",
    "                 num_inds=32, \n",
    "                 dim_hidden=128, \n",
    "                 num_heads=4, \n",
    "                 num_outputs=1+1,  # classification 1 + completion 1\n",
    "                 num_enc_layers=4, \n",
    "                 num_dec_layers=2,\n",
    "                 ln=True,          # LayerNorm option\n",
    "                 dropout=0.2,      # Dropout option\n",
    "                 classify=True,    # completion만 하고 싶으면 False로\n",
    "                 complete=True,    # classification만 하고 싶으면 False로\n",
    "                 freeze_classify=False, # classification만 관련된 parameter freeze\n",
    "                 freeze_complete=False  # completion만 관련된 parameter freeze\n",
    "                 ):\n",
    "   \n",
    "        super(CCNet, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.padding_idx = num_items\n",
    "        self.classify, self.complete = classify, complete\n",
    "        self.embedding =  nn.Embedding(num_embeddings=num_items+1, embedding_dim=dim_embedding, padding_idx=-1)\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [ISAB(dim_embedding, dim_hidden, num_heads, num_inds, ln=ln, dropout=dropout)] +\n",
    "            [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln, dropout=dropout) for _ in range(num_enc_layers-1)])\n",
    "        self.pooling = PMA(dim_hidden, num_heads, num_outputs, ln=ln)\n",
    "        self.decoder1 = nn.Sequential(\n",
    "                *[SAB(dim_hidden, dim_hidden, num_heads, ln=ln, dropout=dropout) for _ in range(num_dec_layers)])\n",
    "        if classify:\n",
    "            self.ff1 = nn.Sequential(\n",
    "                nn.Linear(dim_hidden, dim_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim_hidden, dim_output))\n",
    "            if freeze_classify:\n",
    "                for p in self.ff1.parameters():\n",
    "                    p.requires_grad = False\n",
    "        if complete:\n",
    "            self.decoder2 = nn.ModuleList(\n",
    "                [MAB(dim_hidden, dim_embedding, dim_hidden, num_heads, ln=ln, dropout=dropout) for _ in range(num_dec_layers)])\n",
    "            self.ff2 = nn.Linear(dim_hidden, num_items)\n",
    "            if freeze_complete:\n",
    "                for p in self.decoder2.parameters():\n",
    "                    p.requires_grad = False\n",
    "                for p in self.ff2.parameters():\n",
    "                    p.requires_grad = False\n",
    "    \n",
    "    def forward(self, x, bin_x=None): \n",
    "        # x(=recipes): (batch, max_num_ingredient=65) : int_data.\n",
    "        if not (self.classify or self.complete):\n",
    "            return\n",
    "        feature = self.embedding(x)\n",
    "        # feature: (batch, max_num_ingredient=65, dim_embedding=256)\n",
    "        # cf. embedding.weight: (num_items+1=6715, dim_embedding=256)\n",
    "\n",
    "        mask = (x == self.padding_idx).repeat(self.num_heads,1).unsqueeze(1)\n",
    "        # mask: (batch*num_heads, 1, max_num_ingredient=65)\n",
    "        \n",
    "        code = feature.clone()\n",
    "        for module in self.encoder:\n",
    "            code = module(code, mask=mask)\n",
    "        # code: (batch, max_num_ingredient=65, dim_hidden=128) : permutation-equivariant.\n",
    "\n",
    "        pooled = self.pooling(code, mask=mask)\n",
    "        # pooled: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "\n",
    "        signals = self.decoder1(pooled)\n",
    "        # no mask; signals: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "\n",
    "        if signals.size(1) == 2 and self.classify and self.complete:\n",
    "            # split two signals: for classification & completion.\n",
    "            signal_classification, signal_completion = signals.chunk(2, dim=1)  # (batch, 1, dim_hidden=128) * 2\n",
    "        elif signals.size(1) == 1:\n",
    "            if self.classify and not self.complete:\n",
    "                signal_classification = signals\n",
    "            elif self.complete and not self.classify:\n",
    "                signal_completion = signals\n",
    "        else:\n",
    "            raise ValueError(f\"num_outputs={signals.size(1)}; but classify={self.classify} and complete={self.complete}\")\n",
    "\n",
    "        logit_classification, logit_completion = None, None\n",
    "\n",
    "        # Classification:\n",
    "        if self.classify:\n",
    "            logit_classification = self.ff1(signal_classification.squeeze(1))  # (batch, dim_output)\n",
    "        \n",
    "        # Completion:\n",
    "        if self.complete:\n",
    "            if bin_x is None:\n",
    "                bin_x = make_one_hot(x)\n",
    "            bool_x = (bin_x == True)\n",
    "\n",
    "            used_ingred_mask = bool_x.repeat(self.num_heads,1).unsqueeze(1)\n",
    "            # used_ingred_mask: (batch*num_heads, 1, num_items=6714)\n",
    "            \n",
    "            embedding_weight = self.embedding.weight[:-1].unsqueeze(0).repeat(feature.size(0),1,1)\n",
    "            # embedding_weight: (batch, num_items+1=6715, dim_embedding=256)\n",
    "            \n",
    "            for module in self.decoder2:\n",
    "                signal_completion = module(signal_completion, embedding_weight, mask=used_ingred_mask)\n",
    "            logit_completion = self.ff2(signal_completion.squeeze()) # (batch, num_items=6714)\n",
    "            logit_completion[bool_x] = float('-inf')\n",
    "\n",
    "        return logit_classification, logit_completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2609d3",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d08699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB, 일단 뺐음\n",
    "# 0415: valid loss는 classification과 completion 각각 구해서 합한 것을 취해야 할 것으로 보임.\n",
    "# ____: f1_score는 sklearn.metrics.f1_score로 대체, torchmetrics의 dependency 제거, 관련 오류 고침(f1score 항상 1 나오는 오류)\n",
    "\n",
    "def train(model,\n",
    "          dataloaders,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          #metrics,\n",
    "          dataset_sizes,\n",
    "          device='cpu',\n",
    "          num_epochs=20,\n",
    "          #wandb_log=False,\n",
    "          early_stop_patience=None,\n",
    "          classify=True,\n",
    "          complete=True,\n",
    "          random_seed=1):\n",
    "\n",
    "    def _concatenate(running_v, new_v):\n",
    "        if running_v is not None:\n",
    "            return np.concatenate((running_v, new_v.clone().detach().cpu().numpy()), axis=0)\n",
    "        else:\n",
    "            return new_v.clone().detach().cpu().numpy()\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    torch.random.manual_seed(random_seed)\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_loss = 1e4\n",
    "    \n",
    "    if early_stop_patience is not None:\n",
    "        if not isinstance(early_stop_patience, int):\n",
    "            raise TypeError('early_stop_patience should be an integer.')\n",
    "        patience_cnt = 0\n",
    "    \n",
    "    print('-' * 5 + 'Training the model' + '-' * 5)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        val_loss = 0. # sum of classification and completion loss\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid_class', 'valid_compl']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                if not classify and phase == 'valid_class':\n",
    "                    continue\n",
    "                elif not complete and phase == 'valid_compl':\n",
    "                    continue\n",
    "\n",
    "            running_loss_class = 0.\n",
    "            running_loss_compl = 0.\n",
    "            running_corrects_compl = 0.\n",
    "            running_corrects_class = 0.\n",
    "            running_labels_class = None\n",
    "            running_preds_class = None\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for idx, (bin_inputs, int_inputs, bin_labels, int_labels) in enumerate(dataloaders[phase]):\n",
    "                batch_size = bin_inputs.size(0)\n",
    "                if classify and phase in ['train', 'valid_class']:\n",
    "                    labels_class = int_labels.to(device)\n",
    "                if complete:\n",
    "                    # randomly remove one ingredient for each recipe/batch\n",
    "                    if phase == 'train':\n",
    "                        labels_compl = torch.zeros_like(int_labels)\n",
    "                        for batch in range(batch_size):\n",
    "                            ingreds = torch.arange(bin_inputs.size(-1))[bin_inputs[batch]==1]\n",
    "                            if len(ingreds) < 2:\n",
    "                                raise RuntimeError(\"Train data has a single-ingredient recipe\")\n",
    "                            mask_ingred_idx = ingreds[np.random.randint(len(ingreds))]\n",
    "                            bin_inputs[batch][mask_ingred_idx] = 0\n",
    "                            int_inputs[batch][int_inputs[batch] == mask_ingred_idx] = int(bin_inputs.size(-1))\n",
    "                            labels_compl[batch] = mask_ingred_idx\n",
    "                        labels_compl = labels_compl.to(device)\n",
    "                    elif phase == 'valid_compl':\n",
    "                        labels_compl = int_labels.to(device)\n",
    "                bin_inputs = bin_inputs.to(device)\n",
    "                int_inputs = int_inputs.to(device)\n",
    "                    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs_class, outputs_compl = model(int_inputs, bin_x=bin_inputs)  # bin_x 없어도 작동은 가능\n",
    "                    if classify and phase in ['train', 'valid_class']:\n",
    "                        _, preds_class = torch.max(outputs_class, 1)\n",
    "                    if complete and phase in ['train', 'valid_compl']:\n",
    "                        _, preds_compl = torch.max(outputs_compl, 1)\n",
    "\n",
    "                    if idx == 0 and phase == 'train':  # 원래 idx == 0 \n",
    "                        if classify and phase in ['train', 'valid_class']:\n",
    "                            print('labels_classification', labels_class.cpu().numpy())\n",
    "                            #print('outputs_classification', outputs_class.clone().detach().cpu().numpy())\n",
    "                            print('preds_classification', preds_class.cpu().numpy())\n",
    "                        if complete and phase in ['train', 'valid_compl']:\n",
    "                            print('labels_completion', labels_compl.cpu().numpy())\n",
    "                            #print('outputs_completion', outputs_compl[0])\n",
    "                            print('preds_completion', preds_compl.cpu().numpy())\n",
    "                    \n",
    "                    if classify and phase in ['train', 'valid_class']:\n",
    "                        loss_class = criterion(outputs_class, labels_class.long())\n",
    "                    if complete and phase in ['train', 'valid_compl']:\n",
    "                        loss_compl = criterion(outputs_compl, labels_compl.long())\n",
    "\n",
    "                    if classify and complete and phase == 'train':\n",
    "                        loss = loss_class + loss_compl\n",
    "                    elif classify and phase in ['train', 'valid_class']:\n",
    "                        loss = loss_class\n",
    "                    elif complete and phase in ['train', 'valid_compl']:\n",
    "                        loss = loss_compl\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # gradient clipping\n",
    "                        optimizer.step()\n",
    "\n",
    "                if idx % 100 == 0:\n",
    "                    log_str = f'    {phase} {idx * 100 // len(dataloaders[phase]):3d}% of an epoch | '\n",
    "                    if classify and phase in ['train', 'valid_class']:\n",
    "                        log_str += f'Loss(classif.): {loss_class.item():.4f} | '\n",
    "                    if complete and phase in ['train', 'valid_compl']:\n",
    "                        log_str += f'Loss(complet.): {loss_compl.item():.4f} | '\n",
    "                    print(log_str)\n",
    "\n",
    "                # statistics\n",
    "                if classify and phase in ['train', 'valid_class']: # for F1 score & accuracy\n",
    "                    running_loss_class += loss_class.item() * batch_size\n",
    "                    running_labels_class = _concatenate(running_labels_class, labels_class)\n",
    "                    running_preds_class = _concatenate(running_preds_class, preds_class)\n",
    "                    running_corrects_class += torch.sum(preds_class == labels_class.data)\n",
    "                if complete and phase in ['train', 'valid_compl']: # for accuracy\n",
    "                    running_loss_compl += loss_compl.item() * batch_size\n",
    "                    running_corrects_compl += torch.sum(preds_compl == labels_compl.data)\n",
    "\n",
    "\n",
    "            epoch_loss = 0.\n",
    "            log_str = f'{phase.upper()} | '\n",
    "            if classify and phase in ['train', 'valid_class']:\n",
    "                epoch_loss_class = running_loss_class / dataset_sizes[phase]\n",
    "                epoch_loss += epoch_loss_class\n",
    "                running_labels_class = torch.from_numpy(running_labels_class)\n",
    "                running_preds_class = torch.from_numpy(running_preds_class)\n",
    "                epoch_macro_f1 = f1_score(running_labels_class, running_preds_class, average='macro')  # classification: f1 scores.\n",
    "                epoch_micro_f1 = f1_score(running_labels_class, running_preds_class, average='micro')\n",
    "                epoch_acc_class = running_corrects_class / dataset_sizes[phase]\n",
    "                log_str += f'Loss(classif.): {epoch_loss_class:.3f} Acc(classif.): {epoch_acc_class:.3f} Macro-F1: {epoch_macro_f1:.3f} Micro-F1: {epoch_micro_f1:.3f} | '\n",
    "            if complete and phase in ['train', 'valid_compl']:\n",
    "                epoch_loss_compl = running_loss_compl / dataset_sizes[phase]\n",
    "                epoch_loss += epoch_loss_compl\n",
    "                epoch_acc_compl = running_corrects_compl / dataset_sizes[phase]  # completion task: accuracy.\n",
    "                log_str += f'Loss(complet.): {epoch_loss_compl:.3f} Acc(complet.): {epoch_acc_compl:.3f} | '\n",
    "            print(log_str)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = epoch_loss\n",
    "                if classify:\n",
    "                    train_macro_f1 = epoch_macro_f1\n",
    "                    train_micro_f1 = epoch_micro_f1\n",
    "                # if wandb_log:\n",
    "                #     wandb.watch(model)\n",
    "            elif 'val' in phase:\n",
    "                val_loss += epoch_loss\n",
    "                if classify and phase == 'valid_class':\n",
    "                    val_macro_f1 = epoch_macro_f1\n",
    "                    val_micro_f1 = epoch_micro_f1\n",
    "            \n",
    "        if 'val' in phase:\n",
    "            scheduler.step(val_loss)\n",
    "            # deep copy the model\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "                if early_stop_patience is not None:\n",
    "                    patience_cnt = 0\n",
    "            elif early_stop_patience is not None:\n",
    "                patience_cnt += 1\n",
    "\n",
    "        \"\"\"\n",
    "        if wandb_log:\n",
    "            wandb.log({'train_loss': train_loss,\n",
    "                       'val_loss': val_loss,\n",
    "                       'train_macro_f1': train_macro_f1,\n",
    "                       'train_micro_f1': train_micro_f1,\n",
    "                       'val_macro_f1': val_macro_f1,\n",
    "                       'val_micro_f1': val_micro_f1,\n",
    "                       'best_val_loss': best_loss,\n",
    "                       'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "                                        # scheduler.get_last_lr()[0] for CosineAnnealingWarmRestarts\n",
    "        \"\"\"\n",
    "        if early_stop_patience is not None:\n",
    "            if patience_cnt > early_stop_patience:\n",
    "                print(f'Early stop at epoch {epoch}.')\n",
    "                break\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4405ba",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae7f8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(dim_embedding=256,\n",
    "               dropout=0.2,\n",
    "               subset_length=None,\n",
    "               exclude_idx=[],\n",
    "               batch_size=16,\n",
    "               n_epochs=50,\n",
    "               lr=1e-3,\n",
    "               step_size=10,  # training scheduler\n",
    "               seed=0,\n",
    "               classify=True,\n",
    "               complete=True,\n",
    "               freeze_classify=False,\n",
    "               freeze_complete=False\n",
    "               ):\n",
    "    \n",
    "    dataset_name = ['train', 'valid_class', 'valid_compl']\n",
    "    subset_indices = {x: [i for i in range(len(recipe_datasets[x]) if subset_length is None else subset_length)\n",
    "                            if i not in exclude_idx] for x in dataset_name}\n",
    "    dataloaders = {x: DataLoader(Subset(recipe_datasets[x], subset_indices[x]),\n",
    "                                 batch_size=batch_size, shuffle=True) for x in dataset_name}\n",
    "    dataset_sizes = {x: len(subset_indices[x]) for x in dataset_name}\n",
    "    #print(dataset_sizes)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('device: ', device)\n",
    "\n",
    "    # Get a batch of training data\n",
    "    bin_inputs, int_inputs, bin_labels, int_labels = next(iter(dataloaders['train']))\n",
    "    print('inputs.shape', bin_inputs.shape, int_inputs.shape)\n",
    "    print('labels.shape', bin_labels.shape, int_labels.shape)\n",
    "\n",
    "    model_ft = CCNet(dim_embedding=dim_embedding, dim_output=len(bin_labels[0]),\n",
    "                     num_items=len(bin_inputs[0]), num_outputs=2 if classify and complete else 1,\n",
    "                     num_enc_layers=4, num_dec_layers=2, ln=True, dropout=0.2,\n",
    "                     classify=classify, complete=complete,\n",
    "                     freeze_classify=freeze_classify, freeze_complete=freeze_complete).to(device)\n",
    "    print(model_ft)\n",
    "    total_params = sum(dict((p.data_ptr(), p.numel()) for p in model_ft.parameters() if p.requires_grad ).values())\n",
    "    print(\"Total Number of Parameters\", total_params)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optimizer = optim.AdamW([p for p in model_ft.parameters() if p.requires_grad == True],\n",
    "                                        lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.2)\n",
    "    exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=step_size,\n",
    "                                                      eps=1e-08, verbose=True)\n",
    "    # (metric) F1Score objects - sklearn으로 대체했음. train 함수 cell 참고\n",
    "    #macro_f1 = F1Score(num_classes=20, average='macro')\n",
    "    #micro_f1 = F1Score(num_classes=20, average='micro')\n",
    "    #metrics = {'macro_f1': macro_f1, 'micro_f1': micro_f1}\n",
    "\n",
    "    model_ft = train(model_ft, dataloaders, criterion, optimizer, exp_lr_scheduler, #metrics,\n",
    "                     dataset_sizes, device=device, num_epochs=n_epochs, early_stop_patience=20,\n",
    "                     classify=classify, complete=complete, random_seed=seed)\n",
    "    \n",
    "    fname = ['ckpt', 'CCNet', 'batch', str(batch_size),\n",
    "             'n_epochs', str(n_epochs), 'lr', str(lr), 'step_size', str(step_size),\n",
    "             'seed', str(seed), 'dim_embedding', str(dim_embedding), 'subset_length', str(subset_length)]\n",
    "    fname = '_'.join(fname) + '.pt'\n",
    "    if not os.path.isdir('./weights/'):\n",
    "        os.mkdir('./weights/')\n",
    "    torch.save(model_ft.state_dict(), os.path.join('./weights/', fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb37603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 23547, 'valid_class': 7848, 'valid_compl': 7848}\n",
      "device:  cpu\n",
      "inputs.shape torch.Size([16, 6714]) torch.Size([16, 59])\n",
      "labels.shape torch.Size([16, 20]) torch.Size([16])\n",
      "CCNet(\n",
      "  (embedding): Embedding(6715, 300, padding_idx=6714)\n",
      "  (encoder): ModuleList(\n",
      "    (0): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=300, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=300, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=300, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooling): PMA(\n",
      "    (mab): MAB(\n",
      "      (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_o): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder1): Sequential(\n",
      "    (0): SAB(\n",
      "      (mab): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): SAB(\n",
      "      (mab): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ff1): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=20, bias=True)\n",
      "  )\n",
      ")\n",
      "Total Number of Parameters 3029944\n",
      "-----Training the model-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e155f0ca8b84436b924ab1b73d68fda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "labels_classification [10 13 16 13 18 18  5  7  9 16  9  9  4 16 13  3]\n",
      "preds_classification [19 19 19 19 19 19 19  9 19 16 15  8  9 19 19 19]\n",
      "    train   0% of an epoch | Loss(classif.): 3.0359 | \n",
      "    train   6% of an epoch | Loss(classif.): 2.4225 | \n",
      "    train  13% of an epoch | Loss(classif.): 2.0828 | \n",
      "    train  20% of an epoch | Loss(classif.): 2.0018 | \n",
      "    train  27% of an epoch | Loss(classif.): 2.0229 | \n",
      "    train  33% of an epoch | Loss(classif.): 2.0012 | \n",
      "    train  40% of an epoch | Loss(classif.): 1.9579 | \n",
      "    train  47% of an epoch | Loss(classif.): 1.7954 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.6529 | \n",
      "    train  61% of an epoch | Loss(classif.): 1.8673 | \n",
      "    train  67% of an epoch | Loss(classif.): 1.8212 | \n",
      "    train  74% of an epoch | Loss(classif.): 1.4853 | \n",
      "    train  81% of an epoch | Loss(classif.): 2.2095 | \n",
      "    train  88% of an epoch | Loss(classif.): 1.1956 | \n",
      "    train  95% of an epoch | Loss(classif.): 1.3653 | \n",
      "TRAIN | Loss(classif.): 1.865 Acc(classif.): 0.462 Macro-F1: 0.169 Micro-F1: 0.462 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.8494 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 1.7875 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.7879 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 2.4938 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.2633 | \n",
      "VALID_CLASS | Loss(classif.): 1.453 Acc(classif.): 0.578 Macro-F1: 0.261 Micro-F1: 0.578 | \n",
      "\n",
      "Epoch 2/50\n",
      "labels_classification [16  9 16  5  7 18  2  5 13 16  3 12 13  9 19 18]\n",
      "preds_classification [16  9 16  9  7 18 16 16 13 16  3  3 13  9  7 18]\n",
      "    train   0% of an epoch | Loss(classif.): 1.1018 | \n",
      "    train   6% of an epoch | Loss(classif.): 1.0920 | \n",
      "    train  13% of an epoch | Loss(classif.): 1.4219 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.9866 | \n",
      "    train  27% of an epoch | Loss(classif.): 2.0852 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.7049 | \n",
      "    train  40% of an epoch | Loss(classif.): 2.0337 | \n",
      "    train  47% of an epoch | Loss(classif.): 1.4479 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.3796 | \n",
      "    train  61% of an epoch | Loss(classif.): 2.3593 | \n",
      "    train  67% of an epoch | Loss(classif.): 1.2807 | \n",
      "    train  74% of an epoch | Loss(classif.): 1.1278 | \n",
      "    train  81% of an epoch | Loss(classif.): 1.1283 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.8059 | \n",
      "    train  95% of an epoch | Loss(classif.): 1.4854 | \n",
      "TRAIN | Loss(classif.): 1.350 Acc(classif.): 0.602 Macro-F1: 0.323 Micro-F1: 0.602 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.6791 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 1.4800 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.2637 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 0.9737 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.6404 | \n",
      "VALID_CLASS | Loss(classif.): 1.280 Acc(classif.): 0.623 Macro-F1: 0.344 Micro-F1: 0.623 | \n",
      "\n",
      "Epoch 3/50\n",
      "labels_classification [ 6 19 13 14  9 16  3  7  9 18  7  9  9  7 10  8]\n",
      "preds_classification [ 9 18 13 14  9 16  3  7  9 18  7  9  9  7 16 16]\n",
      "    train   0% of an epoch | Loss(classif.): 0.8835 | \n",
      "    train   6% of an epoch | Loss(classif.): 1.7256 | \n",
      "    train  13% of an epoch | Loss(classif.): 1.9962 | \n",
      "    train  20% of an epoch | Loss(classif.): 1.2168 | \n",
      "    train  27% of an epoch | Loss(classif.): 1.1913 | \n",
      "    train  33% of an epoch | Loss(classif.): 1.3444 | \n",
      "    train  40% of an epoch | Loss(classif.): 1.5177 | \n",
      "    train  47% of an epoch | Loss(classif.): 1.1395 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.1641 | \n",
      "    train  61% of an epoch | Loss(classif.): 1.3238 | \n",
      "    train  67% of an epoch | Loss(classif.): 1.7947 | \n",
      "    train  74% of an epoch | Loss(classif.): 1.1269 | \n",
      "    train  81% of an epoch | Loss(classif.): 1.5886 | \n",
      "    train  88% of an epoch | Loss(classif.): 1.5087 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.9458 | \n",
      "TRAIN | Loss(classif.): 1.169 Acc(classif.): 0.654 Macro-F1: 0.399 Micro-F1: 0.654 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.8053 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 2.1159 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.9664 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.3817 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.6003 | \n",
      "VALID_CLASS | Loss(classif.): 1.225 Acc(classif.): 0.646 Macro-F1: 0.412 Micro-F1: 0.646 | \n",
      "\n",
      "Epoch 4/50\n",
      "labels_classification [17  9  4 13  0 11 13  1 18  1 19 13  3  3  7  9]\n",
      "preds_classification [ 5  9 18 13  2 11 16 16 18  5 18 13 18  3 14  9]\n",
      "    train   0% of an epoch | Loss(classif.): 1.6660 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.9348 | \n",
      "    train  13% of an epoch | Loss(classif.): 1.1148 | \n",
      "    train  20% of an epoch | Loss(classif.): 1.6283 | \n",
      "    train  27% of an epoch | Loss(classif.): 1.0959 | \n",
      "    train  33% of an epoch | Loss(classif.): 1.1964 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.7701 | \n",
      "    train  47% of an epoch | Loss(classif.): 1.0666 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.8390 | \n",
      "    train  61% of an epoch | Loss(classif.): 1.0917 | \n",
      "    train  67% of an epoch | Loss(classif.): 1.9571 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.5711 | \n",
      "    train  81% of an epoch | Loss(classif.): 1.9140 | \n",
      "    train  88% of an epoch | Loss(classif.): 1.1298 | \n",
      "    train  95% of an epoch | Loss(classif.): 1.2314 | \n",
      "TRAIN | Loss(classif.): 1.070 Acc(classif.): 0.683 Macro-F1: 0.459 Micro-F1: 0.683 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.7304 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 1.2496 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.1574 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 0.8259 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.7898 | \n",
      "VALID_CLASS | Loss(classif.): 1.255 Acc(classif.): 0.628 Macro-F1: 0.400 Micro-F1: 0.628 | \n",
      "\n",
      "Epoch 5/50\n",
      "labels_classification [ 5  6 19  9  7  2 11  8  5 13  6 16 16  6 16  8]\n",
      "preds_classification [ 5  6 18  9  7 16 11  5  5 13  5 16  5 13 16  5]\n",
      "    train   0% of an epoch | Loss(classif.): 1.5624 | \n",
      "    train   6% of an epoch | Loss(classif.): 1.0343 | \n",
      "    train  13% of an epoch | Loss(classif.): 1.4578 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.8218 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.8679 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.7400 | \n",
      "    train  40% of an epoch | Loss(classif.): 1.5542 | \n",
      "    train  47% of an epoch | Loss(classif.): 1.3844 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.9441 | \n",
      "    train  61% of an epoch | Loss(classif.): 1.1099 | \n",
      "    train  67% of an epoch | Loss(classif.): 1.4150 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.7759 | \n",
      "    train  81% of an epoch | Loss(classif.): 1.2086 | \n",
      "    train  88% of an epoch | Loss(classif.): 1.5684 | \n",
      "    train  95% of an epoch | Loss(classif.): 1.1974 | \n",
      "TRAIN | Loss(classif.): 1.006 Acc(classif.): 0.705 Macro-F1: 0.498 Micro-F1: 0.705 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.9189 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 0.8969 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.4527 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.7014 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.8353 | \n",
      "VALID_CLASS | Loss(classif.): 1.174 Acc(classif.): 0.666 Macro-F1: 0.445 Micro-F1: 0.666 | \n",
      "\n",
      "Epoch 6/50\n",
      "labels_classification [16  9  9 16  9 13  8  9 13 13 19  7  9  9 13 16]\n",
      "preds_classification [16  9  9  2  9  9  1  9 13  2  3  5  9  9 13  5]\n",
      "    train   0% of an epoch | Loss(classif.): 1.2341 | \n",
      "    train   6% of an epoch | Loss(classif.): 1.2189 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.9796 | \n",
      "    train  20% of an epoch | Loss(classif.): 1.1450 | \n",
      "    train  27% of an epoch | Loss(classif.): 1.2104 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.9016 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.8197 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.4076 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.2163 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.4761 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.7486 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.4362 | \n",
      "    train  81% of an epoch | Loss(classif.): 1.3815 | \n",
      "    train  88% of an epoch | Loss(classif.): 1.4312 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.4865 | \n",
      "TRAIN | Loss(classif.): 0.961 Acc(classif.): 0.721 Macro-F1: 0.524 Micro-F1: 0.721 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.4758 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 0.9119 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 0.5220 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.2264 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.6888 | \n",
      "VALID_CLASS | Loss(classif.): 1.175 Acc(classif.): 0.672 Macro-F1: 0.460 Micro-F1: 0.672 | \n",
      "\n",
      "Epoch 7/50\n",
      "labels_classification [11  5  5 16 16 16 12  9  3 18  9  7 13  6  9  6]\n",
      "preds_classification [11  6  5 16 16 16 12  9  3 18  9 14 13  6  9  6]\n",
      "    train   0% of an epoch | Loss(classif.): 0.6487 | \n",
      "    train   6% of an epoch | Loss(classif.): 1.1838 | \n",
      "    train  13% of an epoch | Loss(classif.): 1.2485 | \n",
      "    train  20% of an epoch | Loss(classif.): 1.1083 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.7973 | \n",
      "    train  33% of an epoch | Loss(classif.): 1.5907 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.5762 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.9167 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.6955 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.5803 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.9061 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.2444 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.7555 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.7589 | \n",
      "    train  95% of an epoch | Loss(classif.): 1.3346 | \n",
      "TRAIN | Loss(classif.): 0.930 Acc(classif.): 0.729 Macro-F1: 0.538 Micro-F1: 0.729 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.2802 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 0.6468 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.6587 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.2129 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.1481 | \n",
      "VALID_CLASS | Loss(classif.): 1.203 Acc(classif.): 0.672 Macro-F1: 0.455 Micro-F1: 0.672 | \n",
      "\n",
      "Epoch 8/50\n",
      "labels_classification [ 7 19 16 17 16  8  9  7 13 13  2  3 16  9  2 13]\n",
      "preds_classification [ 7 18  1 17  2  5  9  7 13 13 13  3 16  5  2 13]\n",
      "    train   0% of an epoch | Loss(classif.): 0.9464 | \n",
      "    train   6% of an epoch | Loss(classif.): 1.2534 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.6394 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.8148 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.3278 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.8483 | \n",
      "    train  40% of an epoch | Loss(classif.): 1.3063 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.7989 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.0257 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.7736 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.8851 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.9990 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.5934 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.9044 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.4177 | \n",
      "TRAIN | Loss(classif.): 0.908 Acc(classif.): 0.737 Macro-F1: 0.552 Micro-F1: 0.737 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.3780 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 0.9715 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 0.9429 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.7175 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.9772 | \n",
      "VALID_CLASS | Loss(classif.): 1.214 Acc(classif.): 0.675 Macro-F1: 0.477 Micro-F1: 0.675 | \n",
      "\n",
      "Epoch 9/50\n",
      "labels_classification [ 1 13  5  9 13  9 13  5 16  0 16 13 13 12 13  9]\n",
      "preds_classification [ 1 13  5  9 13 16 13  5 16 16 16 13 13  3 13  9]\n",
      "    train   0% of an epoch | Loss(classif.): 0.8704 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.5230 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.7143 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.6350 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.6357 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.6251 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.6050 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.8305 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.4646 | \n",
      "    train  61% of an epoch | Loss(classif.): 1.0768 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.6655 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.7326 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.5978 | \n",
      "    train  88% of an epoch | Loss(classif.): 1.8730 | \n",
      "    train  95% of an epoch | Loss(classif.): 1.3430 | \n",
      "TRAIN | Loss(classif.): 0.906 Acc(classif.): 0.738 Macro-F1: 0.555 Micro-F1: 0.738 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.1936 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 0.4109 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.6250 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.4677 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.1233 | \n",
      "VALID_CLASS | Loss(classif.): 1.204 Acc(classif.): 0.668 Macro-F1: 0.473 Micro-F1: 0.668 | \n",
      "\n",
      "Epoch 10/50\n",
      "labels_classification [13  3 17  8 16  7  9  7  9 16 16  5  1  3  6  0]\n",
      "preds_classification [13  3  9  1 16  7  9  7  9 16 16  5  1  3 14 10]\n",
      "    train   0% of an epoch | Loss(classif.): 0.6212 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.6997 | \n",
      "    train  13% of an epoch | Loss(classif.): 1.1930 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.8634 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.7049 | \n",
      "    train  33% of an epoch | Loss(classif.): 1.0821 | \n",
      "    train  40% of an epoch | Loss(classif.): 1.1901 | \n",
      "    train  47% of an epoch | Loss(classif.): 1.1095 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.9612 | \n",
      "    train  61% of an epoch | Loss(classif.): 1.0452 | \n",
      "    train  67% of an epoch | Loss(classif.): 1.1592 | \n",
      "    train  74% of an epoch | Loss(classif.): 1.1770 | \n",
      "    train  81% of an epoch | Loss(classif.): 1.0487 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.8216 | \n",
      "    train  95% of an epoch | Loss(classif.): 1.0225 | \n",
      "TRAIN | Loss(classif.): 0.897 Acc(classif.): 0.740 Macro-F1: 0.558 Micro-F1: 0.740 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.7668 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 1.0068 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.0475 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 0.8217 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.8427 | \n",
      "VALID_CLASS | Loss(classif.): 1.254 Acc(classif.): 0.646 Macro-F1: 0.427 Micro-F1: 0.646 | \n",
      "\n",
      "Epoch 11/50\n",
      "labels_classification [ 9 13  8 13 16 13 18 14  7  9  5 19  6 16 13 13]\n",
      "preds_classification [ 9 13  1 13 16 13  3  7  7  9  5 18  9 16 13 13]\n",
      "    train   0% of an epoch | Loss(classif.): 0.9852 | \n",
      "    train   6% of an epoch | Loss(classif.): 1.1663 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.8524 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.7578 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.7492 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.6337 | \n",
      "    train  40% of an epoch | Loss(classif.): 1.3110 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.2588 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.0156 | \n",
      "    train  61% of an epoch | Loss(classif.): 1.2715 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.8564 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.9219 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.5988 | \n",
      "    train  88% of an epoch | Loss(classif.): 1.2772 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.7563 | \n",
      "TRAIN | Loss(classif.): 0.907 Acc(classif.): 0.737 Macro-F1: 0.553 Micro-F1: 0.737 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.8875 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 1.4748 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.3806 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 0.5640 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.6499 | \n",
      "VALID_CLASS | Loss(classif.): 1.185 Acc(classif.): 0.667 Macro-F1: 0.489 Micro-F1: 0.667 | \n",
      "\n",
      "Epoch 12/50\n",
      "labels_classification [ 5 16  9  7  9 13  7  1  4  9 14  2  5 14  3 13]\n",
      "preds_classification [ 9 16  9  7  9 13  7  8 19  9 14  2  5 13  3 13]\n",
      "    train   0% of an epoch | Loss(classif.): 0.7106 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.8324 | \n",
      "    train  13% of an epoch | Loss(classif.): 1.0081 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.7446 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.6850 | \n",
      "    train  33% of an epoch | Loss(classif.): 1.6020 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.8240 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.6872 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.1381 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.3504 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.9077 | \n",
      "    train  74% of an epoch | Loss(classif.): 1.1762 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.9466 | \n",
      "    train  88% of an epoch | Loss(classif.): 1.0221 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.9304 | \n",
      "TRAIN | Loss(classif.): 0.907 Acc(classif.): 0.738 Macro-F1: 0.555 Micro-F1: 0.738 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.7679 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 1.1114 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.6981 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 2.1327 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.5671 | \n",
      "VALID_CLASS | Loss(classif.): 1.246 Acc(classif.): 0.660 Macro-F1: 0.449 Micro-F1: 0.660 | \n",
      "\n",
      "Epoch 13/50\n",
      "labels_classification [16  3  2  7  3 16  2 13 13  3  3 13  2  8  1  9]\n",
      "preds_classification [16  3  2  7  3 16 13 13 13  3  3 13  2  8 15  9]\n",
      "    train   0% of an epoch | Loss(classif.): 0.6246 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.4273 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.9250 | \n",
      "    train  20% of an epoch | Loss(classif.): 1.1653 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.7650 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.6694 | \n",
      "    train  40% of an epoch | Loss(classif.): 1.2740 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.7297 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.1699 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.9307 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.7409 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.6933 | \n",
      "    train  81% of an epoch | Loss(classif.): 1.1096 | \n",
      "    train  88% of an epoch | Loss(classif.): 1.4336 | \n",
      "    train  95% of an epoch | Loss(classif.): 1.2463 | \n",
      "TRAIN | Loss(classif.): 0.910 Acc(classif.): 0.739 Macro-F1: 0.560 Micro-F1: 0.739 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.4693 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 1.0173 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 0.8956 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.8991 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.9697 | \n",
      "VALID_CLASS | Loss(classif.): 1.219 Acc(classif.): 0.673 Macro-F1: 0.462 Micro-F1: 0.673 | \n",
      "\n",
      "Epoch 14/50\n",
      "labels_classification [ 9 11  7 13  6  9  9 18 16  9  2  5 11  7  9  5]\n",
      "preds_classification [ 9 11  7 13  5  9  9  7 16  9  2  5 11  7  9  5]\n",
      "    train   0% of an epoch | Loss(classif.): 0.6284 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.5580 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.9324 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.4729 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.9223 | \n",
      "    train  33% of an epoch | Loss(classif.): 2.0572 | \n",
      "    train  40% of an epoch | Loss(classif.): 1.0405 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.4695 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.8647 | \n",
      "    train  61% of an epoch | Loss(classif.): 1.7409 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.2985 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.5929 | \n",
      "    train  81% of an epoch | Loss(classif.): 1.5191 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.6320 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.9977 | \n",
      "TRAIN | Loss(classif.): 0.914 Acc(classif.): 0.741 Macro-F1: 0.561 Micro-F1: 0.741 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.0354 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 0.8772 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.3696 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 0.6309 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.2596 | \n",
      "VALID_CLASS | Loss(classif.): 1.190 Acc(classif.): 0.675 Macro-F1: 0.484 Micro-F1: 0.675 | \n",
      "\n",
      "Epoch 15/50\n",
      "labels_classification [ 4 16  7  9 16  5 18  3  1 13  7  1 13 16 16 11]\n",
      "preds_classification [ 4 16 18  9 16  5 18  3  1 13  7  1 16 16  9 12]\n",
      "    train   0% of an epoch | Loss(classif.): 0.9481 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.4604 | \n",
      "    train  13% of an epoch | Loss(classif.): 1.3643 | \n",
      "    train  20% of an epoch | Loss(classif.): 1.5652 | \n",
      "    train  27% of an epoch | Loss(classif.): 1.5272 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.5452 | \n",
      "    train  40% of an epoch | Loss(classif.): 1.7421 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.6231 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.1112 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.5306 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.9506 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.7928 | \n",
      "    train  81% of an epoch | Loss(classif.): 1.5046 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.9479 | \n",
      "    train  95% of an epoch | Loss(classif.): 1.1237 | \n",
      "TRAIN | Loss(classif.): 0.914 Acc(classif.): 0.741 Macro-F1: 0.564 Micro-F1: 0.741 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.9977 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 0.7468 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.7156 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 0.9321 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.6576 | \n",
      "VALID_CLASS | Loss(classif.): 1.205 Acc(classif.): 0.675 Macro-F1: 0.500 Micro-F1: 0.675 | \n",
      "\n",
      "Epoch 16/50\n",
      "labels_classification [10  9  9 14  9 13  0 19  7  2  9  5 16  9 11  9]\n",
      "preds_classification [16  9  9 14  9  8  0  4  7  2  9  5 16  9 11  9]\n",
      "    train   0% of an epoch | Loss(classif.): 0.6935 | \n",
      "    train   6% of an epoch | Loss(classif.): 1.1782 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.9964 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.7129 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.4250 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.8511 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.7384 | \n",
      "    train  47% of an epoch | Loss(classif.): 1.3453 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.8954 | \n",
      "    train  61% of an epoch | Loss(classif.): 1.1923 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.5986 | \n",
      "    train  74% of an epoch | Loss(classif.): 1.5492 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.6144 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.5362 | \n",
      "    train  95% of an epoch | Loss(classif.): 1.0226 | \n",
      "TRAIN | Loss(classif.): 0.903 Acc(classif.): 0.740 Macro-F1: 0.568 Micro-F1: 0.740 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.3875 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 2.2639 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 0.9425 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 0.9733 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.5999 | \n",
      "VALID_CLASS | Loss(classif.): 1.230 Acc(classif.): 0.649 Macro-F1: 0.434 Micro-F1: 0.649 | \n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\n",
      "Epoch 17/50\n",
      "labels_classification [18  9 16 16  9  9 10 16 13 16 14 18 16 16 14 15]\n",
      "preds_classification [18  9 16 16  9  9 16  2 13 16 14 18 16 16 14 16]\n",
      "    train   0% of an epoch | Loss(classif.): 0.8661 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.9121 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.6578 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.8360 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.5635 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.7622 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.3432 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.6776 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.7075 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.8813 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.7808 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.2190 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.2322 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.7907 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.3097 | \n",
      "TRAIN | Loss(classif.): 0.681 Acc(classif.): 0.813 Macro-F1: 0.674 Micro-F1: 0.813 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.5977 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 1.3828 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.3884 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.2188 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.3221 | \n",
      "VALID_CLASS | Loss(classif.): 1.139 Acc(classif.): 0.701 Macro-F1: 0.559 Micro-F1: 0.701 | \n",
      "\n",
      "Epoch 18/50\n",
      "labels_classification [ 9  3  9  2 13  7 17 18 13  8 16 17 17  7 18 13]\n",
      "preds_classification [ 9  3  9  2 13  7  5 18 13  1 16 17 17  7 18 13]\n",
      "    train   0% of an epoch | Loss(classif.): 0.5406 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.4157 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.7866 | \n",
      "    train  20% of an epoch | Loss(classif.): 1.1070 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.6354 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.3855 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.2968 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.1575 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.6950 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.1050 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.9378 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.5000 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.5913 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.3671 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.4573 | \n",
      "TRAIN | Loss(classif.): 0.592 Acc(classif.): 0.839 Macro-F1: 0.722 Micro-F1: 0.839 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.5822 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 1.3192 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 0.9665 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 2.0698 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.9987 | \n",
      "VALID_CLASS | Loss(classif.): 1.140 Acc(classif.): 0.709 Macro-F1: 0.578 Micro-F1: 0.709 | \n",
      "\n",
      "Epoch 19/50\n",
      "labels_classification [ 7  1 18 16 16  2 14 13 13 13 15  9  9  9  7  9]\n",
      "preds_classification [ 7  1 18 16 13  2 14 13 13 13 15  9  9  9  7  9]\n",
      "    train   0% of an epoch | Loss(classif.): 0.3509 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.6079 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.4491 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.3879 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.4820 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.4145 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.8980 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.2214 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.1971 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.6189 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.5285 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.7801 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.9745 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.0761 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.3676 | \n",
      "TRAIN | Loss(classif.): 0.542 Acc(classif.): 0.857 Macro-F1: 0.757 Micro-F1: 0.857 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.8295 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 1.8500 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 0.8289 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 2.1184 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.4291 | \n",
      "VALID_CLASS | Loss(classif.): 1.168 Acc(classif.): 0.710 Macro-F1: 0.575 Micro-F1: 0.710 | \n",
      "\n",
      "Epoch 20/50\n",
      "labels_classification [13 15 16  9  6 16 13  3  7 13  7 16  4 13  9  1]\n",
      "preds_classification [13 15 16  9  6 16 13  3  7 13  7 16  3 13  9 16]\n",
      "    train   0% of an epoch | Loss(classif.): 0.4129 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.2066 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.1177 | \n",
      "    train  20% of an epoch | Loss(classif.): 1.2867 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.8201 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.2539 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.3611 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.9283 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.1470 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.8597 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.5137 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.5626 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.2249 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.2286 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.2998 | \n",
      "TRAIN | Loss(classif.): 0.505 Acc(classif.): 0.866 Macro-F1: 0.770 Micro-F1: 0.866 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.6637 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 0.9978 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.0641 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.1542 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 2.2447 | \n",
      "VALID_CLASS | Loss(classif.): 1.201 Acc(classif.): 0.710 Macro-F1: 0.580 Micro-F1: 0.710 | \n",
      "\n",
      "Epoch 21/50\n",
      "labels_classification [13 13 16 16  5 13 12 16 13 13  6  3 13  9  9 16]\n",
      "preds_classification [13 13 16 16 17 13 12 16 13 13  6  3 13  9  9 16]\n",
      "    train   0% of an epoch | Loss(classif.): 0.1417 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.5380 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.5424 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.6812 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.7008 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.1507 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.6032 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.1700 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.2920 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.5874 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.6098 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.4039 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.4152 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.4623 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.8719 | \n",
      "TRAIN | Loss(classif.): 0.473 Acc(classif.): 0.877 Macro-F1: 0.794 Micro-F1: 0.877 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.2400 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 0.4791 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 0.5839 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.2542 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.4418 | \n",
      "VALID_CLASS | Loss(classif.): 1.232 Acc(classif.): 0.708 Macro-F1: 0.580 Micro-F1: 0.708 | \n",
      "\n",
      "Epoch 22/50\n",
      "labels_classification [ 7 18  5 16  9 16 13  7 16  9 16 14  3 16  9  3]\n",
      "preds_classification [ 7 18 16 16  9 16 13  7 16  9 16 14  3 16  9  3]\n",
      "    train   0% of an epoch | Loss(classif.): 0.2423 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.5191 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.0911 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.4227 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.2247 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.2356 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.6931 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.5031 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.0744 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.1986 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.3183 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.2975 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.3939 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.1238 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.2104 | \n",
      "TRAIN | Loss(classif.): 0.446 Acc(classif.): 0.886 Macro-F1: 0.810 Micro-F1: 0.886 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.8186 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 1.4474 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 2.3667 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.1068 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.2428 | \n",
      "VALID_CLASS | Loss(classif.): 1.277 Acc(classif.): 0.698 Macro-F1: 0.564 Micro-F1: 0.698 | \n",
      "\n",
      "Epoch 23/50\n",
      "labels_classification [ 5  9 12  5 13 17 19 18  9 13 13 12  6 13 13  9]\n",
      "preds_classification [ 5 13 12  5 13 17 18 18  9 13 13 12  6 13 13  9]\n",
      "    train   0% of an epoch | Loss(classif.): 0.6410 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.5567 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.4329 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.5501 | \n",
      "    train  27% of an epoch | Loss(classif.): 1.3120 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.1249 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.2897 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.2979 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.2369 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.2368 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.2450 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.4650 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.9736 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.3469 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.2382 | \n",
      "TRAIN | Loss(classif.): 0.425 Acc(classif.): 0.892 Macro-F1: 0.819 Micro-F1: 0.892 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.2344 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 3.4940 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.6155 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.2662 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.4944 | \n",
      "VALID_CLASS | Loss(classif.): 1.298 Acc(classif.): 0.709 Macro-F1: 0.583 Micro-F1: 0.709 | \n",
      "\n",
      "Epoch 24/50\n",
      "labels_classification [13  3  9 11  2  9  9  4  3  3 16  2  7 13  9 16]\n",
      "preds_classification [13  3  9 11  2  9  9  4  3  3 16  2  7 13  9 16]\n",
      "    train   0% of an epoch | Loss(classif.): 0.0400 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.0668 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.0945 | \n",
      "    train  20% of an epoch | Loss(classif.): 1.1152 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.4745 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.6183 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.0567 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.1770 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.1616 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.8977 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.3731 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.3079 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.1826 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.1788 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.1821 | \n",
      "TRAIN | Loss(classif.): 0.405 Acc(classif.): 0.900 Macro-F1: 0.834 Micro-F1: 0.900 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 0.6390 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 0.6035 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.3129 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 0.9183 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.1578 | \n",
      "VALID_CLASS | Loss(classif.): 1.352 Acc(classif.): 0.705 Macro-F1: 0.580 Micro-F1: 0.705 | \n",
      "\n",
      "Epoch 25/50\n",
      "labels_classification [12  9  5 16  6  5 11  9  3  3  2 13  9 18  7  2]\n",
      "preds_classification [12  9  5 16  6  5 11  9  3  3  2 13  9 18  7  2]\n",
      "    train   0% of an epoch | Loss(classif.): 0.0954 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.6819 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.8874 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.6219 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.3136 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.2581 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.5179 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.3906 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.5141 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.2685 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.1502 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.2993 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.1440 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.0586 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.4389 | \n",
      "TRAIN | Loss(classif.): 0.387 Acc(classif.): 0.904 Macro-F1: 0.842 Micro-F1: 0.904 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 2.2953 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 0.2982 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.3006 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 2.1967 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 1.2210 | \n",
      "VALID_CLASS | Loss(classif.): 1.425 Acc(classif.): 0.700 Macro-F1: 0.572 Micro-F1: 0.700 | \n",
      "\n",
      "Epoch 26/50\n",
      "labels_classification [ 4 13  9 12  7  9  9  9  7  9 13 13  2  9  9 11]\n",
      "preds_classification [ 4 13  9 12  7  9  9  9  7  9 13 13  2  9  9 11]\n",
      "    train   0% of an epoch | Loss(classif.): 0.0575 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.3696 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.3564 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.7981 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.4317 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.2632 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.4784 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.4919 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.8050 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.1009 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.0811 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.4748 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.8324 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.1240 | \n",
      "    train  95% of an epoch | Loss(classif.): 0.3666 | \n",
      "TRAIN | Loss(classif.): 0.375 Acc(classif.): 0.907 Macro-F1: 0.847 Micro-F1: 0.907 | \n",
      "    valid_class   0% of an epoch | Loss(classif.): 1.4243 | \n",
      "    valid_class  20% of an epoch | Loss(classif.): 2.3578 | \n",
      "    valid_class  40% of an epoch | Loss(classif.): 1.1949 | \n",
      "    valid_class  61% of an epoch | Loss(classif.): 1.0624 | \n",
      "    valid_class  81% of an epoch | Loss(classif.): 0.3626 | \n",
      "VALID_CLASS | Loss(classif.): 1.400 Acc(classif.): 0.693 Macro-F1: 0.567 Micro-F1: 0.693 | \n",
      "\n",
      "Epoch 27/50\n",
      "labels_classification [16  6  3  3 13  3 14 16  4  2  9  9 13 17  7  1]\n",
      "preds_classification [16  6  3  3 13  3 14 16  3  2  9  9 13 17  7  1]\n",
      "    train   0% of an epoch | Loss(classif.): 0.3862 | \n",
      "    train   6% of an epoch | Loss(classif.): 0.7608 | \n",
      "    train  13% of an epoch | Loss(classif.): 0.1636 | \n",
      "    train  20% of an epoch | Loss(classif.): 0.6738 | \n",
      "    train  27% of an epoch | Loss(classif.): 0.1893 | \n",
      "    train  33% of an epoch | Loss(classif.): 0.3731 | \n",
      "    train  40% of an epoch | Loss(classif.): 0.9381 | \n",
      "    train  47% of an epoch | Loss(classif.): 0.3266 | \n",
      "    train  54% of an epoch | Loss(classif.): 0.1864 | \n",
      "    train  61% of an epoch | Loss(classif.): 0.9543 | \n",
      "    train  67% of an epoch | Loss(classif.): 0.5418 | \n",
      "    train  74% of an epoch | Loss(classif.): 0.2719 | \n",
      "    train  81% of an epoch | Loss(classif.): 0.5533 | \n",
      "    train  88% of an epoch | Loss(classif.): 0.2411 | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training CCNet.ipynb Cell 17'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000017?line=0'>1</a>\u001b[0m \u001b[39m# Experiment 1. classification only\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000017?line=1'>2</a>\u001b[0m experiment(batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m, dim_embedding\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, complete\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training CCNet.ipynb Cell 16'\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(dim_embedding, dropout, subset_length, exclude_idx, batch_size, n_epochs, lr, step_size, seed, classify, complete, freeze_classify, freeze_complete)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=44'>45</a>\u001b[0m exp_lr_scheduler \u001b[39m=\u001b[39m lr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, patience\u001b[39m=\u001b[39mstep_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=45'>46</a>\u001b[0m                                                   eps\u001b[39m=\u001b[39m\u001b[39m1e-08\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=46'>47</a>\u001b[0m \u001b[39m# (metric) F1Score objects - sklearn으로 대체했음. train 함수 cell 참고\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=47'>48</a>\u001b[0m \u001b[39m#macro_f1 = F1Score(num_classes=20, average='macro')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=48'>49</a>\u001b[0m \u001b[39m#micro_f1 = F1Score(num_classes=20, average='micro')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=49'>50</a>\u001b[0m \u001b[39m#metrics = {'macro_f1': macro_f1, 'micro_f1': micro_f1}\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=51'>52</a>\u001b[0m model_ft \u001b[39m=\u001b[39m train(model_ft, dataloaders, criterion, optimizer, exp_lr_scheduler, \u001b[39m#metrics,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=52'>53</a>\u001b[0m                  dataset_sizes, device\u001b[39m=\u001b[39;49mdevice, num_epochs\u001b[39m=\u001b[39;49mn_epochs, early_stop_patience\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=53'>54</a>\u001b[0m                  classify\u001b[39m=\u001b[39;49mclassify, complete\u001b[39m=\u001b[39;49mcomplete, random_seed\u001b[39m=\u001b[39;49mseed)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=55'>56</a>\u001b[0m fname \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mckpt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCCNet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(batch_size),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=56'>57</a>\u001b[0m          \u001b[39m'\u001b[39m\u001b[39mn_epochs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(n_epochs), \u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(lr), \u001b[39m'\u001b[39m\u001b[39mstep_size\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(step_size),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=57'>58</a>\u001b[0m          \u001b[39m'\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(seed), \u001b[39m'\u001b[39m\u001b[39mdim_embedding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(dim_embedding), \u001b[39m'\u001b[39m\u001b[39msubset_length\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(subset_length)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=58'>59</a>\u001b[0m fname \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(fname) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32m/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training CCNet.ipynb Cell 14'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloaders, criterion, optimizer, scheduler, dataset_sizes, device, num_epochs, early_stop_patience, classify, complete, random_seed)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000013?line=119'>120</a>\u001b[0m \u001b[39m# backward + optimize only if in training phase\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000013?line=120'>121</a>\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000013?line=121'>122</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000013?line=122'>123</a>\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m0.5\u001b[39m) \u001b[39m# gradient clipping\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000013?line=123'>124</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experiment 1. classification only\n",
    "experiment(batch_size=16, n_epochs=50, lr=1e-3, dim_embedding=300, complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba61805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 23528, 'valid_class': 7839, 'valid_compl': 7839}\n",
      "device:  cpu\n",
      "inputs.shape torch.Size([16, 6714]) torch.Size([16, 59])\n",
      "labels.shape torch.Size([16, 20]) torch.Size([16])\n",
      "CCNet(\n",
      "  (embedding): Embedding(6715, 300, padding_idx=6714)\n",
      "  (encoder): ModuleList(\n",
      "    (0): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=300, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=300, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=300, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooling): PMA(\n",
      "    (mab): MAB(\n",
      "      (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_o): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder1): Sequential(\n",
      "    (0): SAB(\n",
      "      (mab): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): SAB(\n",
      "      (mab): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder2): ModuleList(\n",
      "    (0): MAB(\n",
      "      (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc_k): Linear(in_features=300, out_features=128, bias=True)\n",
      "      (fc_v): Linear(in_features=300, out_features=128, bias=True)\n",
      "      (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_o): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): MAB(\n",
      "      (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc_k): Linear(in_features=300, out_features=128, bias=True)\n",
      "      (fc_v): Linear(in_features=300, out_features=128, bias=True)\n",
      "      (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_o): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ff2): Linear(in_features=128, out_features=6714, bias=True)\n",
      ")\n",
      "Total Number of Parameters 4131166\n",
      "-----Training the model-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc3226c6fb04f3ca124a98d1797efef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "labels_completion [3644 5544   47  579  937 1046 3128 6502 2945 4270 2945  212 4317  527\n",
      " 4059 6384]\n",
      "preds_completion [2132 4498 4437  630 5817 1947 1232 5633   60 2069 1540 3182 2915 1252\n",
      " 4311 3279]\n",
      "    train   0% of an epoch | Loss(complet.): 9.0121 | \n",
      "    train   6% of an epoch | Loss(complet.): 6.7498 | \n",
      "    train  13% of an epoch | Loss(complet.): 7.7459 | \n",
      "    train  20% of an epoch | Loss(complet.): 8.3611 | \n",
      "    train  27% of an epoch | Loss(complet.): 7.0306 | \n",
      "    train  33% of an epoch | Loss(complet.): 7.1089 | \n",
      "    train  40% of an epoch | Loss(complet.): 7.1561 | \n",
      "    train  47% of an epoch | Loss(complet.): 6.4559 | \n",
      "    train  54% of an epoch | Loss(complet.): 6.8203 | \n",
      "    train  61% of an epoch | Loss(complet.): 6.8003 | \n",
      "    train  67% of an epoch | Loss(complet.): 6.9168 | \n",
      "    train  74% of an epoch | Loss(complet.): 5.8960 | \n",
      "    train  81% of an epoch | Loss(complet.): 6.3747 | \n",
      "    train  88% of an epoch | Loss(complet.): 6.0016 | \n",
      "    train  95% of an epoch | Loss(complet.): 6.1195 | \n",
      "TRAIN | Loss(complet.): 6.771 Acc(complet.): 0.053 | \n",
      "    valid_compl   0% of an epoch | Loss(complet.): 6.0994 | \n",
      "    valid_compl  20% of an epoch | Loss(complet.): 5.9857 | \n",
      "    valid_compl  40% of an epoch | Loss(complet.): 5.7805 | \n",
      "    valid_compl  61% of an epoch | Loss(complet.): 6.7020 | \n",
      "    valid_compl  81% of an epoch | Loss(complet.): 7.0827 | \n",
      "VALID_COMPL | Loss(complet.): 6.555 Acc(complet.): 0.052 | \n",
      "\n",
      "Epoch 2/20\n",
      "labels_completion [3630  937 4266 6653 2375 3885 1308 1593   59  281 3388 1790 2406 1554\n",
      " 2122 4335]\n",
      "preds_completion [ 937  937  937  937  937  937  937  937  937 5377  937 5536 5536  937\n",
      " 2122 5536]\n",
      "    train   0% of an epoch | Loss(complet.): 6.3844 | \n",
      "    train   6% of an epoch | Loss(complet.): 5.7283 | \n",
      "    train  13% of an epoch | Loss(complet.): 6.9900 | \n",
      "    train  20% of an epoch | Loss(complet.): 6.3075 | \n",
      "    train  27% of an epoch | Loss(complet.): 6.0243 | \n",
      "    train  33% of an epoch | Loss(complet.): 6.4069 | \n",
      "    train  40% of an epoch | Loss(complet.): 6.1354 | \n",
      "    train  47% of an epoch | Loss(complet.): 7.0449 | \n",
      "    train  54% of an epoch | Loss(complet.): 6.5558 | \n",
      "    train  61% of an epoch | Loss(complet.): 6.4321 | \n",
      "    train  67% of an epoch | Loss(complet.): 6.5977 | \n",
      "    train  74% of an epoch | Loss(complet.): 7.2133 | \n",
      "    train  81% of an epoch | Loss(complet.): 6.9615 | \n",
      "    train  88% of an epoch | Loss(complet.): 5.7786 | \n",
      "    train  95% of an epoch | Loss(complet.): 6.4608 | \n",
      "TRAIN | Loss(complet.): 6.562 Acc(complet.): 0.053 | \n",
      "    valid_compl   0% of an epoch | Loss(complet.): 8.0165 | \n",
      "    valid_compl  20% of an epoch | Loss(complet.): 6.8506 | \n",
      "    valid_compl  40% of an epoch | Loss(complet.): 6.1640 | \n",
      "    valid_compl  61% of an epoch | Loss(complet.): 6.3312 | \n",
      "    valid_compl  81% of an epoch | Loss(complet.): 5.5578 | \n",
      "VALID_COMPL | Loss(complet.): 6.524 Acc(complet.): 0.051 | \n",
      "\n",
      "Epoch 3/20\n",
      "labels_completion [6422 5884  937 1112 1842 2945 5516 3725 5377 2485 1167 5454  937 2945\n",
      " 2756 6516]\n",
      "preds_completion [2945  937  937 2945 5377 2945  937 5377  937  937  937  937  937 2945\n",
      "  937  937]\n",
      "    train   0% of an epoch | Loss(complet.): 5.9520 | \n",
      "    train   6% of an epoch | Loss(complet.): 6.0404 | \n",
      "    train  13% of an epoch | Loss(complet.): 7.4462 | \n",
      "    train  20% of an epoch | Loss(complet.): 6.2990 | \n",
      "    train  27% of an epoch | Loss(complet.): 6.4314 | \n",
      "    train  33% of an epoch | Loss(complet.): 6.5200 | \n",
      "    train  40% of an epoch | Loss(complet.): 7.0699 | \n",
      "    train  47% of an epoch | Loss(complet.): 7.1147 | \n",
      "    train  54% of an epoch | Loss(complet.): 7.3011 | \n",
      "    train  61% of an epoch | Loss(complet.): 6.9698 | \n",
      "    train  67% of an epoch | Loss(complet.): 7.0146 | \n",
      "    train  74% of an epoch | Loss(complet.): 5.8168 | \n",
      "    train  81% of an epoch | Loss(complet.): 6.8567 | \n",
      "    train  88% of an epoch | Loss(complet.): 7.4533 | \n",
      "    train  95% of an epoch | Loss(complet.): 5.5417 | \n",
      "TRAIN | Loss(complet.): 6.567 Acc(complet.): 0.053 | \n",
      "    valid_compl   0% of an epoch | Loss(complet.): 6.3282 | \n",
      "    valid_compl  20% of an epoch | Loss(complet.): 6.3022 | \n",
      "    valid_compl  40% of an epoch | Loss(complet.): 6.7629 | \n",
      "    valid_compl  61% of an epoch | Loss(complet.): 5.6787 | \n",
      "    valid_compl  81% of an epoch | Loss(complet.): 6.4304 | \n",
      "VALID_COMPL | Loss(complet.): 6.522 Acc(complet.): 0.048 | \n",
      "\n",
      "Epoch 4/20\n",
      "labels_completion [4414  916  694 4827 2790 2260 2809 4677 4544 2849  811  937 1382 2122\n",
      " 3228 2437]\n",
      "preds_completion [ 937  937  937 5377  937 1308 5377  937  937  937 1308  937  937 1308\n",
      " 5377  937]\n",
      "    train   0% of an epoch | Loss(complet.): 6.7116 | \n",
      "    train   6% of an epoch | Loss(complet.): 6.5262 | \n",
      "    train  13% of an epoch | Loss(complet.): 6.1859 | \n",
      "    train  20% of an epoch | Loss(complet.): 6.2629 | \n",
      "    train  27% of an epoch | Loss(complet.): 6.9821 | \n",
      "    train  33% of an epoch | Loss(complet.): 7.0595 | \n",
      "    train  40% of an epoch | Loss(complet.): 6.0534 | \n",
      "    train  47% of an epoch | Loss(complet.): 6.6362 | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training CCNet.ipynb Cell 18'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000018?line=0'>1</a>\u001b[0m \u001b[39m# Experiment 2. completion only\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000018?line=1'>2</a>\u001b[0m experiment(batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m, dim_embedding\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, classify\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, exclude_idx\u001b[39m=\u001b[39;49mlist_single_ingredient_recipe)\n",
      "\u001b[1;32m/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training CCNet.ipynb Cell 16'\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(dim_embedding, dropout, subset_length, exclude_idx, batch_size, n_epochs, lr, step_size, seed, classify, complete, freeze_classify, freeze_complete)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=44'>45</a>\u001b[0m exp_lr_scheduler \u001b[39m=\u001b[39m lr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, patience\u001b[39m=\u001b[39mstep_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=45'>46</a>\u001b[0m                                                   eps\u001b[39m=\u001b[39m\u001b[39m1e-08\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=46'>47</a>\u001b[0m \u001b[39m# (metric) F1Score objects - sklearn으로 대체했음. train 함수 cell 참고\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=47'>48</a>\u001b[0m \u001b[39m#macro_f1 = F1Score(num_classes=20, average='macro')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=48'>49</a>\u001b[0m \u001b[39m#micro_f1 = F1Score(num_classes=20, average='micro')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=49'>50</a>\u001b[0m \u001b[39m#metrics = {'macro_f1': macro_f1, 'micro_f1': micro_f1}\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=51'>52</a>\u001b[0m model_ft \u001b[39m=\u001b[39m train(model_ft, dataloaders, criterion, optimizer, exp_lr_scheduler, \u001b[39m#metrics,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=52'>53</a>\u001b[0m                  dataset_sizes, device\u001b[39m=\u001b[39;49mdevice, num_epochs\u001b[39m=\u001b[39;49mn_epochs, early_stop_patience\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=53'>54</a>\u001b[0m                  classify\u001b[39m=\u001b[39;49mclassify, complete\u001b[39m=\u001b[39;49mcomplete, random_seed\u001b[39m=\u001b[39;49mseed)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=55'>56</a>\u001b[0m fname \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mckpt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCCNet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(batch_size),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=56'>57</a>\u001b[0m          \u001b[39m'\u001b[39m\u001b[39mn_epochs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(n_epochs), \u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(lr), \u001b[39m'\u001b[39m\u001b[39mstep_size\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(step_size),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=57'>58</a>\u001b[0m          \u001b[39m'\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(seed), \u001b[39m'\u001b[39m\u001b[39mdim_embedding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(dim_embedding), \u001b[39m'\u001b[39m\u001b[39msubset_length\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(subset_length)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000015?line=58'>59</a>\u001b[0m fname \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(fname) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32m/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training CCNet.ipynb Cell 14'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloaders, criterion, optimizer, scheduler, dataset_sizes, device, num_epochs, early_stop_patience, classify, complete, random_seed)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000013?line=119'>120</a>\u001b[0m \u001b[39m# backward + optimize only if in training phase\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000013?line=120'>121</a>\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000013?line=121'>122</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000013?line=122'>123</a>\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m0.5\u001b[39m) \u001b[39m# gradient clipping\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hanseul_jo/Desktop/AI506-termproject/cuisine-prediction/Hanseul/Training%20CCNet.ipynb#ch0000013?line=123'>124</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experiment 2. completion only\n",
    "experiment(batch_size=16, n_epochs=20, lr=1e-3, dim_embedding=300, classify=False, exclude_idx=list_single_ingredient_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3. classification + completion \n",
    "experiment(batch_size=16, n_epochs=20, lr=1e-3, dim_embedding=300, exclude_idx=list_single_ingredient_recipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
