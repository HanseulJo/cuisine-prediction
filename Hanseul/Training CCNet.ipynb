{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c56f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5py 안 될 때\n",
    "#!brew reinstall hdf5\n",
    "#!export CPATH=\"/opt/homebrew/include/\"\n",
    "#!export HDF5_DIR=/opt/homebrew/\n",
    "#!python3 -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35449e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd drive/MyDrive/cuisine-prediction/Hanseul/\n",
    "# !pip3 install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29816293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb55c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '../'\n",
    "path_container = './Container/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bfefc",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "788f8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_container + 'id_cuisine_dict.pickle', 'rb') as f:\n",
    "    id_cuisine_dict = pickle.load(f)\n",
    "with open(path_container + 'cuisine_id_dict.pickle', 'rb') as f:\n",
    "    cuisine_id_dict = pickle.load(f)\n",
    "with open(path_container + 'id_ingredient_dict.pickle', 'rb') as f:\n",
    "    id_ingredient_dict = pickle.load(f)\n",
    "with open(path_container + 'ingredient_id_dict.pickle', 'rb') as f:\n",
    "    ingredient_id_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c92050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, data_dir, test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.test = test\n",
    "        with h5py.File(data_dir, 'r') as data_file:\n",
    "            self.bin_data = data_file['bin_data'][:]  # Size (num_recipes=23547, num_ingredients=6714)\n",
    "            if 'int_labels' in data_file.keys():\n",
    "                self.int_labels = data_file['int_labels'][:]  # Size (num_recipes=23547,), about cuisines\n",
    "            if 'bin_labels' in data_file.keys():\n",
    "                self.bin_labels = data_file['bin_labels'][:]  # Size (num_recipes=23547, 20), about cuisines\n",
    "        \n",
    "        self.padding_idx = self.bin_data.shape[1]  # == num_ingredient == 6714\n",
    "        self.max_num_ingredients_per_recipe = self.bin_data.sum(1).max()  # valid & test의 경우 65\n",
    "        \n",
    "        # (59나 65로) 고정된 길이의 row vector에 해당 recipe의 indices 넣고 나머지는 padding index로 채워넣기\n",
    "        # self.int_data: Size (num_recipes=23547, self.max_num_ingredients_per_recipe=59 or 65)\n",
    "        self.int_data = np.full((len(self.bin_data), self.max_num_ingredients_per_recipe), self.padding_idx) \n",
    "        for i, bin_recipe in enumerate(self.bin_data):\n",
    "            recipe = np.arange(self.padding_idx)[bin_recipe==1]\n",
    "            self.int_data[i][:len(recipe)] = recipe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.bin_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bin_data = self.bin_data[idx]\n",
    "        int_data = self.int_data[idx]\n",
    "        if self.test:\n",
    "            return bin_data, int_data\n",
    "        \n",
    "        bin_label = self.bin_labels[idx]\n",
    "        if 'valid_compl' in self.data_dir:\n",
    "            return bin_data, int_data, bin_label\n",
    "        \n",
    "        int_label = self.int_labels[idx]\n",
    "        return bin_data, int_data, bin_label, int_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b56130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = ['train_class', 'train_compl', 'valid_class', 'valid_compl', 'test_class', 'test_compl']\n",
    "\n",
    "recipe_datasets = {x: RecipeDataset(os.path.join(path_container, x), test='test' in x) for x in dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "562aabe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "[564, 1263, 4074, 4203, 4901, 5277, 5360, 6232, 7835, 10585, 10777, 12476, 13301, 13989, 15951, 17374, 18153, 19039, 20469]\n"
     ]
    }
   ],
   "source": [
    "count_single_ingredient_recipe = 0\n",
    "list_single_ingredient_recipe = []\n",
    "for i in range(len(recipe_datasets['train_class'])):\n",
    "    _bd, _,_,_ = recipe_datasets['train_class'][i]\n",
    "    if _bd.sum()<2:\n",
    "        count_single_ingredient_recipe += 1\n",
    "        list_single_ingredient_recipe.append(i)\n",
    "print(count_single_ingredient_recipe)\n",
    "print(list_single_ingredient_recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295438fa",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be1d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building blocks of Set Transformers ##\n",
    "# added masks.\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False, dropout=0):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.p = dropout\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Sequential(\n",
    "            nn.Linear(dim_V, dim_V),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_V, dim_V))\n",
    "        self.Dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, Q, K, mask=None):\n",
    "        # Q (batch, q_len, d_hid)\n",
    "        # K (batch, k_len, d_hid)\n",
    "        # V (batch, v_len, d_hid == dim_V)\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "        \n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        # Q_ (batch * num_heads, q_len, d_hid // num_heads)\n",
    "        # K_ (batch * num_heads, k_len, d_hid // num_heads)\n",
    "        # V_ (batch * num_heads, v_len, d_hid // num_heads)\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "        \n",
    "        # energy (batch * num_heads, q_len, k_len)\n",
    "        energy = Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V)\n",
    "        if mask is not None:\n",
    "            energy.masked_fill_(mask, float('-inf'))\n",
    "        A = torch.softmax(energy, 2)\n",
    "        \n",
    "        # O (batch, q_len, d_hid)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
    "        _O = self.fc_o(O)\n",
    "        if self.p > 0:\n",
    "            _O = self.Dropout(_O)\n",
    "        O = O + _O \n",
    "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False, dropout=0.2):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(X, X, mask=mask)\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False, dropout=0.2):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X, mask=mask)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False, dropout=0.2):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln, dropout=dropout)\n",
    "        \n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3c6ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(x):\n",
    "        \"\"\" Convert int_data into bin_data, if needed. \"\"\"\n",
    "        if type(x) is not torch.Tensor:\n",
    "            x = torch.LongTensor(x)\n",
    "        if x.dim() > 2:\n",
    "            x = x.squeeze()\n",
    "            if x.dim() > 2:\n",
    "                return False\n",
    "        elif x.dim() < 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        return F.one_hot(x).sum(1)[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5ae7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0416: num_output 제거. 왜냐하면 pooling layer에서 굳이 (batch 당) 두 개의 벡터를 내보낼 이유가 없다고 판단함.\n",
    "# ____: 이에 따라 decoder1 제거. decoder2는 decoder로 변경. 사실 decoder 굳이 필요할까 싶어서 일단 주석처리 해둠.\n",
    "# 0417: CCNet을 여러 요소로 쪼갬. (Encoder, Classifier, Completer)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.use_skip_conn = (dim_input == dim_output)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.BatchNorm1d(dim_input),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim_input, dim_hidden),\n",
    "            nn.BatchNorm1d(dim_hidden),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim_hidden, dim_output))\n",
    "    def forward(self, x):\n",
    "        if self.use_skip_conn:\n",
    "            return self.ff(x) + x\n",
    "        return self.ff(x)\n",
    "\n",
    "class ResBlockLN(nn.Module):\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output):\n",
    "        super(ResBlockLN, self).__init__()\n",
    "        self.use_skip_conn = (dim_input == dim_output)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.LayerNorm(dim_input),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim_input, dim_hidden),\n",
    "            nn.LayerNorm(dim_hidden),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim_hidden, dim_output))\n",
    "    def forward(self, x):\n",
    "        if self.use_skip_conn:\n",
    "            return self.ff(x) + x\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\" Create Feature Vector of Given Recipe. \"\"\"\n",
    "    def __init__(self, dim_embedding=256,\n",
    "                 num_items=6714, \n",
    "                 num_inds=32,      # For ISAB\n",
    "                 dim_hidden=128, \n",
    "                 num_heads=4, \n",
    "                 num_enc_layers=4,\n",
    "                 ln=True,          # LayerNorm option\n",
    "                 dropout=0.2      # Dropout option\n",
    "                ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.padding_idx = num_items\n",
    "        self.embedding =  nn.Embedding(num_embeddings=num_items+1, embedding_dim=dim_embedding, padding_idx=-1)\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [ISAB(dim_embedding, dim_hidden, num_heads, num_inds, ln=ln, dropout=dropout)] +\n",
    "            [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln, dropout=dropout) for _ in range(num_enc_layers-1)])\n",
    "        self.pooling = PMA(dim_hidden, num_heads, 1, ln=ln)\n",
    "        \n",
    "        self.out = self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x(=recipes): (batch, max_num_ingredient=65) : int_data.\n",
    "\n",
    "        self.out = self.embedding(x)\n",
    "        # (batch, max_num_ingredient=65, dim_embedding=256)\n",
    "        # cf. embedding.weight: (num_items+1=6715, dim_embedding=256)\n",
    "\n",
    "        self.mask = (x == self.padding_idx).repeat(self.num_heads,1).unsqueeze(1)\n",
    "        # mask: (batch*num_heads, 1, max_num_ingredient=65)\n",
    "        \n",
    "        for module in self.encoder:\n",
    "            self.out = module(self.out, mask=self.mask)\n",
    "        # (batch, max_num_ingredient=65, dim_hidden=128) : permutation-equivariant.\n",
    "\n",
    "        return self.pooling(self.out, mask=self.mask) # (batch, 1, dim_hidden=128) : permutation-invariant.\n",
    "    \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dim_hidden=128, dim_output=20):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            ResBlock(dim_hidden, dim_hidden, dim_hidden),\n",
    "            ResBlock(dim_hidden, dim_hidden, dim_hidden),\n",
    "            ResBlock(dim_hidden, dim_hidden, dim_output))\n",
    "       \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class Completer(nn.Module):\n",
    "    def __init__(self, dim_embedding=256,\n",
    "                 num_items=6714, \n",
    "                 num_inds=32,      # For ISAB\n",
    "                 dim_hidden=128, \n",
    "                 num_heads=4, \n",
    "                 num_dec_layers=2,\n",
    "                 ln=True,          # LayerNorm option\n",
    "                 dropout=0.2,      # Dropout option\n",
    "                 mode = 'attention',\n",
    "                ):\n",
    "        super(Completer, self).__init__()\n",
    "        assert mode in ['attention', 'concat']\n",
    "        self.mode = mode\n",
    "        self.ff = nn.Sequential(\n",
    "                ResBlock(dim_hidden, dim_hidden, dim_hidden), ResBlock(dim_hidden, dim_hidden, dim_hidden))\n",
    "        if mode == 'attention':\n",
    "            self.decoder = nn.ModuleList(\n",
    "                [MAB(dim_hidden, dim_embedding, dim_hidden, num_heads, ln=ln, dropout=dropout) for _ in range(num_dec_layers)])\n",
    "            self.ff1 = nn.Sequential(\n",
    "                ResBlock(dim_hidden, dim_hidden, dim_hidden), ResBlock(dim_hidden, dim_hidden, num_items))\n",
    "        elif mode == 'concat':\n",
    "            self.ff2 = ResBlockLN(dim_embedding, dim_hidden, dim_hidden)\n",
    "            self.decoder = nn.ModuleList(\n",
    "                [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln, dropout=dropout) for _ in range(max(num_dec_layers, 1))])\n",
    "            self.ff3 = nn.Sequential(\n",
    "                ResBlockLN(2*dim_hidden, 2*dim_hidden, 2*dim_hidden),\n",
    "                ResBlockLN(2*dim_hidden, dim_hidden, 2*dim_hidden),\n",
    "                ResBlockLN(2*dim_hidden, dim_hidden, 1))\n",
    "        \n",
    "        self.out = self.emb_feature = None\n",
    "        \n",
    "    def forward(self, x, embedding_weight):\n",
    "        batch_size = x.size(0)\n",
    "        num_items = embedding_weight.size(0)\n",
    "        \n",
    "        self.out = self.ff(x.squeeze(1)).unsqueeze(1) # (batch, 1, dim_hidden=128)\n",
    "        embedding_weight = embedding_weight.unsqueeze(0)\n",
    "        # embedding_weight: (1, num_items=6714, dim_embedding=256)\n",
    "        \n",
    "        if self.mode == 'attention':\n",
    "            for module in self.decoder:\n",
    "                self.out = module(self.out, embedding_weight.repeat(batch_size,1,1))\n",
    "            return self.ff1(self.out.squeeze(1))  # (batch, num_items=6714)\n",
    "        elif self.mode == 'concat':\n",
    "            self.emb_feature = self.ff2(embedding_weight) # (1, num_items, dim_hidden)\n",
    "            for module in self.decoder:\n",
    "                self.emb_feature = module(self.emb_feature)\n",
    "            self.out = torch.cat([self.out.repeat(1,num_items,1),\n",
    "                self.emb_feature.repeat(batch_size,1,1)], dim=2) # (batch, num_items, 2*dim_hidden)\n",
    "            return self.ff3(self.out).squeeze() # (batch, num_items)\n",
    "        \n",
    "    \n",
    "class CCNet(nn.Module):\n",
    "    def __init__(self, dim_embedding=256, #\n",
    "                 dim_output=20,\n",
    "                 num_items=6714, \n",
    "                 num_inds=32, \n",
    "                 dim_hidden=128, \n",
    "                 num_heads=4, \n",
    "                 num_enc_layers=4, \n",
    "                 num_dec_layers=2,\n",
    "                 ln=True,          # LayerNorm option\n",
    "                 dropout=0.5,      # Dropout option\n",
    "                 classify=True,    # completion만 하고 싶으면 False로\n",
    "                 complete=True,    # classification만 하고 싶으면 False로\n",
    "                 freeze_classify=False, # classification만 관련된 parameter freeze\n",
    "                 freeze_complete=False,  # completion만 관련된 parameter freeze\n",
    "                 decoder_mode = 'attention',\n",
    "                 ignore_used_ingredients=False\n",
    "                 ):\n",
    "   \n",
    "        super(CCNet, self).__init__()\n",
    "        self.classify, self.complete = classify, complete\n",
    "        self.ignore_used_ingredients = ignore_used_ingredients\n",
    "        self.encoder = Encoder(dim_embedding=dim_embedding,\n",
    "                               num_items=num_items, \n",
    "                               num_inds=num_inds,\n",
    "                               dim_hidden=dim_hidden, \n",
    "                               num_heads=num_heads, \n",
    "                               num_enc_layers=num_enc_layers,\n",
    "                               ln=ln, dropout=dropout)\n",
    "        if classify:\n",
    "            self.classifier = Classifier(dim_hidden=dim_hidden,\n",
    "                                         dim_output=dim_output)\n",
    "            if freeze_classify:\n",
    "                for p in self.classifier.parameters():\n",
    "                    p.requires_grad = False\n",
    "        if complete:\n",
    "            self.completer = Completer(dim_embedding=dim_embedding,\n",
    "                                       num_items=num_items, \n",
    "                                       num_inds=num_inds,\n",
    "                                       dim_hidden=dim_hidden, \n",
    "                                       num_heads=num_heads, \n",
    "                                       num_dec_layers=num_dec_layers,\n",
    "                                       ln=ln, dropout=dropout,\n",
    "                                       mode = decoder_mode)\n",
    "            if freeze_complete:\n",
    "                for p in self.completer.parameters():\n",
    "                    p.requires_grad = False\n",
    "    \n",
    "    def forward(self, x, bin_x=None): \n",
    "        # x(=recipes): (batch, max_num_ingredient=65) : int_data.\n",
    "        #print('input', x.size())\n",
    "        if not (self.classify or self.complete):\n",
    "            return\n",
    "        recipe_feature = self.encoder(x) # (batch, 1, dim_hidden)\n",
    "        #print('encoder output', recipe_feature.size())\n",
    "        \n",
    "        logit_classification, logit_completion = None, None\n",
    "\n",
    "        # Classification:\n",
    "        if self.classify:\n",
    "            logit_classification = self.classifier(recipe_feature.squeeze(1))  # (batch, dim_output)\n",
    "            \n",
    "        # Completion:\n",
    "        if self.complete:\n",
    "            embedding_weight = self.encoder.embedding.weight[:-1]\n",
    "            # embedding_weight: (1, num_items=6714, dim_embedding=256)\n",
    "            \n",
    "            logit_completion = self.completer(recipe_feature, embedding_weight)\n",
    "            if self.ignore_used_ingredients:\n",
    "                if bin_x is None:\n",
    "                    bin_x = make_one_hot(x)\n",
    "                bool_x = (bin_x == 1)\n",
    "                logit_completion[bool_x] = float('-inf')\n",
    "\n",
    "        return logit_classification, logit_completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1997ed3",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "848ee4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationASLoss(nn.Module):\n",
    "    '''\n",
    "    MultiClass ASL(single label) + F1 Loss.\n",
    "    \n",
    "    References:\n",
    "    - ASL paper: https://arxiv.org/abs/2009.14119\n",
    "    - optimized ASL: https://github.com/Alibaba-MIIL/ASL/blob/main/src/loss_functions/losses.py\n",
    "    '''\n",
    "    def __init__(self, gamma_pos=0, gamma_neg=4, eps: float = 0.1, reduction='mean', average='macro'):\n",
    "        super(ClassificationASLoss, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "        self.targets_classes = []\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.reduction = reduction\n",
    "        self.average = average\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        '''\n",
    "        \"input\" dimensions: - (batch_size,number_classes)\n",
    "        \"target\" dimensions: - (batch_size)\n",
    "        '''\n",
    "        num_classes = inputs.size()[-1]\n",
    "        log_preds = self.logsoftmax(inputs)\n",
    "        self.targets_classes = torch.zeros_like(inputs).scatter_(1, target.long().unsqueeze(1), 1) # make binary label\n",
    "\n",
    "        targets = self.targets_classes\n",
    "        anti_targets = 1 - targets\n",
    "        xs_pos = torch.exp(log_preds)\n",
    "        xs_neg = 1 - xs_pos\n",
    "        \n",
    "        # TP / FP / FN\n",
    "        tp = (xs_pos * targets).sum(dim=0)\n",
    "        fp = (xs_pos * anti_targets).sum(dim=0)\n",
    "        fn = (xs_neg * targets).sum(dim=0) \n",
    "        \n",
    "        if self.average == 'micro':\n",
    "            tp = tp.sum()\n",
    "            fp = fp.sum()\n",
    "            fn = fn.sum()\n",
    "        \n",
    "        # F1 score\n",
    "        f1 = (tp / (tp + 0.5*(fp + fn) + self.eps)).clamp(min=self.eps, max=1-self.eps).mean()\n",
    "        \n",
    "        # ASL weights\n",
    "        xs_pos = xs_pos * targets\n",
    "        xs_neg = xs_neg * anti_targets\n",
    "        asymmetric_w = torch.pow(1 - xs_pos - xs_neg,\n",
    "                                 self.gamma_pos * targets + self.gamma_neg * anti_targets)\n",
    "        log_preds = log_preds * asymmetric_w\n",
    "\n",
    "        if self.eps > 0:  # label smoothing\n",
    "            self.targets_classes = self.targets_classes.mul(1 - self.eps).add(self.eps / num_classes)\n",
    "\n",
    "        # loss calculation\n",
    "        loss = - self.targets_classes.mul(log_preds)\n",
    "\n",
    "        loss = loss.sum(dim=-1)\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss + (1. - f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76b6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationFocalLoss(nn.Module):\n",
    "    '''\n",
    "    MultiClass F1 Loss + FocalLoss.\n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
    "    - https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\n",
    "    '''\n",
    "    def __init__(self, eps=1e-8, average='macro', reduction='mean', gamma=2):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.average = average\n",
    "        self.reduction = reduction\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # focal loss\n",
    "        ce_loss = F.cross_entropy(pred, target, reduction=self.reduction)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "\n",
    "        focalloss = (1-pt)**self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            focalloss = focalloss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focalloss.sum()\n",
    "        \n",
    "        # f1 loss\n",
    "        target = F.one_hot(target, pred.size(-1)).float()\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        \n",
    "        tp = (target * pred).sum(dim=0).float()\n",
    "        fp = ((1 - target) * pred).sum(dim=0).float()\n",
    "        fn = (target * (1 - pred)).sum(dim=0).float()\n",
    "\n",
    "        if self.average == 'micro':\n",
    "            tp, fp, fn = tp.sum(), fp.sum(), fn.sum()\n",
    "        \n",
    "        f1 = (tp / (tp + 0.5*(fp + fn) + self.eps)).clamp(min=self.eps, max=1-self.eps)\n",
    "\n",
    "        return 1 - f1.mean() + focalloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc3dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionASLoss(nn.Module):\n",
    "    '''\n",
    "    MultiLabel ASL Loss + F1 Loss\n",
    "    \n",
    "    References:\n",
    "    - ASL paper: https://arxiv.org/abs/2009.14119\n",
    "    - optimized ASL: https://github.com/Alibaba-MIIL/ASL/blob/main/src/loss_functions/losses.py\n",
    "    '''\n",
    "    def __init__(self, gamma_pos=1, gamma_neg=4, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False, average='macro'):\n",
    "        super(CompletionASLoss, self).__init__()\n",
    "        \n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "        self.average = average\n",
    "\n",
    "        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n",
    "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
    "        self.tp = self.fp = self.fn = self.f1 = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "\n",
    "        self.targets = y.float()\n",
    "        self.anti_targets = 1 - y.float()\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        self.xs_pos = torch.sigmoid(x.float())\n",
    "        self.xs_neg = 1.0 - self.xs_pos\n",
    "        \n",
    "        # TP/FP/FN\n",
    "        self.tp = (self.xs_pos * self.targets).sum(dim=0)\n",
    "        self.fp = (self.xs_pos * self.anti_targets).sum(dim=0)\n",
    "        self.fn = (self.xs_neg * self.targets).sum(dim=0)        \n",
    "        \n",
    "        if self.average == 'micro':\n",
    "            self.tp = self.tp.sum()\n",
    "            self.fp = self.fp.sum()\n",
    "            self.fn = self.fn.sum()\n",
    "        \n",
    "        # F1 score\n",
    "        self.f1 = (self.tp / (self.tp + 0.5*(self.fp + self.fn) + self.eps)).clamp(\n",
    "            min=self.eps, max=1-self.eps)\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            self.xs_neg.add_(self.clip)\n",
    "            self.xs_neg.clamp_(max=1.)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
    "        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            self.xs_pos = self.xs_pos * self.targets\n",
    "            self.xs_neg = self.xs_neg * self.anti_targets\n",
    "            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            self.loss *= self.asymmetric_w\n",
    "\n",
    "        return -self.loss.mean(dim=0).sum() + (1.-self.f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c34830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionBCELoss(nn.Module):\n",
    "    '''\n",
    "    MultiLabel weighted F1_Loss + BCEWithLogitsLosss.\n",
    "    '''\n",
    "    def __init__(self, eps=1e-8, average='macro', reduction='mean', weight=None, gamma=2):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "        self.eps = eps\n",
    "        self.average = average\n",
    "        self.reduction = reduction\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        if average not in ['macro', 'micro']:\n",
    "            raise ValueError('average should be macro or micro.')\n",
    "        \n",
    "    def forward(self, pred, target): # same dimension\n",
    "        bce_loss = self.bce(pred.float(), target.float())\n",
    "        \n",
    "        # f1 loss\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        \n",
    "        tp = (target * pred).sum(dim=0).float()\n",
    "        fp = ((1 - target) * pred).sum(dim=0).float()\n",
    "        fn = (target * (1 - pred)).sum(dim=0).float()\n",
    "\n",
    "        if self.average == 'micro':\n",
    "            tp, fp, fn = tp.sum(), fp.sum(), fn.sum()\n",
    "        \n",
    "        f1 = tp / (tp + 0.5*(fp + fn) + self.eps).clamp(min=self.eps, max=1-self.eps)\n",
    "\n",
    "        return 1 - f1.mean() + bce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd775f9",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "183a9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB, 일단 뺐음\n",
    "# 0415: valid loss는 classification과 completion 각각 구해서 합한 것을 취해야 할 것으로 보임.\n",
    "# ____: f1_score는 sklearn.metrics.f1_score로 대체, torchmetrics의 dependency 제거, 관련 오류 고침(f1score 항상 1 나오는 오류)\n",
    "\n",
    "def train(model,\n",
    "          dataloaders,\n",
    "          #criterion,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          #metrics,\n",
    "          dataset_sizes,\n",
    "          device='cpu',\n",
    "          num_epochs=20,\n",
    "          #wandb_log=False,\n",
    "          early_stop_patience=None,\n",
    "          classify=True,\n",
    "          complete=True,\n",
    "          random_seed=1):\n",
    "\n",
    "    def _concatenate(running_v, new_v):\n",
    "        if running_v is not None:\n",
    "            return np.concatenate((running_v, new_v.clone().detach().cpu().numpy()), axis=0)\n",
    "        else:\n",
    "            return new_v.clone().detach().cpu().numpy()\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    torch.random.manual_seed(random_seed)\n",
    "\n",
    "    #global label_weight\n",
    "    if classify:\n",
    "        criterion_class = ClassificationASLoss().to(device) # ClassificationFocalLoss().to(device)\n",
    "    if complete:\n",
    "        criterion_compl = CompletionASLoss().to(device) #CompletionBCELoss().to(device)\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_loss = 1e4\n",
    "    best_micro_f1 = 0. # classification only\n",
    "\n",
    "    if early_stop_patience is not None:\n",
    "        if not isinstance(early_stop_patience, int):\n",
    "            raise TypeError('early_stop_patience should be an integer.')\n",
    "        patience_cnt = 0\n",
    "    \n",
    "    print('-' * 5 + 'Training the model' + '-' * 5)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        val_loss = 0. # sum of classification and completion loss\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid_class', 'valid_compl']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                if not classify and phase == 'valid_class':\n",
    "                    continue\n",
    "                elif not complete and phase == 'valid_compl':\n",
    "                    continue\n",
    "\n",
    "            running_loss_class = 0.\n",
    "            running_loss_compl = 0.\n",
    "            running_corrects_compl = 0.\n",
    "            running_corrects_class = 0.\n",
    "            running_labels_class = None\n",
    "            running_preds_class = None\n",
    "            \n",
    "            dataset_name = phase\n",
    "            if phase == 'train':\n",
    "                dataset_name = 'train_class' if classify and not complete else 'train_compl'\n",
    "            log_gap = 100 if dataset_name != 'train_compl' else 1000\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for idx, loaded_data in enumerate(dataloaders[dataset_name]):\n",
    "                if phase == 'valid_compl':\n",
    "                    bin_inputs, int_inputs, bin_labels = loaded_data  # no int_label\n",
    "                else:\n",
    "                    bin_inputs, int_inputs, bin_labels, int_labels = loaded_data\n",
    "                \n",
    "                batch_size = bin_inputs.size(0)\n",
    "                num_items = bin_inputs.size(-1)\n",
    "                if classify and phase in ['train', 'valid_class']:\n",
    "                    labels_class = int_labels.to(device)\n",
    "                if complete and phase in ['train', 'valid_compl']:\n",
    "                    labels_compl = bin_labels.to(device)\n",
    "                bin_inputs = bin_inputs.to(device)\n",
    "                int_inputs = int_inputs.to(device)\n",
    "                    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs_class, outputs_compl = model(int_inputs, bin_x=bin_inputs)  # bin_x 없어도 작동은 가능\n",
    "                    if classify and phase in ['train', 'valid_class']:\n",
    "                        _, preds_class = torch.max(outputs_class, 1)\n",
    "                    if complete and phase in ['train', 'valid_compl']:\n",
    "                        _out = outputs_compl.clone().detach()\n",
    "                        _out[bin_inputs==1] = float('-inf')\n",
    "                        _, preds_compl = torch.max(_out, 1)\n",
    "                        _label = labels_compl.clone().detach()\n",
    "                        _label[bin_inputs==1] = 0  # 주어진 재료 말고,\n",
    "                        _label = torch.arange(num_items).repeat(batch_size,1)[_label==1].long()  # 새로 넣어야 할 재료만 골라내기\n",
    "                        \n",
    "                    if idx == 0:  # 원래 idx == 0 \n",
    "                        if classify and phase in ['train', 'valid_class']:\n",
    "                            print('labels_classification', labels_class.cpu().numpy())\n",
    "                            print('preds_classification', preds_class.cpu().numpy())\n",
    "                        if complete and phase in ['train', 'valid_compl']:\n",
    "                            print('labels_completion', _label.cpu().numpy())\n",
    "                            print('preds_completion', preds_compl.cpu().numpy())\n",
    "                    \n",
    "                    if classify and phase in ['train', 'valid_class']:\n",
    "                        loss_class = criterion_class(outputs_class, labels_class.long())\n",
    "                    if complete and phase in ['train', 'valid_compl']:\n",
    "                        loss_compl = criterion_compl(outputs_compl, labels_compl)\n",
    "\n",
    "                    if classify and complete and phase == 'train':\n",
    "                        loss = loss_class + loss_compl\n",
    "                    elif classify and phase in ['train', 'valid_class']:\n",
    "                        loss = loss_class\n",
    "                    elif complete and phase in ['train', 'valid_compl']:\n",
    "                        loss = loss_compl\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1) # gradient clipping\n",
    "                        optimizer.step()\n",
    "\n",
    "                if idx % 100 == 0 and phase == 'train':\n",
    "                    log_str = f'    {phase} {idx * 100 // len(dataloaders[dataset_name]):3d}% of an epoch | '\n",
    "                    if classify and phase in ['train', 'valid_class']:\n",
    "                        log_str += f'Loss(classif.): {loss_class.item():.4f} | '\n",
    "                    if complete and phase in ['train', 'valid_compl']:\n",
    "                        log_str += f'Loss(complet.): {loss_compl.item():.4f} | '\n",
    "                    print(log_str)\n",
    "\n",
    "                # statistics\n",
    "                if classify and phase in ['train', 'valid_class']: # for F1 score & accuracy\n",
    "                    running_loss_class += loss_class.item() * batch_size\n",
    "                    running_labels_class = _concatenate(running_labels_class, labels_class)\n",
    "                    running_preds_class = _concatenate(running_preds_class, preds_class)\n",
    "                    running_corrects_class += torch.sum(preds_class == labels_class.data)\n",
    "                if complete and phase in ['train', 'valid_compl']: # for accuracy\n",
    "                    running_loss_compl += loss_compl.item() * batch_size\n",
    "                    running_corrects_compl += torch.sum(preds_compl.detach().cpu() == _label)\n",
    "\n",
    "\n",
    "            epoch_loss = 0.\n",
    "            log_str = f'{phase.upper()} | '\n",
    "            if classify and phase in ['train', 'valid_class']:\n",
    "                epoch_loss_class = running_loss_class / dataset_sizes[dataset_name]\n",
    "                epoch_loss += epoch_loss_class\n",
    "                running_labels_class = torch.from_numpy(running_labels_class)\n",
    "                running_preds_class = torch.from_numpy(running_preds_class)\n",
    "                epoch_macro_f1 = f1_score(running_labels_class, running_preds_class, average='macro')  # classification: f1 scores.\n",
    "                epoch_micro_f1 = f1_score(running_labels_class, running_preds_class, average='micro')\n",
    "                epoch_acc_class = running_corrects_class / dataset_sizes[dataset_name]\n",
    "                log_str += f'Loss(classif.): {epoch_loss_class:.3f} Acc(classif.): {epoch_acc_class:.3f} Macro-F1: {epoch_macro_f1:.3f} Micro-F1: {epoch_micro_f1:.3f} | '\n",
    "            if complete and phase in ['train', 'valid_compl']:\n",
    "                epoch_loss_compl = running_loss_compl / dataset_sizes[dataset_name]\n",
    "                epoch_loss += epoch_loss_compl\n",
    "                epoch_acc_compl = running_corrects_compl / dataset_sizes[dataset_name]  # completion task: accuracy.\n",
    "                log_str += f'Loss(complet.): {epoch_loss_compl:.3f} Acc(complet.): {epoch_acc_compl:.3f} | '\n",
    "            print(log_str)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = epoch_loss\n",
    "                if classify:\n",
    "                    train_macro_f1 = epoch_macro_f1\n",
    "                    train_micro_f1 = epoch_micro_f1\n",
    "                # if wandb_log:\n",
    "                #     wandb.watch(model)\n",
    "            elif 'val' in phase:\n",
    "                val_loss += epoch_loss\n",
    "                if classify and phase == 'valid_class':\n",
    "                    val_macro_f1 = epoch_macro_f1\n",
    "                    val_micro_f1 = epoch_micro_f1\n",
    "            \n",
    "        \n",
    "        if classify and not complete:\n",
    "            scheduler.step(-val_micro_f1)  # because schedular's mode == 'min'\n",
    "            # deep copy the model\n",
    "            if val_micro_f1 > best_micro_f1:\n",
    "                best_micro_f1 = val_micro_f1\n",
    "                best_loss = val_loss  # Actually, it is not the best_loss, but just save it.\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "                if early_stop_patience is not None:\n",
    "                    patience_cnt = 0\n",
    "            elif early_stop_patience is not None:\n",
    "                patience_cnt += 1\n",
    "        else:\n",
    "            scheduler.step(val_loss)\n",
    "            # deep copy the model\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "                if early_stop_patience is not None:\n",
    "                    patience_cnt = 0\n",
    "            elif early_stop_patience is not None:\n",
    "                patience_cnt += 1\n",
    "            \n",
    "\n",
    "        \"\"\"\n",
    "        if wandb_log:\n",
    "            wandb.log({'train_loss': train_loss,\n",
    "                       'val_loss': val_loss,\n",
    "                       'train_macro_f1': train_macro_f1,\n",
    "                       'train_micro_f1': train_micro_f1,\n",
    "                       'val_macro_f1': val_macro_f1,\n",
    "                       'val_micro_f1': val_micro_f1,\n",
    "                       'best_val_loss': best_loss,\n",
    "                       'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "                                        # scheduler.get_last_lr()[0] for CosineAnnealingWarmRestarts\n",
    "        \"\"\"\n",
    "        if early_stop_patience is not None:\n",
    "            if patience_cnt > early_stop_patience:\n",
    "                print(f'Early stop at epoch {epoch}.')\n",
    "                break\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val Loss: {:4f}'.format(best_loss))\n",
    "    if classify and not complete:\n",
    "        print('Best micro f1 score: {:4f}'.format(best_micro_f1))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0c5eb",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5c6d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(dim_embedding=256,\n",
    "               dim_hidden=128,\n",
    "               dropout=0.5,\n",
    "               subset_length=None,\n",
    "               decoder_mode='attention',\n",
    "               batch_size=16,\n",
    "               n_epochs=50,\n",
    "               lr=1e-3,\n",
    "               step_size=10,  # training scheduler\n",
    "               seed=0,\n",
    "               classify=True,\n",
    "               complete=True,\n",
    "               freeze_classify=False,\n",
    "               freeze_complete=False,\n",
    "               pretrained_model_path=None\n",
    "               ):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    \n",
    "    train_data_name = 'train_class' if classify and not complete else 'train_compl'\n",
    "    dataset_names = [train_data_name, 'valid_class', 'valid_compl']\n",
    "    subset_indices = {x: [i for i in range(len(recipe_datasets[x]) if subset_length is None else subset_length)\n",
    "                          ] for x in dataset_names}\n",
    "    dataloaders = {x: DataLoader(Subset(recipe_datasets[x], subset_indices[x]),\n",
    "                                 batch_size=batch_size, shuffle=True) for x in dataset_names}\n",
    "    dataset_sizes = {x: len(subset_indices[x]) for x in dataset_names}\n",
    "    print(dataset_sizes)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('device: ', device)\n",
    "\n",
    "    # Get a batch of training data\n",
    "    bin_inputs, int_inputs, bin_labels, int_labels = next(iter(dataloaders[train_data_name]))\n",
    "    print('inputs.shape', bin_inputs.shape, int_inputs.shape)\n",
    "    print('labels.shape', bin_labels.shape, int_labels.shape)\n",
    "\n",
    "    model_ft = CCNet(dim_embedding=dim_embedding, dim_output=20, dim_hidden=dim_hidden,\n",
    "                     num_items=len(bin_inputs[0]), num_enc_layers=4, num_dec_layers=2, ln=True, dropout=dropout,\n",
    "                     decoder_mode=decoder_mode, classify=classify, complete=complete,\n",
    "                     freeze_classify=freeze_classify, freeze_complete=freeze_complete).to(device)\n",
    "    if pretrained_model_path is not None:\n",
    "        pretrained_dict = torch.load(pretrained_model_path)\n",
    "        model_dict = model_ft.state_dict()\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict) \n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(pretrained_dict)\n",
    "        \n",
    "    #print(model_ft)\n",
    "    total_params = sum(dict((p.data_ptr(), p.numel()) for p in model_ft.parameters() if p.requires_grad ).values())\n",
    "    print(\"Total Number of Parameters\", total_params)\n",
    "\n",
    "    #criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optimizer = optim.AdamW([p for p in model_ft.parameters() if p.requires_grad == True],\n",
    "                                        lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.2)\n",
    "    exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=step_size,\n",
    "                                                      eps=1e-08, verbose=True)\n",
    "    # (metric) F1Score objects - sklearn으로 대체했음. train 함수 cell 참고\n",
    "    #macro_f1 = F1Score(num_classes=20, average='macro')\n",
    "    #micro_f1 = F1Score(num_classes=20, average='micro')\n",
    "    #metrics = {'macro_f1': macro_f1, 'micro_f1': micro_f1}\n",
    "\n",
    "    model_ft, best_loss = train(model_ft, dataloaders, #criterion,\n",
    "                                optimizer, exp_lr_scheduler, #metrics,\n",
    "                                dataset_sizes, device=device, num_epochs=n_epochs, early_stop_patience=20,\n",
    "                                classify=classify, complete=complete, random_seed=seed)\n",
    "    \n",
    "    fname = ['ckpt', 'CCNet', 'class', str(classify), 'compl', str(complete), 'best_loss', f'{best_loss:.4f}',\n",
    "             'dim_embedding', str(dim_embedding), 'batch', str(batch_size), 'n_epochs', str(n_epochs),\n",
    "             'lr', str(lr), 'step_size', str(step_size), 'seed', str(seed), 'subset_length', str(subset_length)]\n",
    "    if complete:\n",
    "        fname == ['decoder_mode', decoder_mode]\n",
    "    fname = '_'.join(fname) + '.pt'\n",
    "    if not os.path.isdir('./weights/'):\n",
    "        os.mkdir('./weights/')\n",
    "    torch.save(model_ft.state_dict(), os.path.join('./weights/', fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bb609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiment 3-1. classification + completion \n",
    "experiment(batch_size=32, n_epochs=100, lr=1e-3, dim_embedding=256, dim_hidden=256, decoder_mode='concat', dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3-2. classification + completion \n",
    "experiment(batch_size=32, n_epochs=100, lr=1e-3, dim_embedding=256, dim_hidden=256, decoder_mode='attention', dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2-1. completion only\n",
    "experiment(batch_size=32, n_epochs=100, lr=1e-4, dim_embedding=256, dim_hidden=256, decoder_mode='concat', classify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b273a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2-2. completion only\n",
    "experiment(batch_size=32, n_epochs=100, lr=1e-4, dim_embedding=256, dim_hidden=256, decoder_mode='attention', classify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ab644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiment 1. classification only\n",
    "experiment(batch_size=128, n_epochs=100, lr=1e-3, dim_embedding=256, dim_hidden=256, dropout=0.5, complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3af2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49401f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 시간: grad_clip 없을 때 기준. (gpu: 3090)\n",
    "experiment(batch_size=2048, n_epochs=100, lr=1e-3, dim_embedding=256, dim_hidden=256, dropout=0.5, complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c61ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment(batch_size=1024, n_epochs=100, lr=1e-3, dim_embedding=256, dim_hidden=256, dropout=0.5, complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a0b25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment(batch_size=512, n_epochs=100, lr=1e-3, dim_embedding=256, dim_hidden=256, dropout=0.5, complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc756d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment(batch_size=256, n_epochs=100, lr=1e-3, dim_embedding=256, dim_hidden=256, dropout=0.5, complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58164e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment(batch_size=128, n_epochs=100, lr=1e-3, dim_embedding=256, dim_hidden=256, dropout=0.5, complete=False)  # (Macro f1 0.658)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf840d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment(batch_size=64, n_epochs=100, lr=1e-3, dim_embedding=256, dim_hidden=256, dropout=0.5, complete=False)  # best micro f1 score (Macro f1 0.661)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b51afb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_class': 23547, 'valid_class': 7848, 'valid_compl': 7848}\n",
      "device:  cuda:0\n",
      "inputs.shape torch.Size([32, 6714]) torch.Size([32, 59])\n",
      "labels.shape torch.Size([32, 20]) torch.Size([32])\n",
      "Total Number of Parameters 5059092\n",
      "-----Training the model-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "labels_classification [ 9 16  9  5 13 11  9  3  9 16 11 16  3 18  9 14  7 13 11 16 13 13  5 16\n",
      "  3 13  9  7 13  5  5  5]\n",
      "preds_classification [ 1  5  3 11  3 13  0 11 10 13  1  1  0 17  3  1  0  1 13  1 10  3  3  0\n",
      "  0  3 13 11 13  0  3  3]\n",
      "    train   0% of an epoch | Loss(classif.): 3.6449 | \n",
      "    train  13% of an epoch | Loss(classif.): 2.6280 | \n",
      "    train  27% of an epoch | Loss(classif.): 2.8409 | \n",
      "    train  40% of an epoch | Loss(classif.): 2.2156 | \n",
      "    train  54% of an epoch | Loss(classif.): 2.5720 | \n",
      "    train  67% of an epoch | Loss(classif.): 2.0802 | \n",
      "    train  81% of an epoch | Loss(classif.): 1.9998 | \n",
      "    train  95% of an epoch | Loss(classif.): 2.0518 | \n",
      "TRAIN | Loss(classif.): 2.356 Acc(classif.): 0.497 Macro-F1: 0.223 Micro-F1: 0.497 | \n",
      "labels_classification [ 0 16  5  6 16  9  7 13 19  3  5  9 14  2  5  8  7 14  9  7  0  1  9 11\n",
      " 11 18  7  9 11  3 13  9]\n",
      "preds_classification [ 9 16 16  9 16  9  7 13 18 16  9  9 14  2  9  5  7  7  2  7 13 16  9  7\n",
      " 11 13  7 13  7  3 13  9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:28<46:36, 28.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID_CLASS | Loss(classif.): 2.081 Acc(classif.): 0.569 Macro-F1: 0.299 Micro-F1: 0.569 | \n",
      "\n",
      "Epoch 2/100\n",
      "labels_classification [14  7  9 13  9  8  9  9  3  8  3 10 11 13  9  9 13 19 13 16  7 13 16 13\n",
      " 17  7 13 17  0 16 17  5]\n",
      "preds_classification [14  7  9 13  9  9  9  9 12  9 11 18  3 13  9  9 13 16 13 16 18 13 16 13\n",
      "  9  7 13  9 18 16  5  9]\n",
      "    train   0% of an epoch | Loss(classif.): 1.8286 | \n",
      "    train  13% of an epoch | Loss(classif.): 1.7182 | \n",
      "    train  27% of an epoch | Loss(classif.): 1.7571 | \n",
      "    train  40% of an epoch | Loss(classif.): 1.9555 | \n",
      "    train  54% of an epoch | Loss(classif.): 1.7283 | \n",
      "    train  67% of an epoch | Loss(classif.): 2.2410 | \n",
      "    train  81% of an epoch | Loss(classif.): 1.8258 | \n"
     ]
    }
   ],
   "source": [
    "experiment(batch_size=32, n_epochs=100, lr=1e-3, dim_embedding=256, dim_hidden=256, dropout=0.5, complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343cc71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment(batch_size=16, n_epochs=100, lr=1e-3, dim_embedding=256, dim_hidden=256, dropout=0.5, complete=False) # best loss (Macro f1 0.652)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d516743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Full on Python 3.7 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
