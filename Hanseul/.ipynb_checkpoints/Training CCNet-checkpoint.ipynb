{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f208fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5py 안 될 때\n",
    "#!brew reinstall hdf5\n",
    "#!export CPATH=\"/opt/homebrew/include/\"\n",
    "#!export HDF5_DIR=/opt/homebrew/\n",
    "#!python3 -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6909cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import F1Score\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaacbb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdb3e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '../'\n",
    "path_container = './Container/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb2659",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4db71052",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_container + 'id_cuisine_dict.pickle', 'rb') as f:\n",
    "    id_cuisine_dict = pickle.load(f)\n",
    "with open(path_container + 'cuisine_id_dict.pickle', 'rb') as f:\n",
    "    cuisine_id_dict = pickle.load(f)\n",
    "with open(path_container + 'id_ingredient_dict.pickle', 'rb') as f:\n",
    "    id_ingredient_dict = pickle.load(f)\n",
    "with open(path_container + 'ingredient_id_dict.pickle', 'rb') as f:\n",
    "    ingredient_id_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d9249e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, data_dir, test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.test = test\n",
    "        with h5py.File(data_dir, 'r') as data_file:\n",
    "            self.bin_data = data_file['bin_data'][:]  # Size (num_recipes=23547, num_ingredients=6714)\n",
    "            if not test:\n",
    "                self.int_labels = data_file['int_labels'][:]  # Size (num_recipes=23547,), about cuisines\n",
    "                self.bin_labels = data_file['bin_labels'][:]  # Size (num_recipes=23547, 20), about cuisines\n",
    "        \n",
    "        self.padding_idx = self.bin_data.shape[1]  # == num_ingredient == 6714\n",
    "        self.max_num_ingredients_per_recipe = self.bin_data.sum(1).max()  # valid & test의 경우 65\n",
    "        \n",
    "        # (59나 65로) 고정된 길이의 row vector에 해당 recipe의 indices 넣고 나머지는 padding index로 채워넣기\n",
    "        # self.int_data: Size (num_recipes=23547, self.max_num_ingredients_per_recipe=59 or 65)\n",
    "        self.int_data = np.full((len(self.bin_data), self.max_num_ingredients_per_recipe), self.padding_idx) \n",
    "        for i, bin_recipe in enumerate(self.bin_data):\n",
    "            recipe = np.arange(self.padding_idx)[bin_recipe==1]\n",
    "            self.int_data[i][:len(recipe)] = recipe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.bin_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bin_data = self.bin_data[idx]\n",
    "        int_data = self.int_data[idx]\n",
    "        bin_label = None if self.test else self.bin_labels[idx]\n",
    "        int_label = None if self.test else self.int_labels[idx]\n",
    "        \n",
    "        return bin_data, int_data, bin_label, int_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f0e17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = ['train', 'valid_class', 'valid_compl', 'test_class', 'test_compl']\n",
    "\n",
    "recipe_datasets = {x: RecipeDataset(os.path.join(path_container, x), test='test' in x) for x in dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1bcc5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6714,)\n",
      "(59,)\n",
      "(20,)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "_bd,_id,_bl,_il = recipe_datasets['train'][0]\n",
    "print(_bd.shape)\n",
    "print(_id.shape)\n",
    "print(_bl.shape)\n",
    "print(_il.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559311cb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bd58c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building blocks of Set Transformers ##\n",
    "# added masks.\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False, dropout=0):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.p = dropout\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Sequential(\n",
    "            nn.Linear(dim_V, 2*dim_V),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*dim_V, dim_V))\n",
    "        self.Dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, Q, K, mask=None):\n",
    "        # Q (batch, q_len, d_hid)\n",
    "        # K (batch, k_len, d_hid)\n",
    "        # V (batch, v_len, d_hid == dim_V)\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "        \n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        \n",
    "        # Q_ (batch * num_heads, q_len, d_hid // num_heads)\n",
    "        # K_ (batch * num_heads, k_len, d_hid // num_heads)\n",
    "        # V_ (batch * num_heads, v_len, d_hid // num_heads)\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "        \n",
    "        # energy (batch * num_heads, q_len, k_len)\n",
    "        energy = Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V)\n",
    "        if mask is not None:\n",
    "            energy.masked_fill_(mask, float('-inf'))\n",
    "        A = torch.softmax(energy, 2)\n",
    "        \n",
    "        # O (batch, q_len, d_hid)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
    "        _O = self.fc_o(O)\n",
    "        if self.p > 0:\n",
    "            _O = self.Dropout(_O)\n",
    "        O = O + _O \n",
    "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False, dropout=0.2):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(X, X, mask=mask)\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False, dropout=0.2):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X, mask=mask)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False, dropout=0.2):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln, dropout=dropout)\n",
    "        \n",
    "    def forward(self, X, mask=None):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e90444e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(x):\n",
    "        \"\"\" Convert int_data into bin_data, if needed. \"\"\"\n",
    "        if type(x) is not torch.Tensor:\n",
    "            x = torch.LongTensor(x)\n",
    "        if x.dim() > 2:\n",
    "            x = x.squeeze()\n",
    "            if x.dim() > 2:\n",
    "                return False\n",
    "        elif x.dim() < 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        return F.one_hot(x).sum(1)[:,:-1]\n",
    "\n",
    "class CCNet(nn.Module):\n",
    "    def __init__(self, dim_embedding=256, #\n",
    "                 dim_output=20,\n",
    "                 num_items=6714, \n",
    "                 num_inds=32, \n",
    "                 dim_hidden=128, \n",
    "                 num_heads=4, \n",
    "                 num_outputs=1+1,  # classification 1 + completion 1\n",
    "                 num_enc_layers=4, \n",
    "                 num_dec_layers=2,\n",
    "                 ln=True,          # LayerNorm option\n",
    "                 dropout=0.2,      # Dropout option\n",
    "                 classify=True,    # completion만 하고 싶으면 False로\n",
    "                 complete=True):   # classification만 하고 싶으면 False로\n",
    "        super(CCNet, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.padding_idx = num_items\n",
    "        self.classify, self.complete = classify, complete\n",
    "        self.embedding =  nn.Embedding(num_embeddings=num_items+1, embedding_dim=dim_embedding, padding_idx=-1)\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [ISAB(dim_embedding, dim_hidden, num_heads, num_inds, ln=ln)] +\n",
    "            [ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln) for _ in range(num_enc_layers-1)])\n",
    "        self.pooling = PMA(dim_hidden, num_heads, num_outputs, ln=ln)\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            *[SAB(dim_hidden, dim_hidden, num_heads, ln=ln, dropout=dropout) for _ in range(num_dec_layers)])\n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_output))\n",
    "        self.decoder2 = nn.ModuleList(\n",
    "            [MAB(dim_hidden, dim_embedding, dim_hidden, num_heads, ln=ln, dropout=dropout) for _ in range(2)])\n",
    "        self.ff2 = nn.Linear(dim_hidden, num_items)\n",
    "    \n",
    "    def forward(self, x, bin_x=None): \n",
    "        # x(=recipes): (batch, max_num_ingredient=65) : int_data.\n",
    "        if not (self.classify or self.complete):\n",
    "            return\n",
    "        \n",
    "        x = torch.LongTensor(x)\n",
    "        feature = self.embedding(x)\n",
    "        # feature: (batch, max_num_ingredient=65, dim_embedding=256)\n",
    "        # cf. embedding.weight: (num_items+1=6715, dim_embedding=256)\n",
    "        mask = (x == self.padding_idx).repeat(self.num_heads,1).unsqueeze(1)\n",
    "        # mask: (batch*num_heads, 1, max_num_ingredient=65)\n",
    "        code = feature.clone()\n",
    "        for module in self.encoder:\n",
    "            code = module(code, mask=mask)\n",
    "        # code: (batch, max_num_ingredient=65, dim_hidden=128) : permutation-equivariant.\n",
    "        \n",
    "        pooled = self.pooling(code, mask=mask)\n",
    "        # pooled: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "        \n",
    "        signals = self.decoder1(pooled)\n",
    "        # no mask; signals: (batch, num_outputs=2, dim_hidden=128) : permutation-invariant.\n",
    "        \n",
    "        # split two signals: for classification & completion.\n",
    "        signal_classification = signals[:][0]                   # (batch, dim_hidden=128)\n",
    "        signal_completion = signals.clone()[:][1].unsqueeze(1)  # (batch, 1, dim_hidden=128)\n",
    "        \n",
    "        logit_classification, logit_completion = None, None\n",
    "        \n",
    "        # Classification:\n",
    "        if self.classify:\n",
    "            logit_classification = self.ff1(signal_classification)  # (batch, dim_output)\n",
    "        \n",
    "        # Completion:\n",
    "        if self.complete:\n",
    "            if bin_x is None:\n",
    "                bin_x = make_one_hot(x)\n",
    "            bool_x = (bin_x == True)\n",
    "            used_ingred_mask = bool_x.repeat(self.num_heads,1).unsqueeze(1)\n",
    "            # used_ingred_mask: (batch*num_heads, 1, num_items=6714)\n",
    "            \n",
    "            embedding_weight = self.embedding.weight[:-1].unsqueeze(0).repeat(feature.size(0),1,1)\n",
    "            # embedding_weight: (batch, num_items+1=6715, dim_embedding=256)\n",
    "            \n",
    "            for module in self.decoder2:\n",
    "                signal_completion = module(signal_completion, embedding_weight, mask=used_ingred_mask)\n",
    "            logit_completion = self.ff2(signal_completion.squeeze()) # (batch, num_items=6714)\n",
    "            logit_completion[bool_x] = float('-inf')\n",
    "\n",
    "        return logit_classification, logit_completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2609d3",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6d08699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB, early_stop patience 일단 뺐음\n",
    "\n",
    "def train(model,\n",
    "          dataloaders,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          metrics,\n",
    "          dataset_sizes,\n",
    "          device='cpu',\n",
    "          num_epochs=20,\n",
    "          #wandb_log=False,\n",
    "          early_stop_patience=None,\n",
    "          classify=True,\n",
    "          complete=True,\n",
    "          random_seed=1):\n",
    "    \n",
    "    assert isinstance(metrics, dict), f\"'metrics' argument should be a dictionary, but {type(metrics)}.\"\n",
    "    \n",
    "    def _concatenate(running_v, new_v):\n",
    "        if running_v is not None:\n",
    "            return np.concatenate((running_v, new_v.clone().detach().cpu().numpy()), axis=0)\n",
    "        else:\n",
    "            return new_v.clone().detach().cpu().numpy()\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    torch.random.manual_seed(random_seed)\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_loss = 1e4\n",
    "    \n",
    "    \"\"\"\n",
    "    if early_stop_patience is not None:\n",
    "        if not isinstance(early_stop_patience, int):\n",
    "            raise TypeError('early_stop_patience should be an integer.')\n",
    "        patience_cnt = 0\n",
    "    \"\"\"\n",
    "\n",
    "    print('-' * 5 + 'Training the model' + '-' * 5)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid_class', 'valid_compl']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                if not classify and phase == 'valid_class':\n",
    "                    continue\n",
    "                elif not complete and phase == 'valid_compl':\n",
    "                    continue\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects_compl = 0.0\n",
    "            running_labels_class = None\n",
    "            running_preds_class = None\n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (bin_inputs, int_inputs, bin_labels, int_labels) in enumerate(dataloaders[phase]):\n",
    "                if classify and phase in ['train', 'valid_class']:\n",
    "                    labels_class = int_labels.to(device)\n",
    "                if complete:\n",
    "                    # randomly remove one ingredient for each recipe/batch\n",
    "                    if phase == 'train':\n",
    "                        # completion을 위한 label을 만들거임\n",
    "                        labels_compl = torch.zeros_like(int_labels)\n",
    "                        # 각 batch 별로,\n",
    "                        for batch in range(int_labels.size(0)):\n",
    "                            # \n",
    "                            ingreds = torch.arange(bin_inputs.size(-1))[bin_inputs[batch]==1]\n",
    "                            mask_ingred_idx = ingreds[np.random.randint(len(ingreds))]\n",
    "                            bin_inputs[batch][mask_ingred_idx] = 0\n",
    "                            int_inputs[batch][int_inputs[batch] == mask_ingred_idx] = int(bin_inputs.size(-1))\n",
    "                            labels_compl[batch] = mask_ingred_idx\n",
    "                    elif phase == 'valid_compl':\n",
    "                        labels_compl = int_labels.to(device)\n",
    "                bin_inputs = bin_inputs.to(device)\n",
    "                int_inputs = int_inputs.to(device)\n",
    "                \n",
    "                    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs_class, outputs_compl = model(int_inputs, bin_x=bin_inputs)  # bin_x 없어도 작동은 가능\n",
    "                    _, preds_class = torch.max(outputs_class, 1)\n",
    "                    _, preds_compl = torch.max(outputs_compl, 1)\n",
    "                    if idx == 0:\n",
    "                        if classify and phase in ['train', 'valid_class']:\n",
    "                            print('labels_classification', labels_class[0])\n",
    "                            print('outputs_classification', outputs_class[0])\n",
    "                            print('preds_classification', preds_class[0])\n",
    "                        if complete and phase in ['train', 'valid_compl']:\n",
    "                            print('labels_completion', labels_compl[0])\n",
    "                            print('outputs_completion', outputs_compl[0])\n",
    "                            print('preds_completion', preds_compl[0])\n",
    "                    \n",
    "                    if classify and complete and phase == 'train':\n",
    "                        loss = criterion(outputs_class, labels_class.long()) \\\n",
    "                                + criterion(outputs_compl, labels_compl.long())\n",
    "                    elif classify and phase in ['train', 'valid_class']:\n",
    "                        loss = criterion(outputs_class, labels_class.long())\n",
    "                    elif complete and phase in ['train', 'valid_compl']:\n",
    "                        loss = criterion(outputs_compl, labels_compl.long())\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * bin_inputs.size(0)\n",
    "                running_corrects_compl += torch.sum(preds_compl == labels_compl.data)  # for accuracy\n",
    "                \n",
    "                if classify and phase in ['train', 'valid_class']: # for F1 score\n",
    "                    running_labels_class = _concatenate(running_labels_class, labels_class)\n",
    "                    running_preds_class = _concatenate(running_preds_class, labels_class)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects_compl.double() / dataset_sizes[phase]  # completion task: accuracy.\n",
    "            running_labels_class = torch.from_numpy(running_labels_class)\n",
    "            running_preds_class = torch.from_numpy(running_preds_class)\n",
    "            epoch_macro_f1 = metrics['macro_f1'](running_labels_class, running_preds_class)  # classification: f1 scores.\n",
    "            epoch_micro_f1 = metrics['micro_f1'](running_labels_class, running_preds_class)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Macro-F1: {:.4f} Micro-F1: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, epoch_macro_f1, epoch_micro_f1))\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = epoch_loss\n",
    "                train_macro_f1 = epoch_macro_f1\n",
    "                train_micro_f1 = epoch_micro_f1\n",
    "                # if wandb_log:\n",
    "                #     wandb.watch(model)\n",
    "            elif phase == 'val':\n",
    "                val_loss = epoch_loss\n",
    "                val_macro_f1 = epoch_macro_f1\n",
    "                val_micro_f1 = epoch_micro_f1\n",
    "            # if phase == 'train':\n",
    "            #     scheduler.step()\n",
    "            if phase == 'val':\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "            if phase == 'val':\n",
    "                # deep copy the model\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = deepcopy(model.state_dict())\n",
    "                    if early_stop_patience is not None:\n",
    "                        patience_cnt = 0\n",
    "                elif early_stop_patience is not None:\n",
    "                    patience_cnt += 1\n",
    "\n",
    "        \"\"\"\n",
    "        if wandb_log:\n",
    "            wandb.log({'train_loss': train_loss,\n",
    "                       'val_loss': val_loss,\n",
    "                       'train_macro_f1': train_macro_f1,\n",
    "                       'train_micro_f1': train_micro_f1,\n",
    "                       'val_macro_f1': val_macro_f1,\n",
    "                       'val_micro_f1': val_micro_f1,\n",
    "                       'best_val_loss': best_loss,\n",
    "                       'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "                                        # scheduler.get_last_lr()[0] for CosineAnnealingWarmRestarts\n",
    "        \"\"\"\n",
    "        if early_stop_patience is not None:\n",
    "            if patience_cnt > early_stop_patience:\n",
    "                print(f'Early stop at epoch {epoch}.')\n",
    "                break\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # after last epoch, generate confusion matrix of validation phase\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4405ba",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae7f8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(batch_size=16,\n",
    "               n_epochs=50,\n",
    "               lr=1e-3,\n",
    "               step_size=10,  # training scheduler\n",
    "               seed=0):\n",
    "    \n",
    "    dataloaders = {x: DataLoader(recipe_datasets[x], batch_size=batch_size,\n",
    "                                 shuffle=True ) for x in dataset_name[:3]}\n",
    "    dataset_sizes = {x: len(recipe_datasets[x]) for x in dataset_name[:3]}\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('device: ', device)\n",
    "\n",
    "    # Get a batch of training data\n",
    "    bin_inputs, int_inputs, bin_labels, int_labels = next(iter(dataloaders['train']))\n",
    "    print('inputs.shape', bin_inputs.shape, int_inputs.shape)\n",
    "    print('labels.shape', bin_labels.shape, int_labels.shape)\n",
    "\n",
    "    model_ft = CCNet(dim_embedding=256, dim_output=len(int_labels),\n",
    "                     num_items=len(bin_inputs[0]), num_outputs=1+1,\n",
    "                     num_enc_layers=4, num_dec_layers=2, ln=True, dropout=0.2,\n",
    "                     classify=True, complete=True).to(device)\n",
    "    print(model_ft)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer = optim.AdamW(model_ft.parameters(),\n",
    "                                        lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.2)\n",
    "    exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=step_size,\n",
    "                                                      eps=1e-08, verbose=True)\n",
    "    # (metric) F1Score objects\n",
    "    macro_f1 = F1Score(num_classes=len(bin_labels[0]), average='macro')\n",
    "    micro_f1 = F1Score(num_classes=len(bin_labels[0]), average='micro')\n",
    "    metrics = {'macro_f1': macro_f1, 'micro_f1': micro_f1}\n",
    "\n",
    "    model_ft = train(model_ft, dataloaders, criterion, optimizer, exp_lr_scheduler, metrics,\n",
    "                     dataset_sizes, device=device, num_epochs=n_epochs, early_stop_patience=20,\n",
    "                     classify=True, complete=True, random_seed=seed)\n",
    "    \n",
    "    fname = ['ckpt', 'CCNet', 'batch', str(batch_size),\n",
    "             'n_epochs', str(n_epochs), 'lr', str(lr), 'step_size', str(step_size),\n",
    "             'seed', str(seed)]\n",
    "    fname = '_'.join(fname) + '.pt'\n",
    "    if not os.path.isdir('./weights/'):\n",
    "        os.mkdir('./weights/')\n",
    "    torch.save(model_ft.state_dict(), os.path.join('./weights/', fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbbf3a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "inputs.shape torch.Size([16, 6714]) torch.Size([16, 59])\n",
      "labels.shape torch.Size([16, 20]) torch.Size([16])\n",
      "CCNet(\n",
      "  (embedding): Embedding(6715, 256, padding_idx=6714)\n",
      "  (encoder): ModuleList(\n",
      "    (0): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooling): PMA(\n",
      "    (mab): MAB(\n",
      "      (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_o): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder1): Sequential(\n",
      "    (0): SAB(\n",
      "      (mab): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): SAB(\n",
      "      (mab): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc_o): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (Dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ff1): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=16, bias=True)\n",
      "  )\n",
      "  (decoder2): ModuleList(\n",
      "    (0): MAB(\n",
      "      (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc_k): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (fc_v): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_o): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): MAB(\n",
      "      (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc_k): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (fc_v): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (ln0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_o): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ff2): Linear(in_features=128, out_features=6714, bias=True)\n",
      ")\n",
      "-----Training the model-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb50f6ce2ea2495b99b954a3e76d9654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(batch_size, n_epochs, lr, step_size, seed)\u001b[0m\n\u001b[1;32m     32\u001b[0m micro_f1 \u001b[38;5;241m=\u001b[39m F1Score(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(bin_labels[\u001b[38;5;241m0\u001b[39m]), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: macro_f1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: micro_f1}\n\u001b[0;32m---> 35\u001b[0m model_ft \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdataset_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mclassify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m fname \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mckpt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCCNet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(batch_size),\n\u001b[1;32m     40\u001b[0m          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(n_epochs), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(lr), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(step_size),\n\u001b[1;32m     41\u001b[0m          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(seed)]\n\u001b[1;32m     42\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(fname) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloaders, criterion, optimizer, scheduler, metrics, dataset_sizes, device, num_epochs, early_stop_patience, classify, complete, random_seed)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# track history if only in train\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 90\u001b[0m     outputs_class, outputs_compl \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mint_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbin_inputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# bin_x 없어도 작동은 가능\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     _, preds_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs_class, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     92\u001b[0m     _, preds_compl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs_compl, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mCCNet.forward\u001b[0;34m(self, x, bin_x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# embedding_weight: (batch, num_items+1=6715, dim_embedding=256)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder2:\n\u001b[0;32m---> 91\u001b[0m     signal_completion \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal_completion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mused_ingred_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m logit_completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff2(signal_completion\u001b[38;5;241m.\u001b[39msqueeze()) \u001b[38;5;66;03m# (batch, num_items=6714)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m logit_completion[bool_x] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mMAB.forward\u001b[0;34m(self, Q, K, mask)\u001b[0m\n\u001b[1;32m     36\u001b[0m V_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(V\u001b[38;5;241m.\u001b[39msplit(dim_split, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# energy (batch * num_heads, q_len, k_len)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m energy \u001b[38;5;241m=\u001b[39m \u001b[43mQ_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_V)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     energy\u001b[38;5;241m.\u001b[39mmasked_fill_(mask, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "experiment(n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00ca9be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(recipe_datasets['train'], batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecb5eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_, b_, c_, d_  = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86b2ce74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 6714])\n"
     ]
    }
   ],
   "source": [
    "print(a_.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f53a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
