{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "470952a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e72d0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecDataset(Dataset):\n",
    "    def __init__(self, recs_list, answer_dict, query_num, item_num, transform=None, target_transform=None):\n",
    "        # rec_matrix = [query num, model_num, item_num]\n",
    "        self.rec_matrix = []\n",
    "        for i in query_num:\n",
    "            self.rec_matrix.append(csr_matrix((len(rec_file_list), item_num)))\n",
    "        for i, recs in enumerate(recs_list):\n",
    "            for query in recs.keys():\n",
    "                rec = recs[query]\n",
    "                rec_items, rec_scores = [rec_ for rec_, score in rec], [score for rec_, score in rec]\n",
    "                rec_scores = normalize(np.array(rec_scores)[:,np.newaxis], axis=0).ravel()\n",
    "                for item, score in zip(rec_items, rec_scores):\n",
    "                    self.rec_matrix[query][i, item] = score\n",
    "        self.labels = answer_dict\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.rec_matrix.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec_matrix = self.rec_matrix[idx].toarray()\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return rec_matrix, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8e7c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model_len, k=10):\n",
    "        super(Network, self).__init__()\n",
    "        self.w1 = torch.nn.Parameter(torch.randn(k, model_len))\n",
    "        self.w2 = torch.nn.Parameter(torch.randn(1, k))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        x = x.float()\n",
    "        x = torch.einsum('nm, bmp -> bnp', self.w1, x)\n",
    "        x = torch.einsum('nm, bmp -> bnp', self.w2, x).squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6961769",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unable to infer matrix dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\scipy\\sparse\\_compressed.py:95\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     major_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     96\u001b[0m     minor_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of unsized object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ans \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(answer):\n\u001b[0;32m     22\u001b[0m     answer_dict[i] \u001b[38;5;241m=\u001b[39m ans\n\u001b[1;32m---> 24\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mRecDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m###############\u001b[39;00m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mRecDataset.__init__\u001b[1;34m(self, recs_list, answer_dict, query_num, item_num, transform, target_transform)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, recs_list, answer_dict, query_num, item_num, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, target_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# rec_matrix = [query num, model_num, item_num]\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcsr_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrec_file_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, recs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(recs_list):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m recs\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\scipy\\sparse\\_compressed.py:98\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     96\u001b[0m     minor_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munable to infer matrix dimensions\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape \u001b[38;5;241m=\u001b[39m check_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap((major_dim,\n\u001b[0;32m    101\u001b[0m                                           minor_dim)))\n",
      "\u001b[1;31mValueError\u001b[0m: unable to infer matrix dimensions"
     ]
    }
   ],
   "source": [
    "rec_file_list = [\"./train_recs/CF_rec_cpl_dim_64.pickle\",\n",
    "                \"./train_recs/Graph_rec_cpl_1_2_depth_5.pickle\",\n",
    "                \"./train_recs/Graph_rec_cpl_1_4_depth_3.pickle\",\n",
    "                \"./train_recs/Graph_rec_cpl_1_8_depth_3.pickle\",\n",
    "                \"./train_recs/Graph_rec_cpl_1_8_depth_1.pickle\",]\n",
    "\n",
    "recs_list = []\n",
    "for rec_file in rec_file_list:\n",
    "    with open(rec_file, 'rb') as f:\n",
    "        recs = pickle.load(f)\n",
    "        recs_list.append(recs)\n",
    "\n",
    "query_num = len(recs_list[0])\n",
    "item_num = 6714\n",
    "\n",
    "h5f_valid = h5py.File('../Hanseul/Container/train_cpl', 'r')\n",
    "answer = h5f_valid['labels_id'][:].astype(np.int64)\n",
    "h5f_valid.close()\n",
    "\n",
    "answer_dict = {}\n",
    "for i, ans in enumerate(answer):\n",
    "    answer_dict[i] = ans\n",
    "\n",
    "train_data = RecDataset(recs_list, answer_dict, query_num, item_num)\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "###############\n",
    "rec_file_list = [\"./valid_recs/CF_rec_cpl_dim_64.pickle\",\n",
    "                \"./valid_recs/Graph_rec_cpl_1_2_depth_5.pickle\",\n",
    "                \"./valid_recs/Graph_rec_cpl_1_4_depth_3.pickle\",\n",
    "                \"./valid_recs/Graph_rec_cpl_1_8_depth_3.pickle\",\n",
    "                \"./valid_recs/Graph_rec_cpl_1_8_depth_1.pickle\",]\n",
    "recs_list = []\n",
    "for rec_file in rec_file_list:\n",
    "    with open(rec_file, 'rb') as f:\n",
    "        recs = pickle.load(f)\n",
    "        recs_list.append(recs)\n",
    "\n",
    "query_num = len(recs_list[0])\n",
    "item_num = 6714\n",
    "\n",
    "h5f_valid = h5py.File('./Container/valid_cpl', 'r')\n",
    "answer = h5f_valid['labels_id'][:].astype(np.int64)\n",
    "h5f_valid.close()\n",
    "\n",
    "answer_dict = {}\n",
    "for i, ans in enumerate(answer):\n",
    "    answer_dict[i] = ans\n",
    "\n",
    "test_data = RecDataset(recs_list, answer_dict, query_num, item_num)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(len(rec_file_list), k=10)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / (batch + 1)\n",
    "    print(f\"Train loss: {loss:>7f}\")\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\"\"\" \n",
    "###구현할 거###\n",
    "3. 모델 학습마다 앙상블된 결과 제작 -> metric 측정\n",
    "3. train set / valid set 따로 앙상블\n",
    "4. WanDB 적용\n",
    "\"\"\"\n",
    "        \n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8dd36b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x0000021908C45580>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9676d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm, bmp -> bnp (10x3) X 64x(3x6714) = 64x(10x6714)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
